{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "1252c7abebd4f3b33eb4a1b3e1e29ee4ac452210"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# pd.set_option('max_colwidth', 500)\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 500)\n",
    "\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import xgboost as xgb\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "# from catboost import CatBoostRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "np.random.seed(1029)\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import cv2\n",
    "from keras.applications.densenet import preprocess_input, DenseNet121\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\n",
    "import keras.backend as K\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3d176daedc7bb27dd1717b9193500d11bd498b89"
   },
   "source": [
    "# nfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "805a41ec46e550cd935b9e646a408117733f9b61"
   },
   "outputs": [],
   "source": [
    "N_FOLDS = 4\n",
    "FOLDS = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e21e261bcf7ced65dbaf066d316f5d7cc637b34"
   },
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "dc5ed1bd82d0dd7db9aa7f7f5936f16c673292a1"
   },
   "outputs": [],
   "source": [
    "def get_chi2(obs, exp):\n",
    "    diff = set(exp) - set(obs)\n",
    "    f_obs = obs.value_counts()\n",
    "    f_exp = exp.value_counts()\n",
    "    if diff:\n",
    "        for i in diff:\n",
    "            f_obs[i] = 0\n",
    "    f_obs = f_obs.sort_index()\n",
    "    f_exp = f_exp.sort_index()\n",
    "    chi2, _ = stats.chisquare(f_obs.values,f_exp.values)\n",
    "    return chi2\n",
    "\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    rater_a = y_true\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)\n",
    "\n",
    "def get_class_bounds(y, y_pred, N=5, class0_fraction=-1):\n",
    "    ysort = np.sort(y)\n",
    "    predsort = np.sort(y_pred)\n",
    "    bounds = []\n",
    "    for ibound in range(N-1):\n",
    "        iy = len(ysort[ysort <= ibound])\n",
    "        if (ibound == 0) and (class0_fraction >= 0.0) :\n",
    "            iy = int(class0_fraction * iy)\n",
    "        bounds.append(predsort[iy])\n",
    "    return bounds\n",
    "\n",
    "def assign_class(y_pred, boundaries):\n",
    "    y_classes = np.zeros(len(y_pred))\n",
    "    for iclass, bound in enumerate(boundaries):\n",
    "        y_classes[y_pred >= bound] = iclass + 1\n",
    "    return y_classes.astype(int)\n",
    "\n",
    "def get_init_coefs(y_test_pred, y_test):\n",
    "    kappas = []\n",
    "    coefs = []\n",
    "    cl0fracs = np.array(np.arange(0.01,30,0.01))\n",
    "    for cl0frac in cl0fracs:\n",
    "        coef = get_class_bounds(y_test, y_test_pred, class0_fraction=cl0frac)\n",
    "        coefs.append(coef)\n",
    "        y_test_k = assign_class(y_test_pred, coef)\n",
    "        kappa = cohen_kappa_score(y_test, y_test_k, weights='quadratic')\n",
    "        kappas.append(kappa)\n",
    "    ifmax = np.array(kappas).argmax()\n",
    "    best_frac = cl0fracs[ifmax]\n",
    "    best_coef = coefs[ifmax]\n",
    "#     print(\"Best init coefs: \", best_coef)\n",
    "#     print(\"Bset init coefs kappa: \", np.max(kappas))    \n",
    "    return best_coef\n",
    "\n",
    "def rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "df57919e6c9a665f035406395c6efe1862000bf6"
   },
   "outputs": [],
   "source": [
    "def resize_to_square(im):\n",
    "    old_size = im.shape[:2]\n",
    "    ratio = float(img_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "    delta_w = img_size - new_size[1]\n",
    "    delta_h = img_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "    return new_im\n",
    "\n",
    "def load_image(path, pet_id):\n",
    "    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n",
    "    new_image = resize_to_square(image)\n",
    "    new_image = preprocess_input(new_image)\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "6bb28cd1f5be499d75fc8fdeff71111481988a13"
   },
   "outputs": [],
   "source": [
    "class OptimizedRounder(object):\n",
    "    def __init__(self,initial_coefs = None):\n",
    "        if(initial_coefs == None):\n",
    "            self.initial_coefs = [1.775, 2.1057, 2.4438, 2.7892]\n",
    "        else:\n",
    "            self.initial_coefs = initial_coefs.copy()\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, self.initial_coefs, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "\n",
    "class OptimizedRounder_v2(object):\n",
    "    def __init__(self, initial_coefs = None):\n",
    "        if(initial_coefs == None):\n",
    "            self.initial_coefs = [1.775, 2.1057, 2.4438, 2.7892]\n",
    "        else:\n",
    "            self.initial_coefs = initial_coefs.copy()\n",
    "        self.coef_ = 0\n",
    "    \n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n",
    "        ll = cohen_kappa_score(y, X_p, weights = 'quadratic')    \n",
    "        chi2 =  get_chi2(X_p, y)\n",
    "        ll = ll - chi2 * (1.0 / 25000)\n",
    "        return -ll\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X = X, y = y)\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, self.initial_coefs, method = 'nelder-mead')\n",
    "    \n",
    "    def predict(self, X, coef):\n",
    "        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n",
    "        return preds\n",
    "    \n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "\n",
    "\n",
    "class OptimizedRounder_v3(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef, len_0=410):\n",
    "        X_p = np.copy(X)\n",
    "        temp = sorted(list(X_p))\n",
    "        threshold = temp[int(0.9*len_0)-1]\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < threshold:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= threshold and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c5d5c0cd887647b81e2f9255b2fdd2cd981c3dba"
   },
   "source": [
    "# 特征: GZF & ZKR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "00cccd1f2607cf0d5b2957e9a28ceac8e591e931"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\n",
    "test = pd.read_csv(\"../input/petfinder-adoption-prediction/test/test.csv\")\n",
    "breeds = pd.read_csv(\"../input/petfinder-adoption-prediction/breed_labels.csv\")\n",
    "colors = pd.read_csv(\"../input/petfinder-adoption-prediction/color_labels.csv\")\n",
    "states = pd.read_csv(\"../input/petfinder-adoption-prediction/state_labels.csv\")\n",
    "\n",
    "# train = train.sort_values(\"RescuerID\")\n",
    "# train.index = range(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "89fa84cdd31b260ca2335214a3020116079ab51a"
   },
   "outputs": [],
   "source": [
    "origin_train = train[list(train.columns)]\n",
    "origin_test = test[list(test.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "8ab95181d459b746c944c06cc4f94f369aa5f8a9"
   },
   "outputs": [],
   "source": [
    "breedid_map = dict(zip(breeds['BreedID'], breeds['BreedName'].map(lambda x:x.lower())))\n",
    "color_map = dict(zip(colors['ColorID'], colors['ColorName'].map(lambda x:x)))\n",
    "state_map = dict(zip(states['StateID'], states['StateName'].map(lambda x:x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "7ece1c37bd58937e7b788f86d8d023b60e4a8f57"
   },
   "outputs": [],
   "source": [
    "train_id = train['PetID']\n",
    "test_id = test['PetID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "894347ebcedebf336608600974dc27cefe7ce34b"
   },
   "source": [
    "## 0 common feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "689e47ac466057a5df72cbeb91eacfc578e01809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nf count: 551\n",
      "nf count: 133\n"
     ]
    }
   ],
   "source": [
    "def sentiment_feature(data, ids, path):\n",
    "    doc_sent_mag = []\n",
    "    doc_sent_score = []\n",
    "    doc_sent_len = []\n",
    "    doc_sent_mags = []\n",
    "    doc_sent_scores = []\n",
    "\n",
    "    doc_entity_len = []\n",
    "    doc_entity_sali = []\n",
    "\n",
    "    nf_count = 0\n",
    "\n",
    "    for pet in ids:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/%s/' % path + pet + '.json', 'r') as f:\n",
    "                sentiment = json.load(f)\n",
    "            doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "            doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "            \n",
    "            doc_sent_len.append(len(sentiment['sentences']))\n",
    "            if len(sentiment['sentences']) == 0:\n",
    "                doc_sent_mags.append([-999])\n",
    "                doc_sent_scores.append([-999])\n",
    "            else:\n",
    "                doc_sent_mags.append([sent['sentiment']['magnitude'] for sent in sentiment['sentences']])\n",
    "                doc_sent_scores.append([sent['sentiment']['score'] for sent in sentiment['sentences']])\n",
    "            \n",
    "            doc_entity_len.append(len(sentiment['entities']))\n",
    "            if len(sentiment['entities']) == 0:\n",
    "                doc_entity_sali.append([-999])\n",
    "            else:\n",
    "                doc_entity_sali.append([entity['salience'] for entity in sentiment['entities']])\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            doc_sent_mag.append(-1)\n",
    "            doc_sent_score.append(-1)\n",
    "            doc_sent_len.append(-1)\n",
    "            doc_sent_mags.append([-1000])\n",
    "            doc_sent_scores.append([-1000])\n",
    "            doc_entity_len.append(-1)\n",
    "            doc_entity_sali.append([-1000])\n",
    "            \n",
    "    print('nf count:', nf_count) \n",
    "\n",
    "    data.loc[:, 'doc_sent_mag'] = doc_sent_mag\n",
    "    data.loc[:, 'doc_sent_score'] = doc_sent_score\n",
    "    \n",
    "    # train.loc[:, 'doc_sent_len'] = doc_sent_len\n",
    "    # train.loc[:, 'doc_sent_mags'] = doc_sent_mags\n",
    "    # train.loc[:, 'doc_sent_scores'] = doc_sent_scores\n",
    "    # train.loc[:, 'doc_entity_len'] = doc_entity_len\n",
    "    # train.loc[:, 'doc_entity_sali'] = doc_entity_sali\n",
    "    return data\n",
    "\n",
    "train = sentiment_feature(train, train_id, 'train_sentiment')\n",
    "test = sentiment_feature(test, test_id, 'test_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "7960234e10f15a12bc59c6e61bb11719f1f3f483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341\n",
      "2\n",
      "128\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def gen_meta_f(df, ids, meta_path):\n",
    "    vertex_xs = []\n",
    "    vertex_ys = []\n",
    "    bounding_confidences = []\n",
    "    bounding_importance_fracs = []\n",
    "    dominant_blues = []\n",
    "    dominant_greens = []\n",
    "    dominant_reds = []\n",
    "    dominant_pixel_fracs = []\n",
    "    dominant_scores = []\n",
    "    \n",
    "    dominant_blues1 = []\n",
    "    dominant_greens1 = []\n",
    "    dominant_reds1 = []\n",
    "    dominant_pixel_fracs1 = []\n",
    "    dominant_scores1 = []\n",
    "\n",
    "    label_descriptions = []\n",
    "    label_descriptions1 = []\n",
    "    label_descriptions2 = []\n",
    "    label_descriptions3 = []\n",
    "    \n",
    "    label_scores = []\n",
    "    label_scores1 = []\n",
    "    label_scores2 = []\n",
    "    label_scores3 = []\n",
    "    \n",
    "    nf_count = 0\n",
    "    nl_count = 0\n",
    "    label_data = {}\n",
    "    for idx, pet in enumerate(ids):\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/%s/' % meta_path + pet + '-1.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            # 0\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            # 1\n",
    "            if len(data['imagePropertiesAnnotation']['dominantColors']['colors']) > 1 and len(data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['color']) == 3:\n",
    "                dominant_blue1 = data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['color']['blue']\n",
    "                dominant_blues1.append(dominant_blue1)\n",
    "                dominant_green1 = data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['color']['green']\n",
    "                dominant_greens1.append(dominant_green1)\n",
    "                dominant_red1 = data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['color']['red']\n",
    "                dominant_reds1.append(dominant_red1)\n",
    "                dominant_pixel_frac1 = data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['pixelFraction']\n",
    "                dominant_pixel_fracs1.append(dominant_pixel_frac1)\n",
    "                dominant_score1 = data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['score']\n",
    "                dominant_scores1.append(dominant_score1)\n",
    "        \n",
    "            else:\n",
    "                dominant_blues1.append(-1)\n",
    "                dominant_greens1.append(-1)\n",
    "                dominant_reds1.append(-1)\n",
    "                dominant_pixel_fracs1.append(-1)\n",
    "                dominant_scores1.append(-1)\n",
    "                \n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_descriptions.append(label_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "\n",
    "                if len(data['labelAnnotations']) > 1:\n",
    "                    label_description1 = data['labelAnnotations'][1]['description']\n",
    "                    label_descriptions1.append(label_description1)\n",
    "                    label_score1 = data['labelAnnotations'][1]['score']\n",
    "                    label_scores1.append(label_score1)\n",
    "                else:\n",
    "                    label_descriptions1.append('nothing')\n",
    "                    label_scores1.append(-1)\n",
    "                \n",
    "                if len(data['labelAnnotations']) > 2:\n",
    "                    label_description2 = data['labelAnnotations'][2]['description']\n",
    "                    label_descriptions2.append(label_description2)\n",
    "                    label_score2 = data['labelAnnotations'][2]['score']\n",
    "                    label_scores2.append(label_score2)\n",
    "                else:\n",
    "                    label_descriptions2.append('nothing')\n",
    "                    label_scores2.append(-1)\n",
    "\n",
    "                if len(data['labelAnnotations']) > 3:\n",
    "                    label_description3 = data['labelAnnotations'][3]['description']\n",
    "                    label_descriptions3.append(label_description3)\n",
    "                    label_score3 = data['labelAnnotations'][3]['score']\n",
    "                    label_scores3.append(label_score3)\n",
    "                else:\n",
    "                    label_descriptions3.append('nothing')\n",
    "                    label_scores3.append(-1)\n",
    "\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_descriptions.append('nothing')\n",
    "                label_descriptions1.append(label_description1)\n",
    "                label_descriptions2.append(label_description2)\n",
    "                label_descriptions3.append(label_description3)\n",
    "                \n",
    "                label_scores.append(-1)\n",
    "                label_scores1.append(-1)\n",
    "                label_scores2.append(-1)\n",
    "                label_scores3.append(-1)\n",
    "                                                            \n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            vertex_xs.append(-1)\n",
    "            vertex_ys.append(-1)\n",
    "            bounding_confidences.append(-1)\n",
    "            bounding_importance_fracs.append(-1)\n",
    "            dominant_blues.append(-1)\n",
    "            dominant_greens.append(-1)\n",
    "            dominant_reds.append(-1)\n",
    "            dominant_pixel_fracs.append(-1)\n",
    "            dominant_scores.append(-1)\n",
    "            \n",
    "            dominant_blues1.append(-1)\n",
    "            dominant_greens1.append(-1)\n",
    "            dominant_reds1.append(-1)\n",
    "            dominant_pixel_fracs1.append(-1)\n",
    "            dominant_scores1.append(-1)\n",
    "\n",
    "            label_descriptions.append('nothing')\n",
    "            label_descriptions1.append('nothing')\n",
    "            label_descriptions2.append('nothing')\n",
    "            label_descriptions3.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "            label_scores1.append(-1)\n",
    "            label_scores2.append(-1)\n",
    "            label_scores3.append(-1)\n",
    "\n",
    "    print(nf_count)\n",
    "    print(nl_count)\n",
    "    prefix = 'meta_'\n",
    "    df.loc[:, prefix+'vertex_x'] = vertex_xs\n",
    "    df.loc[:, prefix+'vertex_y'] = vertex_ys\n",
    "    df.loc[:, prefix+'bounding_confidence'] = bounding_confidences\n",
    "    df.loc[:, prefix+'bounding_importance'] = bounding_importance_fracs\n",
    "    df.loc[:, prefix+'dominant_blue'] = dominant_blues\n",
    "    df.loc[:, prefix+'dominant_green'] = dominant_greens\n",
    "    df.loc[:, prefix+'dominant_red'] = dominant_reds\n",
    "    df.loc[:, prefix+'dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "    df.loc[:, prefix+'dominant_score'] = dominant_scores\n",
    "    \n",
    "    df.loc[:, prefix+'label_description'] = label_descriptions\n",
    "    df.loc[:, prefix+'label_description1'] = label_descriptions1\n",
    "    df.loc[:, prefix+'label_description2'] = label_descriptions2\n",
    "#     df.loc[:, 'label_description3'] = label_descriptions3\n",
    "\n",
    "    df.loc[:, prefix+'label_score'] = label_scores\n",
    "    df.loc[:, prefix+'label_score1'] = label_scores1\n",
    "    df.loc[:, prefix+'label_score2'] = label_scores1\n",
    "    cate_cols = [prefix+col for col in ['label_description','label_description1','label_description2']]\n",
    "    df.loc[:, cate_cols] = df[cate_cols].astype('category')\n",
    "#     df.loc[:, 'label_score3'] = label_scores3\n",
    "\n",
    "gen_meta_f(train, train_id, 'train_metadata')\n",
    "gen_meta_f(test, test_id, 'test_metadata')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c454456e65545c754da3c13d0d6c2d856ea354eb"
   },
   "source": [
    "## 1.1 origin feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "22321b6528b77d6f4b1d63471b24a412ed61f03e"
   },
   "outputs": [],
   "source": [
    "def rescue_feature(df):\n",
    "    rescue_count = df.groupby('RescuerID')['Quantity'].count()\n",
    "    rescue_count.name = 'rescue_count'\n",
    "    rescue_num = df.groupby('RescuerID')['Quantity'].sum()\n",
    "    rescue_num.name = 'rescue_num'\n",
    "    rescue_unique_type = df.drop_duplicates(['RescuerID', 'Type']).groupby('RescuerID')['RescuerID'].count()\n",
    "    rescue_unique_type.name = 'rescue_unique_type'\n",
    "    df = df.join(rescue_count, on='RescuerID')\n",
    "    df = df.join(rescue_num, on='RescuerID') \n",
    "#     df = df.join(rescue_unique_type, on='RescuerID') \n",
    "#     df['rescue_rank'] = min_max(df['RescuerID'].rank())\n",
    "    df['rescue_rank'] = df.RescuerID.map(df.RescuerID.value_counts().rank()/df.RescuerID.unique().shape[0])\n",
    "    return df\n",
    "\n",
    "def pure_breed_encode(data):\n",
    "    data['pure_breed1'] = np.where((data['Breed1'] != 307) , '0', '1')\n",
    "    data['pure_breed2'] = np.where((data['Breed2'] == 0) , '0', \n",
    "                      np.where(data['Breed2'] != 307, '1', '2'))\n",
    "    data['pure_breed3'] = (data['pure_breed1'] + data['pure_breed2'])\n",
    "    data['pure_animal_pure_breed4'] = np.where((data['Type'].astype(np.str)=='1') & (data['pure_breed3']=='00'), '100', \n",
    "                                          np.where((data['Type'].astype(np.str)=='2') & (data['pure_breed3']=='00'), '200', \n",
    "                                          '333'))\n",
    "    for col in ['pure_breed1', 'pure_breed2', 'pure_breed3', 'pure_animal_pure_breed4']:\n",
    "        data[col] = data[col].astype('category')\n",
    "    del data['pure_animal_pure_breed4']\n",
    "    return data\n",
    "\n",
    "def call_name_f(data):\n",
    "    is_call_name = []\n",
    "    for name, desc in zip(data['Name'], data['Description']):\n",
    "        clean_desc = str(desc).lower()\n",
    "        clean_name = str(name).lower()\n",
    "        if clean_name == 'nan':\n",
    "            is_call_name.append(0)\n",
    "        else:\n",
    "            num = len(clean_desc.split(clean_name))\n",
    "            is_call_name.append(num)\n",
    "    data['call_name_num'] = is_call_name\n",
    "    return data\n",
    "\n",
    "\n",
    "train = rescue_feature(train)\n",
    "test = rescue_feature(test)\n",
    "\n",
    "train = pure_breed_encode(train)\n",
    "test = pure_breed_encode(test)\n",
    "\n",
    "# train = call_name_f(train)\n",
    "# test = call_name_f(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "985767be5219fb4c04d9d3d30c8bf2df0983392d"
   },
   "source": [
    "## 1.3 description feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "d0fc7a6f96302f9ddc0a0c722a7ee70fc43a0af6"
   },
   "outputs": [],
   "source": [
    "def language_type(desc):\n",
    "    desc = str(desc)\n",
    "    if desc=='nan':\n",
    "        return 0\n",
    "    zhmodel = re.compile(u'[\\u4e00-\\u9fa5]')    #检查中文\n",
    "    enmodel = re.compile(u'[a-zA-Z]')   #检查英文\n",
    "    zhmatch = zhmodel.search(desc)\n",
    "    enmatch = enmodel.search(desc)\n",
    "    if zhmatch and enmatch:\n",
    "        return 3  # 中英混合\n",
    "    elif zhmatch:\n",
    "        return 3  # 纯中文\n",
    "    elif enmatch:\n",
    "        return 2  # 纯英文\n",
    "    else:\n",
    "        return 1  # 都是字符\n",
    "\n",
    "def malaiyu_type(desc):\n",
    "    desc = str(desc)\n",
    "#     malai = [' ekor ', ' ngan ', ' dia ', ' sy ', ' dan ', ' leh ', ' nak ', ' dr ', ' dari ', ' la x ' , ' nk ',' nie ', ' umur ', ' di ', 'teruk', ' satu ',' dh ', ' ni ',' tp ', ' yg ', 'mmg', 'msj', ' utk ' ,'neh' ]\n",
    "    malai = [' la x ' , ' nk ',' nie ', ' umur ', ' di ', 'teruk', ' satu ',' dh ', ' ni ',' tp ', ' yg ', 'mmg', 'msj', ' utk ' ,'neh' ]\n",
    "    for ma_tag in malai:\n",
    "        if desc.find(ma_tag) > -1:\n",
    "            return ma_tag,1\n",
    "    \n",
    "    return \"\", 0\n",
    "\n",
    "lang_prefix = 'lang_'\n",
    "train[lang_prefix+'language_type'] = train.Description.map(lambda x:language_type(x))\n",
    "train[lang_prefix+'malaiyu_type'] = train.Description.map(lambda x:malaiyu_type(x)[1])\n",
    "\n",
    "test[lang_prefix+'language_type'] = test.Description.map(lambda x:language_type(x))\n",
    "test[lang_prefix+'malaiyu_type'] = test.Description.map(lambda x:malaiyu_type(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "4b3010de8bd1f282c9fe5f03bdeeec8bbcb82933",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def obtain_text(df):\n",
    "    breed1_text = df['Breed1'].map(lambda x:breedid_map.get(x, 'unknown_breed'))\n",
    "    breed2_text = df['Breed2'].map(lambda x:breedid_map.get(x, 'unknown_breed'))\n",
    "    color1_text = df['Color1'].map(lambda x:color_map.get(x, 'unknown_color'))\n",
    "    color2_text = df['Color2'].map(lambda x:color_map.get(x, 'unknown_color'))\n",
    "    color3_text = df['Color3'].map(lambda x:color_map.get(x, 'unknown_color'))\n",
    "\n",
    "    text = df['Name'].fillna(\"none\") + \" \" \\\n",
    "           + breed1_text  + \" \" \\\n",
    "           + breed2_text + \" \" \\\n",
    "           + color1_text + \" \" \\\n",
    "           + color2_text + \" \" \\\n",
    "            + color3_text + \" \" \\\n",
    "            + df['Description'].fillna(\"none\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "# train_desc = obtain_text(train)\n",
    "# test_desc = obtain_text(test)\n",
    "train_desc = train.Description.fillna(\"none\").values\n",
    "test_desc = test.Description.fillna(\"none\").values\n",
    "\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=10000,\n",
    "        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "        stop_words = 'english')\n",
    "    \n",
    "# Fit TFIDF\n",
    "tfv.fit(list(train_desc))\n",
    "X =  tfv.transform(train_desc)\n",
    "X_test = tfv.transform(test_desc)\n",
    "\n",
    "components = 120\n",
    "svd = TruncatedSVD(n_components=components)\n",
    "svd.fit(X)\n",
    "\n",
    "X = svd.transform(X)\n",
    "X = pd.DataFrame(X, columns=['svd_{}'.format(i) for i in range(components)])\n",
    "train = pd.concat((train, X), axis=1)\n",
    "X_test = svd.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns=['svd_{}'.format(i) for i in range(components)])\n",
    "test = pd.concat((test, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "41824c84db0ec2988a3d5c67a6350732595c0a97"
   },
   "source": [
    "## 1.4 NMF LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "551925fdc6ba4258b3842948a3a894b3c5835846",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def nmf_lda_feature(train, test, train_text, test_text):\n",
    "    tfv = TfidfVectorizer(min_df=3,  max_features=10000,\n",
    "        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "        stop_words = 'english')\n",
    "    # Fit TFIDF\n",
    "    tfv.fit(list(train_text)+list(test_text))\n",
    "    X =  tfv.transform(train_text)\n",
    "    X_test = tfv.transform(test_text)\n",
    "\n",
    "    # nmf\n",
    "    components = 20\n",
    "#     scaler = MinMaxScaler()\n",
    "    # X = scaler.fit_transform(X)\n",
    "#     nmf = NMF(n_components=components, random_state=100).fit(np.vstack([X, X_test]))\n",
    "    nmf = NMF(n_components=components, random_state=100).fit(X)\n",
    "    nmf_x = nmf.transform(X)\n",
    "    nmf_x = pd.DataFrame(nmf_x, columns=['nmf_{}'.format(i) for i in range(components)])\n",
    "    train = pd.concat((train, nmf_x), axis=1)\n",
    "    nmf_x_test = nmf.transform(X_test)\n",
    "    nmf_x_test = pd.DataFrame(nmf_x_test, columns=['nmf_{}'.format(i) for i in range(components)])\n",
    "    test = pd.concat((test, nmf_x_test), axis=1)\n",
    "\n",
    "    # lda\n",
    "    components = 12\n",
    "#     lda = LatentDirichletAllocation(n_components=components, max_iter=10, n_jobs=-1)\n",
    "    lda = LatentDirichletAllocation(n_components=components, max_iter=120, n_jobs=-1)\n",
    "    lda.fit(X)\n",
    "    lda_x = lda.transform(X)\n",
    "    lda_x = pd.DataFrame(lda_x, columns=['lda_{}'.format(i) for i in range(components)])\n",
    "    train = pd.concat((train, lda_x), axis=1)\n",
    "    lda_x_test = lda.transform(X_test)\n",
    "    lda_x_test = pd.DataFrame(lda_x_test, columns=['lda_{}'.format(i) for i in range(components)])\n",
    "    test = pd.concat((test, lda_x_test), axis=1)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train_text = obtain_text(train)\n",
    "test_text = obtain_text(test)\n",
    "\n",
    "train, test = nmf_lda_feature(train, test, train_text, test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "485b57a71ef900c8afbc846cda8ce876c29eb29c"
   },
   "source": [
    "## 1.7 image feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "71d11940693df2c331d7371f8a8e5009fbd02332"
   },
   "source": [
    "### 1.7.1 image meta_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "5dae812f463ae3568f845e953d8a58b3ae06c175"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\n",
    "test_df = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\n",
    "test_pet_ids = test_df['PetID'].values\n",
    "train_pet_ids = train_df['PetID'].values\n",
    "target = train_df['AdoptionSpeed'].values\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D,Dense,Dropout\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.densenet import preprocess_input, DenseNet121\n",
    "from keras.applications.resnet50 import preprocess_input as res_preprocess, ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "696082194e0962116d539bcbae005ee814efb3aa"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "def BASE_MODEL():\n",
    "    inp = Input((128,128,3))\n",
    "    backbone = ResNet50(input_tensor = inp, \n",
    "                           weights=\"../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\",\n",
    "                           include_top = False)\n",
    "    x = backbone.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(1,activation='linear')(x)\n",
    "    return Model(inp,output)\n",
    "\n",
    "def new_load_image(path, pet_id):\n",
    "    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n",
    "    try:\n",
    "        new_image = cv2.resize(image,(128,128))\n",
    "    except:\n",
    "        new_image = np.zeros((128,128,3))\n",
    "    new_image = res_preprocess(new_image)\n",
    "    return new_image\n",
    "\n",
    "#base_model = BASE_MODEL()\n",
    "#model.summary()\n",
    "def train_gen(batch_size=128,shuffle=True,pet_list=None,pet_labels=None,use_labels=True):\n",
    "    images_df = pd.DataFrame({'img_id':pet_list,'label':pet_labels})\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            images_df = images_df.sample(frac=1.0).reset_index(drop=True)\n",
    "        for start in range(0, len(images_df), batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size,len(images_df))\n",
    "            for _id in range(start,end):\n",
    "                image_row = images_df.iloc[_id]\n",
    "                image_id = image_row['img_id']\n",
    "                img = new_load_image(\"../input/petfinder-adoption-prediction/train_images/\", image_id)\n",
    "                if use_labels:\n",
    "                    img_label = image_row['label']\n",
    "                    y_batch.append(img_label)\n",
    "                else:\n",
    "                    y_batch.append(-1.0)\n",
    "                x_batch.append(img)\n",
    "            yield np.array(x_batch),np.array(y_batch)\n",
    "            \n",
    "def test_gen(batch_size=128,shuffle=True,pet_list=None,pet_labels=None,use_labels=True):\n",
    "    images_df = pd.DataFrame({'img_id':pet_list,'label':pet_labels})\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            images_df = images_df.sample(frac=1.0).reset_index(drop=True)\n",
    "        for start in range(0, len(images_df), batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size,len(images_df))\n",
    "            for _id in range(start,end):\n",
    "                image_row = images_df.iloc[_id]\n",
    "                image_id = image_row['img_id']\n",
    "                img = new_load_image(\"../input/petfinder-adoption-prediction/test_images/\", image_id)\n",
    "                if use_labels:\n",
    "                    img_label = image_row['label']\n",
    "                    y_batch.append(img_label)\n",
    "                else:\n",
    "                    y_batch.append(-1.0)\n",
    "                x_batch.append(img)\n",
    "            yield np.array(x_batch),np.array(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "75e03ad05db1381cac084c431b2be0b1115133c9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11242 3751\n",
      "Epoch 1/3\n",
      "88/88 [==============================] - 83s 947ms/step - loss: 8.1142 - val_loss: 1.5222\n",
      "Epoch 2/3\n",
      "88/88 [==============================] - 59s 666ms/step - loss: 1.3985 - val_loss: 1.4866\n",
      "Epoch 3/3\n",
      "88/88 [==============================] - 59s 667ms/step - loss: 1.2784 - val_loss: 1.3842\n",
      "11244 3749\n",
      "Epoch 1/3\n",
      "88/88 [==============================] - 75s 857ms/step - loss: 6.7756 - val_loss: 1.7951\n",
      "Epoch 2/3\n",
      "88/88 [==============================] - 58s 657ms/step - loss: 1.4117 - val_loss: 1.4925\n",
      "Epoch 3/3\n",
      "88/88 [==============================] - 59s 666ms/step - loss: 1.2819 - val_loss: 1.4262\n",
      "11246 3747\n",
      "Epoch 1/3\n",
      "88/88 [==============================] - 78s 886ms/step - loss: 7.7145 - val_loss: 1.8618\n",
      "Epoch 2/3\n",
      "88/88 [==============================] - 57s 652ms/step - loss: 1.3814 - val_loss: 1.3702\n",
      "Epoch 3/3\n",
      "88/88 [==============================] - 57s 652ms/step - loss: 1.2302 - val_loss: 1.4522\n",
      "11247 3746\n",
      "Epoch 1/3\n",
      "88/88 [==============================] - 84s 950ms/step - loss: 10.7627 - val_loss: 1.9395\n",
      "Epoch 2/3\n",
      "88/88 [==============================] - 64s 727ms/step - loss: 1.4188 - val_loss: 1.4393\n",
      "Epoch 3/3\n",
      "88/88 [==============================] - 59s 674ms/step - loss: 1.1816 - val_loss: 1.4358\n"
     ]
    }
   ],
   "source": [
    "test_img_prob = np.zeros(shape=(test_df.shape[0],1))\n",
    "train_img_prob = np.zeros(shape=(train_df.shape[0],1))\n",
    "for tr_idx,te_idx in FOLDS.split(train_pet_ids,\n",
    "                           target):\n",
    "    print(len(tr_idx),len(te_idx))\n",
    "    gen_tr = train_gen(batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    pet_list=train_pet_ids[tr_idx],\n",
    "                    pet_labels=target[tr_idx])\n",
    "    \n",
    "    gen_te = train_gen(batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    pet_list=train_pet_ids[te_idx],\n",
    "                    pet_labels=target[te_idx])\n",
    "    gen_test = test_gen(batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    pet_list=test_pet_ids,\n",
    "                    pet_labels=None,\n",
    "                    use_labels=False)\n",
    "    model = BASE_MODEL()\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mse')\n",
    "    model.fit_generator(gen_tr,\n",
    "                       steps_per_epoch=int(np.ceil(len(tr_idx)*1.0/batch_size)),\n",
    "                       epochs=3,verbose=1,\n",
    "                       validation_data=gen_te,\n",
    "                       validation_steps=int(np.ceil(len(te_idx)*1.0/batch_size)),\n",
    "                       )\n",
    "    _test_prob = model.predict_generator(gen_test,\n",
    "                                         steps=int(np.ceil(len(test_df)*1.0/(batch_size))),\n",
    "                                        )\n",
    "    _val_prob = model.predict_generator(gen_te,                                         \n",
    "                              steps=int(np.ceil(len(te_idx)*1.0/(batch_size))),\n",
    "                             )\n",
    "    train_img_prob[te_idx,:] = _val_prob \n",
    "    test_img_prob += _test_prob\n",
    "\n",
    "    \n",
    "test_img_prob /= N_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "c13634ed4a2f319218bc6c14bcef986427f7b45a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d760949a337f483187a28109dafe1c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "img_size = 256\n",
    "batch_size = 16\n",
    "\n",
    "train_df = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\n",
    "pet_ids = train_df['PetID'].values\n",
    "n_batches = len(pet_ids) // batch_size + 1\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\n",
    "import keras.backend as K\n",
    "inp = Input((256,256,3))\n",
    "backbone = DenseNet121(input_tensor = inp, \n",
    "                       weights=\"../input/densenet-keras/DenseNet-BC-121-32-no-top.h5\",\n",
    "                       include_top = False)\n",
    "x = backbone.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\n",
    "x = AveragePooling1D(4)(x)\n",
    "out = Lambda(lambda x: x[:,:,0])(x)\n",
    "\n",
    "m = Model(inp,out)\n",
    "\n",
    "features = {}\n",
    "for b in tqdm_notebook(range(n_batches)):\n",
    "    start = b*batch_size\n",
    "    end = (b+1)*batch_size\n",
    "    batch_pets = pet_ids[start:end]\n",
    "    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n",
    "    for i,pet_id in enumerate(batch_pets):\n",
    "        try:\n",
    "            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/train_images/\", pet_id)\n",
    "        except:\n",
    "            pass\n",
    "    batch_preds = m.predict(batch_images)\n",
    "    for i,pet_id in enumerate(batch_pets):\n",
    "        features[pet_id] = batch_preds[i]\n",
    "\n",
    "train_feats = pd.DataFrame.from_dict(features, orient='index')\n",
    "train_feats.columns = ['pic_'+str(i) for i in range(train_feats.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "d363871abb9624583f262467f403a6fccbe3e269"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415b89b7f4534fbaafe1013212c5b4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=247), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\n",
    "\n",
    "pet_ids = test_df['PetID'].values\n",
    "n_batches = len(pet_ids) // batch_size + 1\n",
    "\n",
    "features = {}\n",
    "for b in tqdm_notebook(range(n_batches)):\n",
    "    start = b*batch_size\n",
    "    end = (b+1)*batch_size\n",
    "    batch_pets = pet_ids[start:end]\n",
    "    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n",
    "    for i,pet_id in enumerate(batch_pets):\n",
    "        try:\n",
    "            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/test_images/\", pet_id)\n",
    "        except:\n",
    "            pass\n",
    "    batch_preds = m.predict(batch_images)\n",
    "    for i,pet_id in enumerate(batch_pets):\n",
    "        features[pet_id] = batch_preds[i]\n",
    "        \n",
    "test_feats = pd.DataFrame.from_dict(features, orient='index')\n",
    "test_feats.columns = ['pic_'+str(i) for i in range(test_feats.shape[1])]\n",
    "\n",
    "test_feats = test_feats.reset_index()\n",
    "test_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n",
    "\n",
    "train_feats = train_feats.reset_index()\n",
    "train_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n",
    "\n",
    "test_feats.head()\n",
    "train = pd.merge(train, train_feats, how='left', on='PetID')\n",
    "test = pd.merge(test, test_feats, how='left', on='PetID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "31147ec092e6204ca2faa85278829a760246f87c"
   },
   "source": [
    "## 第二份特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "eecdb60d031a60b11f33672924537ea024d7033c"
   },
   "outputs": [],
   "source": [
    "breed_id_map = dict(zip(breeds.BreedID.values,breeds.BreedName.values))\n",
    "breed_type_map = dict(zip(breeds.BreedID.values,breeds.Type.values))\n",
    "color_id_map = dict(zip(colors.ColorID.values,colors.ColorName.values))\n",
    "\n",
    "train['Breed1_text'] = train['Breed1'].map(lambda x:breed_id_map.get(x,'UNK_Breed1'))\n",
    "train['Breed2_text'] = train['Breed2'].map(lambda x:breed_id_map.get(x,'UNK_Breed2'))\n",
    "train['Color1_text'] = train['Color1'].map(lambda x:color_id_map.get(x,'UNK_Color1'))\n",
    "train['Color2_text'] = train['Color2'].map(lambda x:color_id_map.get(x,'UNK_Color2'))\n",
    "train['Color3_text'] = train['Color3'].map(lambda x:color_id_map.get(x,'UNK_Color3'))\n",
    "\n",
    "test['Breed1_text'] = test['Breed1'].map(lambda x:breed_id_map.get(x,'UNK_Breed1'))\n",
    "test['Breed2_text'] = test['Breed2'].map(lambda x:breed_id_map.get(x,'UNK_Breed2'))\n",
    "test['Color1_text'] = test['Color1'].map(lambda x:color_id_map.get(x,'UNK_Color1'))\n",
    "test['Color2_text'] = test['Color2'].map(lambda x:color_id_map.get(x,'UNK_Color2'))\n",
    "test['Color3_text'] = test['Color3'].map(lambda x:color_id_map.get(x,'UNK_Color3'))\n",
    "\n",
    "train['raw_text'] =  train['Name'] + ' ' \\\n",
    "                    + train['Breed1_text'] + ' ' + train['Breed2_text'] + ' ' \\\n",
    "                    + train['Color1_text'] + ' ' + train['Color2_text'] + ' ' \\\n",
    "                    + train['Color3_text'] + ' ' \\\n",
    "                    + train['Description']\n",
    "\n",
    "test['raw_text'] =  test['Name'] + ' ' \\\n",
    "                    + test['Breed1_text'] + ' ' + test['Breed2_text'] + ' ' \\\n",
    "                    + test['Color1_text'] + ' ' + test['Color2_text'] + ' ' \\\n",
    "                    + test['Color3_text'] + ' ' \\\n",
    "                    + test['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "bd8218674ed4d80dba5b749a7d5570f578152c49"
   },
   "outputs": [],
   "source": [
    "gzf_prefix = 'gzf_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "1dd735332ac57a6c1a8c786c2fe2e56c91bbcfce"
   },
   "outputs": [],
   "source": [
    "train[gzf_prefix+'RescureID_rank'] = train.RescuerID.map(train.RescuerID.value_counts().rank()/train.RescuerID.unique().shape[0])\n",
    "train[gzf_prefix+'Description_len'] = train.Description.map(lambda x:len(x) if type(x)!=float else 0)\n",
    "train[gzf_prefix+'Description_word_len'] = train.Description.map(lambda x:len(x.strip().split()) if type(x)!=float else 0)\n",
    "train[gzf_prefix+'Description_distinct_word_len'] = train.Description.map(lambda x:len(set(x.lower().strip().split())) if type(x)!=float else 0)\n",
    "train[gzf_prefix+'Description_distinct_word_ratio'] = train[gzf_prefix+'Description_distinct_word_len'] / (train[gzf_prefix+'Description_word_len'] + 1.0)\n",
    "\n",
    "test[gzf_prefix+'RescureID_rank'] = test.RescuerID.map(test.RescuerID.value_counts().rank()/test.RescuerID.unique().shape[0])\n",
    "test[gzf_prefix+'Description_len'] = test.Description.map(lambda x:len(x) if type(x)!=float else 0)\n",
    "test[gzf_prefix+'Description_word_len'] = test.Description.map(lambda x:len(x.strip().split()) if type(x)!=float else 0)\n",
    "test[gzf_prefix+'Description_distinct_word_len'] = test.Description.map(lambda x:len(set(x.lower().strip().split())) if type(x)!=float else 0)\n",
    "test[gzf_prefix+'Description_distinct_word_ratio'] = test[gzf_prefix+'Description_distinct_word_len'] / (test[gzf_prefix+'Description_word_len'] + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "9d128c1201b2c3ebe59983668364f75cecf48d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 468) (3948, 467)\n",
      "(18941, 468)\n"
     ]
    }
   ],
   "source": [
    "X = pd.concat([train,test],axis=0,ignore_index=True)\n",
    "len_train = len(train)\n",
    "print(train.shape, test.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "2abeea315ff576c2abfdecbb2a7bbf40015a032d"
   },
   "outputs": [],
   "source": [
    "X[gzf_prefix+'is_pure'] = ((X.Breed1!=307) & (X.Breed2!=307) & (X.Breed2!=0)).astype(float)\n",
    "X[gzf_prefix+'is_pure_breed1'] = (X.Breed1!=307).astype(float)\n",
    "X[gzf_prefix+'is_pure_breed2'] = ((X.Breed2!=307) & (X.Breed2!=0)).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "41c5b3b02db665003a48bd99b7f3d27a718955c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18941, 481)\n"
     ]
    }
   ],
   "source": [
    "agg_num_feature = ['Age','Health','PhotoAmt','Quantity',\n",
    "                   'doc_sent_mag', 'doc_sent_score', \n",
    "                   'meta_dominant_score', 'meta_label_score',gzf_prefix+'Description_len']\n",
    "agg_rescureid_1 = X.groupby(['RescuerID'])[agg_num_feature].mean()\n",
    "agg_rescureid_1.columns = ['Age_id','Health_id','PhotoAmt_id','Quantity_id',\n",
    "                   'doc_sent_mag_id', 'doc_sent_score_id', \n",
    "                   'dominant_score_id', 'label_score_id','Description_len_id']\n",
    "agg_rescureid_2 = X.groupby(['RescuerID'])['Breed1'].aggregate({'307_ratio':lambda x:(x==307).mean()})\n",
    "agg_rescureid = pd.concat([agg_rescureid_1,agg_rescureid_2],axis=1)\n",
    "agg_rescureid.columns = [gzf_prefix+x for x in agg_rescureid.columns ]\n",
    "X = pd.merge(X,agg_rescureid,left_on='RescuerID',right_index=True,how='left')\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "e206c257054a213edb8184d20a40fd86476c2fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (tfidf): (18941, 10000)\n",
      "X (svd): (18941, 120)\n",
      "X: (18941, 601)\n",
      "X (nmf): (18941, 20)\n",
      "X: (18941, 621)\n",
      "X (lda): (18941, 12)\n",
      "X: (18941, 633)\n"
     ]
    }
   ],
   "source": [
    "SVD_FEATURES = 120\n",
    "NMF_FEATURES = 20\n",
    "LDA_FEATURES = 12\n",
    "\n",
    "desc = X.raw_text.fillna(\"none\").values\n",
    "tfidf = TfidfVectorizer(min_df=3,  max_features=10000,\n",
    "        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "        stop_words = 'english')\n",
    "    \n",
    "# Fit TFIDF\n",
    "X_tfidf = tfidf.fit_transform(list(desc))\n",
    "print(\"X (tfidf):\", X_tfidf.shape)\n",
    "\n",
    "svd = TruncatedSVD(n_components=SVD_FEATURES)\n",
    "svd.fit(X_tfidf)\n",
    "X_svd = svd.fit_transform(X_tfidf)\n",
    "print(\"X (svd):\", X_svd.shape)\n",
    "\n",
    "X_svd = pd.DataFrame(X_svd, columns=[gzf_prefix+'sdv_{}'.format(i) for i in range(SVD_FEATURES)])\n",
    "X = pd.concat((X, X_svd), axis=1)\n",
    "print(\"X:\", X.shape)\n",
    "\n",
    "nmf = NMF(n_components=NMF_FEATURES)\n",
    "nmf.fit(X_tfidf)\n",
    "X_nmf = nmf.fit_transform(X_tfidf)\n",
    "print(\"X (nmf):\", X_nmf.shape)\n",
    "\n",
    "X_nmf = pd.DataFrame(X_nmf, columns=[gzf_prefix+'mnf_{}'.format(i) for i in range(NMF_FEATURES)])\n",
    "X = pd.concat((X, X_nmf), axis=1)\n",
    "print(\"X:\", X.shape)\n",
    "\n",
    "# take a long time here\n",
    "# lda = LatentDirichletAllocation(n_components=LDA_FEATURES, n_jobs=-1,max_iter=10)\n",
    "lda = LatentDirichletAllocation(n_components=LDA_FEATURES, n_jobs=-1,max_iter=120)\n",
    "lda.fit(X_tfidf)\n",
    "X_lda = lda.fit_transform(X_tfidf)\n",
    "print(\"X (lda):\", X_lda.shape)\n",
    "\n",
    "X_lda = pd.DataFrame(X_lda, columns=[gzf_prefix+'lad_{}'.format(i) for i in range(LDA_FEATURES)])\n",
    "X = pd.concat((X, X_lda), axis=1)\n",
    "print(\"X:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "6e0e91b04f806a9a92eae2cb3a9ec80fcd0ed71f"
   },
   "outputs": [],
   "source": [
    "cat_cols = ['Health',\n",
    " 'Breed1', 'Breed2',\n",
    " 'Type', 'Gender',\n",
    " 'Color3', 'Color2', 'Color1',\n",
    " 'Vaccinated','Sterilized',  'Dewormed',\n",
    " 'MaturitySize', 'FurLength',\n",
    " 'State','meta_label_description','meta_label_description1','meta_label_description2']\n",
    "X.loc[:, cat_cols] = X[cat_cols].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "37ae613cab00f1345db46aacf797e1d8da3a0a5a"
   },
   "outputs": [],
   "source": [
    "# get the categorical features\n",
    "foo = train.dtypes\n",
    "cat_feature_names = foo[foo == \"category\"].index.values\n",
    "cat_features = [i for i in range(X.shape[1]) if X.columns[i] in cat_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "c6ae29181c8a582e48371b7bf100c46fbf30ffd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14993, 633), (14993,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = X[:len_train]\n",
    "test = X[len_train:]\n",
    "train.index = range(len_train)\n",
    "test.index = range(test.shape[0])\n",
    "\n",
    "target = train['AdoptionSpeed']\n",
    "rescue_id = train['RescuerID']\n",
    "\n",
    "train.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "63ab7c50d5c6826311c613dcb800cba7e1242080"
   },
   "source": [
    "## train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "da8c42ee4530d5f11f4247fe8d2d9b39d772952b"
   },
   "outputs": [],
   "source": [
    "# def process_category_feature(train, test, intercept=300):\n",
    "#     cat_cols = [col for col in train.columns if train[col].dtype.name == 'category']   \n",
    "#     cat_feature_train = pd.get_dummies(train[cat_cols], columns=cat_cols)        \n",
    "#     cat_feature_test = pd.get_dummies(test[cat_cols], columns=cat_cols)    \n",
    "    \n",
    "#     pick_col_df = cat_feature_train.sum(axis=0)\n",
    "#     pick_col_df = pick_col_df[pick_col_df > intercept]\n",
    "#     pick_cols = list(pick_col_df.index)\n",
    "    \n",
    "#     dummy_train = pd.concat([train.drop(cat_cols, axis=1), cat_feature_train[pick_cols]], axis=1)\n",
    "#     dummy_test = pd.concat([test.drop(cat_cols, axis=1), cat_feature_test[pick_cols]], axis=1)\n",
    "#     print (\"dummy:\", dummy_train.shape, dummy_test.shape)\n",
    "#     return dummy_train, dummy_test\n",
    "\n",
    "def obtain_train_mse_and_kappa(train_predictions, target):\n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(train_predictions, target)\n",
    "    coefficients_ = optR.coefficients()\n",
    "    rmse_score1 = rmse(target, train_predictions)\n",
    "    train_predictions = optR.predict(train_predictions, optR.coefficients()).astype(int)\n",
    "    qwk_score = quadratic_weighted_kappa(target, train_predictions)\n",
    "    rmse_score2 = rmse(target, train_predictions)\n",
    "    \n",
    "    return rmse_score1, rmse_score2, qwk_score\n",
    "\n",
    "def run_cv_model(train, test, target, weight, model_fn, params={}, eval_fn=None, label='model'):\n",
    "    kf = FOLDS\n",
    "    n_splits = N_FOLDS\n",
    "    \n",
    "    fold_splits = kf.split(train, target)\n",
    "    cv_scores = []\n",
    "    qwk_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros((train.shape[0], n_splits))\n",
    "    pred_test = np.zeros((origin_test.shape[0], n_splits))\n",
    "    \n",
    "    all_coefficients = np.zeros((n_splits, 4))\n",
    "    i = 1\n",
    "    for dev_index, val_index in fold_splits:\n",
    "        print('Started ' + label + ' fold ' + str(i) + '/{}'.format(n_splits))\n",
    "        if isinstance(train, pd.DataFrame):\n",
    "            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n",
    "            dev_y, val_y = target[dev_index], target[val_index]\n",
    "            dev_weight, val_weight = weight[dev_index], weight[val_index]\n",
    "        else:\n",
    "            dev_X, val_X = train[dev_index], train[val_index]\n",
    "            dev_y, val_y = target[dev_index], target[val_index]\n",
    "            dev_weight, val_weight = weight[dev_index], weight[val_index]\n",
    "            \n",
    "        params2 = params.copy()\n",
    "        pred_val_y, pred_test_y, importances, coefficients, qwk = model_fn(dev_X, dev_y, val_X, val_y, dev_weight, val_weight, test, params2)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index] = pred_val_y\n",
    "        pred_test[:, i-1] = pred_test_y.reshape(-1)\n",
    "        \n",
    "        all_coefficients[i-1, :] = coefficients\n",
    "        if eval_fn is not None:\n",
    "            cv_score = eval_fn(val_y, pred_val_y)\n",
    "            cv_scores.append(cv_score)\n",
    "            qwk_scores.append(qwk)\n",
    "            print(label + ' cv score {}: RMSE {} QWK {}'.format(i, cv_score, qwk))\n",
    "        i += 1\n",
    "    train_rmse1,  train_rmse2, train_qwk = obtain_train_mse_and_kappa([r[0] for r in pred_train], target)\n",
    "    print('{} cv RMSE scores : {}'.format(label, cv_scores))\n",
    "    print('{} cv mean        RMSE score : {}'.format(label, np.mean(cv_scores)))\n",
    "    print('{} cv recalculate RMSE1 score : {}'.format(label, train_rmse1))\n",
    "    print('{} cv recalculate RMSE2 score : {}'.format(label, train_rmse2))\n",
    "    print('{} cv std RMSE score : {}'.format(label, np.std(cv_scores)))\n",
    "    print('{} cv QWK scores : {}'.format(label, qwk_scores))\n",
    "    print('{} cv mean        QWK score : {}'.format(label, np.mean(qwk_scores)))\n",
    "    print('{} cv recalculate QWK score : {}'.format(label, train_qwk))\n",
    "    print('{} cv std QWK score : {}'.format(label, np.std(qwk_scores)))\n",
    "    pred_full_test = pred_full_test / float(n_splits)\n",
    "    results = {'label': label,\n",
    "               'train': pred_train, 'test': pred_full_test, 'test_value':pred_test,\n",
    "                'cv': cv_scores, 'qwk': qwk_scores,\n",
    "               'coefficients': all_coefficients}\n",
    "    return results\n",
    "\n",
    "def runLGB(train_X, train_y, test_X, test_y, dev_weight, val_weight, test_X2, params):\n",
    "    print('Prep LGB')\n",
    "\n",
    "    d_train = lgb.Dataset(train_X, label=train_y, weight=dev_weight)\n",
    "    d_valid = lgb.Dataset(test_X, label=test_y, weight=val_weight)\n",
    "    watchlist = [d_train, d_valid]\n",
    "    print('Train LGB')\n",
    "    num_rounds = params.pop('num_rounds')\n",
    "    verbose_eval = params.pop('verbose_eval')\n",
    "    early_stop = None\n",
    "    if params.get('early_stop'):\n",
    "        early_stop = params.pop('early_stop')\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "                      verbose_eval=verbose_eval,\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "    importances = model.feature_importance()\n",
    "    optR = OptimizedRounder_v3()\n",
    "    len_0 = test_y[test_y==0].shape[0]\n",
    "    optR.fit(pred_test_y, test_y)\n",
    "    coefficients = optR.coefficients()\n",
    "    pred_test_y_k = optR.predict(pred_test_y, coefficients, len_0)\n",
    "    print(\"Valid Counts = \", Counter(test_y))\n",
    "    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "    print(\"Coefficients = \", coefficients)\n",
    "    qwk = quadratic_weighted_kappa(test_y, pred_test_y_k)\n",
    "    print(\"QWK = \", qwk)\n",
    "    print('Predict 2/2')\n",
    "    return np.array(pred_test_y).reshape(-1, 1), np.array(pred_test_y2).reshape(-1, 1), importances, coefficients, qwk\n",
    "\n",
    "# def runXGB(train_X, train_y, test_X, test_y, dev_weight, val_weight, test_X2, params):\n",
    "#     print('Prep XGB')\n",
    "#     d_train = xgb.DMatrix(train_X, label=train_y, weight=dev_weight)\n",
    "#     d_valid = xgb.DMatrix(test_X, label=test_y, weight=val_weight)\n",
    "#     d_test = xgb.DMatrix(test_X2)\n",
    "#     watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "#     print('Train XGB')\n",
    "#     num_rounds = params['boost_num']\n",
    "#     verbose_eval = params['verbose_eval']\n",
    "#     early_stop = params['early_stop']\n",
    "#     obj = None\n",
    "#     if 'obj' in params.keys():\n",
    "#         obj = params['obj']\n",
    "#         model = xgb.train(params,\n",
    "#                           dtrain=d_train,\n",
    "#                           num_boost_round=num_rounds,\n",
    "#                           evals=watchlist,\n",
    "#                           verbose_eval=verbose_eval,\n",
    "#                           early_stopping_rounds=early_stop,\n",
    "#                           obj=obj)\n",
    "#         print('Predict 1/2')\n",
    "#         pred_test_y = softmax(model.predict(d_valid)).argmax(axis=1)\n",
    "#         print (pred_test_y)\n",
    "#         pred_test_y2 = softmax(model.predict(d_test)).argmax(axis=1)\n",
    "#     else:\n",
    "#         model = xgb.train(params,\n",
    "#                           dtrain=d_train,\n",
    "#                           num_boost_round=num_rounds,\n",
    "#                           evals=watchlist,\n",
    "#                           verbose_eval=verbose_eval,\n",
    "#                           early_stopping_rounds=early_stop)\n",
    "#         print('Predict 1/2')\n",
    "#         pred_test_y = model.predict(d_valid)\n",
    "#         pred_test_y2 = model.predict(d_test)\n",
    "\n",
    "#     importances = [(x, model.get_score()[x]) for x in train_X.columns if x in model.get_score().keys()]\n",
    "#     importances = [[x[0] for x in importances], [x[1] for x in importances]]\n",
    "#     optR = OptimizedRounder()\n",
    "#     len_0 = test_y[test_y==0].shape[0]\n",
    "#     optR.fit(pred_test_y, test_y)\n",
    "#     coefficients = optR.coefficients()\n",
    "#     pred_test_y_k = optR.predict(pred_test_y, coefficients, len_0)\n",
    "#     print(\"Valid Counts = \", Counter(test_y))\n",
    "#     print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "#     print(\"Coefficients = \", coefficients)\n",
    "#     qwk = quadratic_weighted_kappa(test_y, pred_test_y_k)\n",
    "#     print(\"QWK = \", qwk)\n",
    "#     print('Predict 2/2')\n",
    "#     return np.array(pred_test_y).reshape(-1, 1), np.array(pred_test_y2).reshape(-1, 1), importances, coefficients, qwk\n",
    "\n",
    "# def runCGB(train_X, train_y, test_X, test_y, dev_weight, val_weight, test_X2, params):\n",
    "#     print('Prep CGB')\n",
    "#     global cat_features\n",
    "#     watchlist = (test_X, test_y)\n",
    "#     print('Train CGB')\n",
    "#     verbose_eval = params.pop('verbose_eval')\n",
    "#     early_stop = None\n",
    "#     if params.get('early_stop'):\n",
    "#         early_stop = params.pop('early_stop')\n",
    "        \n",
    "#     model = cgb.CatBoostRegressor(cat_features=list(cat_features), **params)\n",
    "#     model.fit(train_X, train_y, eval_set=watchlist, verbose=verbose_eval)\n",
    "        \n",
    "#     print('Predict 1/2')\n",
    "#     pred_test_y = model.predict(test_X, ntree_start=0, ntree_end=model.get_best_iteration())\n",
    "#     pred_test_y2 = model.predict(test_X2, ntree_start=0, ntree_end=model.get_best_iteration())\n",
    "#     importances = model.get_feature_importance()\n",
    "#     optR = OptimizedRounder()\n",
    "#     len_0 = test_y[test_y==0].shape[0]\n",
    "#     optR.fit(pred_test_y, test_y)\n",
    "#     coefficients = optR.coefficients()\n",
    "#     pred_test_y_k = optR.predict(pred_test_y, coefficients, len_0)\n",
    "#     print(\"Valid Counts = \", Counter(test_y))\n",
    "#     print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "#     print(\"Coefficients = \", coefficients)\n",
    "#     qwk = quadratic_weighted_kappa(test_y, pred_test_y_k)\n",
    "#     print(\"QWK = \", qwk)\n",
    "#     print('Predict 2/2')\n",
    "#     return np.array(pred_test_y).reshape(-1, 1), np.array(pred_test_y2).reshape(-1, 1), importances, coefficients, qwk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "98d67a5b617ef54ffd60f437c6acce75f41e6432"
   },
   "outputs": [],
   "source": [
    "def get_cols(totals, prefixs):\n",
    "    if isinstance(prefixs, list):\n",
    "        cols = []\n",
    "        for prefix in prefixs:\n",
    "            cols += [col for col in totals if col.find(prefix) > -1]\n",
    "        return cols\n",
    "    else:\n",
    "        return [col for col in totals if col.find(prefixs) > -1]\n",
    "\n",
    "origin_cols = [\n",
    "    \"Type\",\"Age\",\n",
    "    \"Breed1\",\"Breed2\",\"Gender\",\n",
    "    \"Color1\",\"Color2\",\"Color3\",\n",
    "    \"MaturitySize\",\"FurLength\",\n",
    "    \"Vaccinated\",\"Dewormed\",\"Sterilized\",\"Health\",\n",
    "    \"Quantity\",\"Fee\",\"State\",\n",
    "    \"VideoAmt\",\"PhotoAmt\"\n",
    "]\n",
    "\n",
    "doc_cols = get_cols(train.columns, 'doc_')\n",
    "meta_cols = get_cols(train.columns, 'meta_')\n",
    "pure_cols = get_cols(train.columns, 'pure_')\n",
    "rescue_cols = get_cols(train.columns, 'rescue_')\n",
    "lang_cols = get_cols(train.columns, 'lang_')\n",
    "sml_cols = get_cols(train.columns, ['svd_', 'lda_', 'nmf_'])\n",
    "pic_cols = get_cols(train.columns, 'pic_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "3d600d917d061d2db2cd7d738d57683fd0456802"
   },
   "outputs": [],
   "source": [
    "train['ResNet_meta'] = train_img_prob.flatten()\n",
    "test['ResNet_meta'] = test_img_prob.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "3efb23cf9be8789f7b0adb3676225526cadeb4de"
   },
   "outputs": [],
   "source": [
    "gzf_cols = doc_cols + lang_cols + origin_cols + pic_cols + [\n",
    "                'meta_dominant_blue', 'meta_dominant_green','meta_dominant_pixel_frac', \n",
    "                 'meta_dominant_red', 'meta_dominant_score', 'meta_label_score', \n",
    "                'meta_vertex_x', 'meta_vertex_y'] + [\n",
    "                gzf_prefix+'RescureID_rank',gzf_prefix+'Description_len',\n",
    "                gzf_prefix+'Description_word_len',gzf_prefix+'Description_distinct_word_len',\n",
    "                gzf_prefix+'Description_distinct_word_ratio',\n",
    "                gzf_prefix+'is_pure',gzf_prefix+'is_pure_breed1',gzf_prefix+'is_pure_breed2',\n",
    "                gzf_prefix+'Quantity_id',gzf_prefix+'307_ratio'\n",
    "                ] + [gzf_prefix+'sdv_{}'.format(i) for i in range(SVD_FEATURES)] \\\n",
    "                  + [gzf_prefix+'mnf_{}'.format(i) for i in range(NMF_FEATURES)] \\\n",
    "                  + [gzf_prefix+'lad_{}'.format(i) for i in range(LDA_FEATURES)] \\\n",
    "                  + ['ResNet_meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "b5c92d3ca8b7bd764b90107366956110b424348a"
   },
   "outputs": [],
   "source": [
    "zkr_cols = origin_cols+doc_cols+meta_cols+pure_cols+rescue_cols+lang_cols+sml_cols+pic_cols+['ResNet_meta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ae8178d84b542125fbd0c9a8eed69fbe139d838"
   },
   "source": [
    "## SAVE !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "7486217ec895cdef8991f9b3c025f81d12881afe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 452) (3948, 452) (14993, 457) (3948, 457)\n"
     ]
    }
   ],
   "source": [
    "train_gzf = train[gzf_cols]\n",
    "test_gzf = test[gzf_cols]\n",
    "\n",
    "train_zkr = train[zkr_cols]\n",
    "test_zkr = test[zkr_cols]\n",
    "\n",
    "train_gzf.to_csv(\"train_gzf.csv\", index=False)\n",
    "test_gzf.to_csv(\"test_gzf.csv\", index=False)\n",
    "\n",
    "train_zkr.to_csv(\"train_zkr.csv\", index=False)\n",
    "test_zkr.to_csv(\"test_zkr.csv\", index=False)\n",
    "\n",
    "print(train_gzf.shape, test_gzf.shape, train_zkr.shape, test_zkr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dfd60364ccac76bc4ed787002a40de6cfeb7f2b7"
   },
   "source": [
    "## 这里应该加入 LR ETC 等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "209bba534fbc192ece6b2701ac8712192600ca1f"
   },
   "source": [
    "**## MLP, not work for ridge stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "a9db487da753821ad1d1decc626e369285d62cab"
   },
   "outputs": [],
   "source": [
    "cat_cols = ['Health',\n",
    " 'Breed1', 'Breed2',\n",
    " 'Type', 'Gender',\n",
    " 'Color3', 'Color2', 'Color1',\n",
    " 'Vaccinated','Sterilized',  'Dewormed',\n",
    " 'MaturitySize', 'FurLength',\n",
    " 'State','meta_label_description','meta_label_description1','meta_label_description2']\n",
    "zkr_numerical_cols = [item for item in zkr_cols if item not in cat_cols]\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "ss = StandardScaler()\n",
    "ss.fit(train_zkr[zkr_numerical_cols].astype(float))\n",
    "train_zkr_numerical_ss = ss.transform(train_zkr[zkr_numerical_cols].astype(float))\n",
    "test_zkr_numerical_ss = ss.transform(test_zkr[zkr_numerical_cols].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "90db5875b29c1af7c6ae58af3e0b38f807b855b4"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer,LabelEncoder\n",
    "max_cnt_list = []\n",
    "train_zkr_cat_cols = []\n",
    "test_zkr_cat_cols = []\n",
    "for item in cat_cols:\n",
    "    max_cnt_list.append(train[item].unique().shape[0] + 1)\n",
    "    le = LabelEncoder().fit(pd.concat([train_zkr[item],test_zkr[item]]))\n",
    "    train_zkr_cat_cols.append(le.transform(train_zkr[item]))\n",
    "    test_zkr_cat_cols.append(le.transform(test_zkr[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "f4f1b6049b7c9c52d40778ac9741f6dec0f8a9a6"
   },
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Dense,Embedding,Conv1D,SpatialDropout1D,Input,GlobalMaxPool1D,GlobalAvgPool1D\n",
    "from keras.layers import concatenate,BatchNormalization,Dropout,Flatten,GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def MLP():\n",
    "    input_warpper,embed_warpper = [],[]\n",
    "    for max_cnt,item in zip(max_cnt_list,cat_cols):\n",
    "        _input = Input(shape=(1,),name=item,dtype='int32')\n",
    "        _embed = Embedding(max_cnt,16,input_length=1)(_input)\n",
    "        _embed = Flatten()(_embed)\n",
    "        input_warpper.append(_input)\n",
    "        embed_warpper.append(_embed)\n",
    "    cate_feature = concatenate(embed_warpper)\n",
    "    _input_numerical = Input(shape=(len(zkr_numerical_cols),),name='numerical')\n",
    "    input_warpper.append(_input_numerical)\n",
    "    numerical_feature = Dense(256,activation='relu')(_input_numerical)\n",
    "    #numerical_feature = Dropout(0.25)(numerical_feature)\n",
    "    feature_map = concatenate([cate_feature,numerical_feature])\n",
    "    fc = BatchNormalization()(feature_map)\n",
    "    fc = Dropout(0.2)(feature_map)\n",
    "    fc_relu = Dense(256,activation='relu')(fc)\n",
    "    fc_tanh = Dense(256,activation='tanh')(fc)\n",
    "    fc = concatenate([fc_relu,fc_tanh])\n",
    "    #fc = BatchNormalization()(fc)\n",
    "    fc = Dropout(0.5)(fc)   \n",
    "    output = Dense(1,activation='linear')(fc)\n",
    "    \n",
    "    return Model(input= input_warpper,output=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "a902b11edc9d8ba51de9e303d399ad4dcd913a85"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-40-9f0746764082>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-9f0746764082>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    return train_prob,test_prob                                       target,train_zkr_cat_cols,test_zkr_cat_cols)\u001b[0m\n\u001b[0m                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def run_cv_mlp(train_numerical,test_numerical,target,train_cat_list,test_cat_list):\n",
    "    test_prob = np.zeros(shape=(test_numerical.shape[0],1))\n",
    "    train_prob = np.zeros(shape=(train_numerical.shape[0],1))\n",
    "    for tr_idx,te_idx in FOLDS.split(train_numerical,\n",
    "                               target.values):\n",
    "        print(len(tr_idx),len(te_idx))\n",
    "        dtr = ([item[tr_idx] for item in train_cat_list] + [train_numerical[tr_idx]],target.values[tr_idx])\n",
    "        dval = ([item[te_idx] for item in train_cat_list] + [train_numerical[te_idx]],target.values[te_idx])\n",
    "        dtest = test_cat_list + [test_numerical]\n",
    "        model = MLP()\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='mse')\n",
    "        model.fit(dtr[0],dtr[1],batch_size=128,epochs=18,validation_data=dval,\n",
    "                  shuffle=True,verbose=1)\n",
    "        _test_prob = model.predict(dtest,batch_size=512)\n",
    "        _val_prob = model.predict(dval[0],batch_size=512)\n",
    "        train_prob[te_idx,:] = _val_prob \n",
    "        test_prob += _test_prob\n",
    "    test_prob /=4.0\n",
    "    return train_prob,test_prob                                       target,train_zkr_cat_cols,test_zkr_cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "180fba8e0d3ecf684bb5754e0f4a88366ada1c73"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlp_zkr_train_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-28f30abc10b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmlp_zkr_train_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmlp_zkr_test_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmlp_zkr_train_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmlp_zkr_test_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mlp_zkr_train_pred' is not defined"
     ]
    }
   ],
   "source": [
    "mlp_zkr_train_pred.shape,mlp_zkr_test_pred.shape\n",
    "mlp_zkr_train_pred.mean(),mlp_zkr_test_pred.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6012b936da8650cdd0eca14befdea8e498224f1c"
   },
   "source": [
    "## LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "07b7ee7267ff3bb7421dcf5df366cd339eeaf7fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started lgb fold 1/4\n",
      "Prep LGB\n",
      "Train LGB\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's rmse: 0.833437\tvalid_1's rmse: 1.04255\n",
      "[1000]\ttraining's rmse: 0.709138\tvalid_1's rmse: 1.0341\n",
      "[1500]\ttraining's rmse: 0.614901\tvalid_1's rmse: 1.03166\n",
      "Early stopping, best iteration is:\n",
      "[1818]\ttraining's rmse: 0.560503\tvalid_1's rmse: 1.03088\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4.0: 1050, 2.0: 1010, 3.0: 815, 1.0: 773, 0.0: 103})\n",
      "Predicted Counts =  Counter({2.0: 1554, 4.0: 1062, 3.0: 763, 1.0: 281, 0.0: 91})\n",
      "Coefficients =  [0.51784122 1.81391186 2.50643653 2.85904262]\n",
      "QWK =  0.4590437565524471\n",
      "Predict 2/2\n",
      "lgb cv score 1: RMSE 1.0308779567400281 QWK 0.4590437565524471\n",
      "Started lgb fold 2/4\n",
      "Prep LGB\n",
      "Train LGB\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's rmse: 0.830509\tvalid_1's rmse: 1.04224\n",
      "[1000]\ttraining's rmse: 0.703029\tvalid_1's rmse: 1.03395\n",
      "Early stopping, best iteration is:\n",
      "[1278]\ttraining's rmse: 0.64846\tvalid_1's rmse: 1.03275\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4.0: 1049, 2.0: 1009, 3.0: 815, 1.0: 773, 0.0: 103})\n",
      "Predicted Counts =  Counter({2.0: 1855, 4.0: 1049, 3.0: 557, 1.0: 197, 0.0: 91})\n",
      "Coefficients =  [0.51994568 1.75294764 2.61142546 2.87757217]\n",
      "QWK =  0.45337987811719904\n",
      "Predict 2/2\n",
      "lgb cv score 2: RMSE 1.0327546391006028 QWK 0.45337987811719904\n",
      "Started lgb fold 3/4\n",
      "Prep LGB\n",
      "Train LGB\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's rmse: 0.833651\tvalid_1's rmse: 1.02553\n",
      "[1000]\ttraining's rmse: 0.710407\tvalid_1's rmse: 1.0184\n",
      "[1500]\ttraining's rmse: 0.613448\tvalid_1's rmse: 1.01647\n",
      "Early stopping, best iteration is:\n",
      "[1564]\ttraining's rmse: 0.601447\tvalid_1's rmse: 1.01633\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4.0: 1049, 2.0: 1009, 3.0: 815, 1.0: 772, 0.0: 102})\n",
      "Predicted Counts =  Counter({2.0: 1527, 4.0: 891, 1.0: 677, 3.0: 562, 0.0: 90})\n",
      "Coefficients =  [0.46831481 2.02613659 2.65101351 2.92292246]\n",
      "QWK =  0.48536152767054175\n",
      "Predict 2/2\n",
      "lgb cv score 3: RMSE 1.0163331685073835 QWK 0.48536152767054175\n",
      "Started lgb fold 4/4\n",
      "Prep LGB\n",
      "Train LGB\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's rmse: 0.832419\tvalid_1's rmse: 1.03322\n",
      "[1000]\ttraining's rmse: 0.699337\tvalid_1's rmse: 1.02549\n",
      "[1500]\ttraining's rmse: 0.600758\tvalid_1's rmse: 1.02364\n",
      "Early stopping, best iteration is:\n",
      "[1441]\ttraining's rmse: 0.612894\tvalid_1's rmse: 1.02342\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4.0: 1049, 2.0: 1009, 3.0: 814, 1.0: 772, 0.0: 102})\n",
      "Predicted Counts =  Counter({2.0: 1733, 4.0: 938, 3.0: 787, 1.0: 198, 0.0: 90})\n",
      "Coefficients =  [0.51895283 1.74825387 2.52532215 2.88505307]\n",
      "QWK =  0.460960958356182\n",
      "Predict 2/2\n",
      "lgb cv score 4: RMSE 1.0234248173349274 QWK 0.460960958356182\n",
      "lgb cv RMSE scores : [1.0308779567400281, 1.0327546391006028, 1.0163331685073835, 1.0234248173349274]\n",
      "lgb cv mean        RMSE score : 1.0258476454207355\n",
      "lgb cv recalculate RMSE1 score : 1.0258707116741508\n",
      "lgb cv recalculate RMSE2 score : 1.2075652500474348\n",
      "lgb cv std RMSE score : 0.0065078022505838905\n",
      "lgb cv QWK scores : [0.4590437565524471, 0.45337987811719904, 0.48536152767054175, 0.460960958356182]\n",
      "lgb cv mean        QWK score : 0.46468653017409245\n",
      "lgb cv recalculate QWK score : 0.47349235024325975\n",
      "lgb cv std QWK score : 0.01225782078643449\n"
     ]
    }
   ],
   "source": [
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 80,\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.9,\n",
    "          'bagging_freq': 3,\n",
    "          'feature_fraction': 0.85,\n",
    "          'min_split_gain': 0.01,\n",
    "          'min_child_samples': 150,\n",
    "          'min_child_weight': 0.1,\n",
    "          'verbosity': -1,\n",
    "          'data_random_seed': 3,\n",
    "          'early_stop': 100,\n",
    "          'verbose_eval': 500,\n",
    "          'num_rounds': 5000\n",
    "         }\n",
    "\n",
    "weight = pd.Series(np.where(train['Type']==2, 1.0, 1.0))\n",
    "lgb_gzf = run_cv_model(train[gzf_cols], test[gzf_cols], target, weight, runLGB, params, rmse, 'lgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "2af8cc081cbec43a01cf3a2ba4fbc7ae11986aaa",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started lgb fold 1/4\n",
      "Prep LGB\n",
      "Train LGB\n",
      "[500]\ttraining's rmse: 0.827229\tvalid_1's rmse: 1.04238\n",
      "[1000]\ttraining's rmse: 0.697797\tvalid_1's rmse: 1.03435\n",
      "[1500]\ttraining's rmse: 0.594214\tvalid_1's rmse: 1.03152\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4.0: 1050, 2.0: 1010, 3.0: 815, 1.0: 773, 0.0: 103})\n",
      "Predicted Counts =  Counter({2.0: 2046, 4.0: 1095, 3.0: 462, 0.0: 91, 1.0: 57})\n",
      "Coefficients =  [0.51205368 1.60020659 2.61892869 2.83391988]\n",
      "QWK =  0.44439251160399384\n",
      "Predict 2/2\n",
      "lgb cv score 1: RMSE 1.0315168018516534 QWK 0.44439251160399384\n",
      "Started lgb fold 2/4\n",
      "Prep LGB\n",
      "Train LGB\n",
      "[500]\ttraining's rmse: 0.825643\tvalid_1's rmse: 1.0359\n",
      "[1000]\ttraining's rmse: 0.693558\tvalid_1's rmse: 1.02647\n",
      "[1500]\ttraining's rmse: 0.593332\tvalid_1's rmse: 1.02389\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4.0: 1049, 2.0: 1009, 3.0: 815, 1.0: 773, 0.0: 103})\n",
      "Predicted Counts =  Counter({2.0: 1700, 3.0: 944, 4.0: 825, 1.0: 189, 0.0: 91})\n",
      "Coefficients =  [0.51081985 1.74319598 2.53636869 2.99962395]\n",
      "QWK =  0.4685863605455455\n",
      "Predict 2/2\n",
      "lgb cv score 2: RMSE 1.0238875389273 QWK 0.4685863605455455\n",
      "Started lgb fold 3/4\n",
      "Prep LGB\n",
      "Train LGB\n",
      "[500]\ttraining's rmse: 0.828154\tvalid_1's rmse: 1.02513\n",
      "[1000]\ttraining's rmse: 0.698116\tvalid_1's rmse: 1.01723\n",
      "[1500]\ttraining's rmse: 0.598277\tvalid_1's rmse: 1.01531\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4.0: 1049, 2.0: 1009, 3.0: 815, 1.0: 772, 0.0: 102})\n",
      "Predicted Counts =  Counter({2.0: 1049, 4.0: 1027, 3.0: 892, 1.0: 689, 0.0: 90})\n",
      "Coefficients =  [0.47353471 2.03360476 2.44858831 2.85356982]\n",
      "QWK =  0.4928959164430414\n",
      "Predict 2/2\n",
      "lgb cv score 3: RMSE 1.0153098629134598 QWK 0.4928959164430414\n",
      "Started lgb fold 4/4\n",
      "Prep LGB\n",
      "Train LGB\n",
      "[500]\ttraining's rmse: 0.829845\tvalid_1's rmse: 1.03009\n",
      "[1000]\ttraining's rmse: 0.69172\tvalid_1's rmse: 1.02074\n",
      "[1500]\ttraining's rmse: 0.58723\tvalid_1's rmse: 1.01871\n",
      "Predict 1/2\n",
      "Valid Counts =  Counter({4.0: 1049, 2.0: 1009, 3.0: 814, 1.0: 772, 0.0: 102})\n",
      "Predicted Counts =  Counter({2.0: 1283, 3.0: 1228, 4.0: 731, 1.0: 414, 0.0: 90})\n",
      "Coefficients =  [0.47186848 1.88086846 2.43224013 3.00968476]\n",
      "QWK =  0.48287609205644144\n",
      "Predict 2/2\n",
      "lgb cv score 4: RMSE 1.018711486350773 QWK 0.48287609205644144\n",
      "lgb cv RMSE scores : [1.0315168018516534, 1.0238875389273, 1.0153098629134598, 1.018711486350773]\n",
      "lgb cv mean        RMSE score : 1.0223564225107964\n",
      "lgb cv recalculate RMSE1 score : 1.022377559872618\n",
      "lgb cv recalculate RMSE2 score : 1.1806426899540605\n",
      "lgb cv std RMSE score : 0.006107299503846796\n",
      "lgb cv QWK scores : [0.44439251160399384, 0.4685863605455455, 0.4928959164430414, 0.48287609205644144]\n",
      "lgb cv mean        QWK score : 0.4721877201622556\n",
      "lgb cv recalculate QWK score : 0.4782813154952982\n",
      "lgb cv std QWK score : 0.01822507953918248\n"
     ]
    }
   ],
   "source": [
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 80,\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.9,\n",
    "          'bagging_freq': 3,\n",
    "          'feature_fraction': 0.84,\n",
    "          'min_split_gain': 0.01,\n",
    "          'min_child_samples': 150,\n",
    "          'min_child_weight': 0.1,\n",
    "          'verbosity': -1,\n",
    "          'data_random_seed': 3,\n",
    "#           'early_stop': 100,\n",
    "          'verbose_eval': 500,\n",
    "          'num_rounds': 1500,\n",
    "         }\n",
    "\n",
    "weight = pd.Series(np.where(train['Type']==2, 1.0, 1.0))\n",
    "lgb_zkr = run_cv_model(train[zkr_cols], test[zkr_cols], target, weight, runLGB, params, rmse, 'lgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bee31d2749404d7823a8e5760b211d3ee6c8e8d0"
   },
   "source": [
    "# 特征: ZYL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "c46adc48f8e281f8b711532fd5939dc435a92758"
   },
   "outputs": [],
   "source": [
    "# 重新导入, 一了百了\n",
    "\n",
    "del train, test\n",
    "gc.collect()\n",
    "\n",
    "train = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\n",
    "test = pd.read_csv(\"../input/petfinder-adoption-prediction/test/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "ceb695e4c6261af2b3e8ec45b0127a8fbf0bafd0"
   },
   "outputs": [],
   "source": [
    "train['Color'] = train.Color1 * 100 + train.Color2 * 10 + train.Color3\n",
    "train.drop(['Color1', 'Color2', 'Color3'], axis=1, inplace=True)\n",
    "\n",
    "test['Color'] = test.Color1 * 100 + test.Color2 * 10 + test.Color3\n",
    "test.drop(['Color1', 'Color2', 'Color3'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "1bc1883b1fcea53b2512319211cecc6ead627fc9"
   },
   "outputs": [],
   "source": [
    "target = train['AdoptionSpeed']\n",
    "train_id = train['PetID']\n",
    "test_id = test['PetID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "e80ed6377975f9af61baa893d364d6e80f2897e3"
   },
   "outputs": [],
   "source": [
    "# sentiment data\n",
    "\n",
    "doc_sent_mag = []\n",
    "doc_sent_score = []\n",
    "nf_count = 0\n",
    "for pet in train_id:\n",
    "    try:\n",
    "        with open('../input/petfinder-adoption-prediction/train_sentiment/' + pet + '.json', 'r') as f:\n",
    "            sentiment = json.load(f)\n",
    "        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "        doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "    except FileNotFoundError:\n",
    "        nf_count += 1\n",
    "        doc_sent_mag.append(-1)\n",
    "        doc_sent_score.append(-1)\n",
    "\n",
    "train.loc[:, 'doc_sent_mag'] = doc_sent_mag\n",
    "train.loc[:, 'doc_sent_score'] = doc_sent_score\n",
    "train[\"doc_sentiment\"] = train.doc_sent_mag * train.doc_sent_score\n",
    "\n",
    "doc_sent_mag = []\n",
    "doc_sent_score = []\n",
    "nf_count = 0\n",
    "for pet in test_id:\n",
    "    try:\n",
    "        with open('../input/petfinder-adoption-prediction/test_sentiment/' + pet + '.json', 'r') as f:\n",
    "            sentiment = json.load(f)\n",
    "        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "        doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "    except FileNotFoundError:\n",
    "        nf_count += 1\n",
    "        doc_sent_mag.append(-1)\n",
    "        doc_sent_score.append(-1)\n",
    "\n",
    "test.loc[:, 'doc_sent_mag'] = doc_sent_mag\n",
    "test.loc[:, 'doc_sent_score'] = doc_sent_score\n",
    "test[\"doc_sentiment\"] = test.doc_sent_mag * test.doc_sent_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "d66570d07ebdf9669e41eca61126ab50394cd6a1"
   },
   "outputs": [],
   "source": [
    "# description TF-IDF\n",
    "\n",
    "n_components = 150\n",
    "\n",
    "train_desc = train.Description.fillna(\"none\").values\n",
    "test_desc = test.Description.fillna(\"none\").values\n",
    "\n",
    "# tfv = TfidfVectorizer(min_df=2,  max_features=None,\n",
    "#         strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n",
    "#         ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "#         stop_words='english')\n",
    "\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None,\n",
    "        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "        stop_words='english')\n",
    "\n",
    "tfv.fit(list(train_desc))\n",
    "X = tfv.transform(train_desc)\n",
    "X_test = tfv.transform(test_desc)\n",
    "\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "svd.fit(X)\n",
    "X = svd.transform(X)\n",
    "\n",
    "X = pd.DataFrame(X, columns=['svd_{}'.format(i) for i in range(n_components)])\n",
    "train = pd.concat((train, X), axis=1)\n",
    "X_test = svd.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns=['svd_{}'.format(i) for i in range(n_components)])\n",
    "test = pd.concat((test, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "4ec54e72c80fc72db27ff11281fd99eb1f1f88a0"
   },
   "outputs": [],
   "source": [
    "# image metadata\n",
    "\n",
    "img_xs = []\n",
    "img_ys = []\n",
    "vertex_xs = []\n",
    "vertex_ys = []\n",
    "bounding_confidences = []\n",
    "bounding_importance_fracs = []\n",
    "dominant_blues = []\n",
    "dominant_greens = []\n",
    "dominant_reds = []\n",
    "dominant_pixel_fracs = []\n",
    "dominant_scores = []\n",
    "label_descriptions = []\n",
    "label_scores = []\n",
    "nf_count = 0\n",
    "nl_count = 0\n",
    "for pet in train_id:\n",
    "    try:\n",
    "        im = Image.open('../input/petfinder-adoption-prediction/train_images/%s-1.jpg' % pet)\n",
    "        width, height = im.size\n",
    "        img_xs.append(width)\n",
    "        img_ys.append(height)\n",
    "        with open('../input/petfinder-adoption-prediction/train_metadata/' + pet + '-1.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "        vertex_xs.append(vertex_x)\n",
    "        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "        vertex_ys.append(vertex_y)\n",
    "        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "        bounding_confidences.append(bounding_confidence)\n",
    "        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "        bounding_importance_fracs.append(bounding_importance_frac)\n",
    "        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n",
    "        dominant_blues.append(dominant_blue)\n",
    "        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n",
    "        dominant_greens.append(dominant_green)\n",
    "        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n",
    "        dominant_reds.append(dominant_red)\n",
    "        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "        dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "        dominant_scores.append(dominant_score)\n",
    "        if data.get('labelAnnotations'):\n",
    "            label_description = data['labelAnnotations'][0]['description']\n",
    "            label_descriptions.append(label_description)\n",
    "            label_score = data['labelAnnotations'][0]['score']\n",
    "            label_scores.append(label_score)\n",
    "        else:\n",
    "            nl_count += 1\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    except FileNotFoundError:\n",
    "        nf_count += 1\n",
    "        img_xs.append(-1)\n",
    "        img_ys.append(-1)\n",
    "        vertex_xs.append(-1)\n",
    "        vertex_ys.append(-1)\n",
    "        bounding_confidences.append(-1)\n",
    "        bounding_importance_fracs.append(-1)\n",
    "        dominant_blues.append(-1)\n",
    "        dominant_greens.append(-1)\n",
    "        dominant_reds.append(-1)\n",
    "        dominant_pixel_fracs.append(-1)\n",
    "        dominant_scores.append(-1)\n",
    "        label_descriptions.append('nothing')\n",
    "        label_scores.append(-1)\n",
    "\n",
    "train.loc[:, 'img_x'] = img_xs\n",
    "train.loc[:, 'img_y'] = img_ys\n",
    "train.loc[:, 'vertex_x'] = vertex_xs\n",
    "train.loc[:, 'vertex_y'] = vertex_ys\n",
    "train.loc[:, 'bounding_confidence'] = bounding_confidences\n",
    "train.loc[:, 'bounding_importance'] = bounding_importance_fracs\n",
    "train.loc[:, 'dominant_blue'] = dominant_blues\n",
    "train.loc[:, 'dominant_green'] = dominant_greens\n",
    "train.loc[:, 'dominant_red'] = dominant_reds\n",
    "train.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "train.loc[:, 'dominant_score'] = dominant_scores\n",
    "train.loc[:, 'label_description'] = label_descriptions\n",
    "train.loc[:, 'label_score'] = label_scores\n",
    "\n",
    "img_xs = []\n",
    "img_ys = []\n",
    "vertex_xs = []\n",
    "vertex_ys = []\n",
    "bounding_confidences = []\n",
    "bounding_importance_fracs = []\n",
    "dominant_blues = []\n",
    "dominant_greens = []\n",
    "dominant_reds = []\n",
    "dominant_pixel_fracs = []\n",
    "dominant_scores = []\n",
    "label_descriptions = []\n",
    "label_scores = []\n",
    "nf_count = 0\n",
    "nl_count = 0\n",
    "for pet in test_id:\n",
    "    try:\n",
    "        im = Image.open('../input/petfinder-adoption-prediction/test_images/%s-1.jpg' % pet)\n",
    "        width, height = im.size\n",
    "        img_xs.append(width)\n",
    "        img_ys.append(height)\n",
    "        with open('../input/petfinder-adoption-prediction/test_metadata/' + pet + '-1.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "        vertex_xs.append(vertex_x)\n",
    "        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "        vertex_ys.append(vertex_y)\n",
    "        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "        bounding_confidences.append(bounding_confidence)\n",
    "        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "        bounding_importance_fracs.append(bounding_importance_frac)\n",
    "        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n",
    "        dominant_blues.append(dominant_blue)\n",
    "        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n",
    "        dominant_greens.append(dominant_green)\n",
    "        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n",
    "        dominant_reds.append(dominant_red)\n",
    "        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "        dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "        dominant_scores.append(dominant_score)\n",
    "        if data.get('labelAnnotations'):\n",
    "            label_description = data['labelAnnotations'][0]['description']\n",
    "            label_descriptions.append(label_description)\n",
    "            label_score = data['labelAnnotations'][0]['score']\n",
    "            label_scores.append(label_score)\n",
    "        else:\n",
    "            nl_count += 1\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    except FileNotFoundError:\n",
    "        nf_count += 1\n",
    "        img_xs.append(-1)\n",
    "        img_ys.append(-1)\n",
    "        vertex_xs.append(-1)\n",
    "        vertex_ys.append(-1)\n",
    "        bounding_confidences.append(-1)\n",
    "        bounding_importance_fracs.append(-1)\n",
    "        dominant_blues.append(-1)\n",
    "        dominant_greens.append(-1)\n",
    "        dominant_reds.append(-1)\n",
    "        dominant_pixel_fracs.append(-1)\n",
    "        dominant_scores.append(-1)\n",
    "        label_descriptions.append('nothing')\n",
    "        label_scores.append(-1)\n",
    "\n",
    "test.loc[:, 'img_x'] = img_xs\n",
    "test.loc[:, 'img_y'] = img_ys\n",
    "test.loc[:, 'vertex_x'] = vertex_xs\n",
    "test.loc[:, 'vertex_y'] = vertex_ys\n",
    "test.loc[:, 'bounding_confidence'] = bounding_confidences\n",
    "test.loc[:, 'bounding_importance'] = bounding_importance_fracs\n",
    "test.loc[:, 'dominant_blue'] = dominant_blues\n",
    "test.loc[:, 'dominant_green'] = dominant_greens\n",
    "test.loc[:, 'dominant_red'] = dominant_reds\n",
    "test.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "test.loc[:, 'dominant_score'] = dominant_scores\n",
    "test.loc[:, 'label_description'] = label_descriptions\n",
    "test.loc[:, 'label_score'] = label_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "1d77f0bc5eb591afbd209212e4e033acf3e22fcd"
   },
   "outputs": [],
   "source": [
    "train[\"vertex_x_ratio\"] = train.vertex_x / train.img_x\n",
    "train[\"vertex_y_ratio\"] = train.vertex_y / train.img_y\n",
    "\n",
    "test[\"vertex_x_ratio\"] = test.vertex_x / test.img_x\n",
    "test[\"vertex_y_ratio\"] = test.vertex_y / test.img_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "facbbb3998c6cdf908ef85c7f975be180137d18e"
   },
   "outputs": [],
   "source": [
    "# name length\n",
    "\n",
    "train.Name = train.Name.fillna('')\n",
    "test.Name = test.Name.fillna('')\n",
    "train[\"Name\"] = train.Name.apply(lambda x: str(x).lower())\n",
    "test[\"Name\"] = test.Name.apply(lambda x: str(x).lower())\n",
    "\n",
    "train[\"name_length\"] = train.Name.apply(lambda x: len(str(x)))\n",
    "test[\"name_length\"] = test.Name.apply(lambda x: len(str(x)))\n",
    "\n",
    "# no name or not\n",
    "# train['No_name'] = 0\n",
    "# train.loc[train.name_length == 0, 'No_name'] = 1\n",
    "# train.loc[train.Name == 'Unnamed', 'No_name'] = 1\n",
    "# train.loc[train.Name == 'No Name', 'No_name'] = 1\n",
    "# train.loc[train.Name == 'No Name Yet', 'No_name'] = 1\n",
    "\n",
    "# test['No_name'] = 0\n",
    "# test.loc[test.name_length == 0, 'No_name'] = 1\n",
    "# test.loc[test.Name == 'Unnamed', 'No_name'] = 1\n",
    "# test.loc[test.Name == 'No Name', 'No_name'] = 1\n",
    "# test.loc[test.Name == 'No Name Yet', 'No_name'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "5f9817c1ffc542b80205a0fb54f77c6a8e357962"
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat((train, test))\n",
    "\n",
    "name_idx, name_val = all_data.Name.value_counts().index, all_data.Name.value_counts().values\n",
    "name_map = dict()\n",
    "for idx, val in zip(name_idx, name_val):\n",
    "    name_map.update({idx: val})\n",
    "\n",
    "train[\"name_cnt\"] = train.Name.map(name_map)\n",
    "test[\"name_cnt\"] = test.Name.map(name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "f9757e92ef33b48ed6e2ab7d8288bfb346cbc470"
   },
   "outputs": [],
   "source": [
    "# description length and words\n",
    "\n",
    "train['Description'] = train['Description'].fillna('')\n",
    "test['Description'] = test['Description'].fillna('')\n",
    "\n",
    "train['desc_length'] = train['Description'].apply(lambda x: len(x))\n",
    "train['desc_words'] = train['Description'].apply(lambda x: len(x.split()))\n",
    "\n",
    "test['desc_length'] = test['Description'].apply(lambda x: len(x))\n",
    "test['desc_words'] = test['Description'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "ae07ed429a63978116f8391e2eca56f1243f8466"
   },
   "outputs": [],
   "source": [
    "# description lexical density\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '^']\n",
    "\n",
    "def lexical_density(x):\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, \"\")\n",
    "    li = x.split(\" \")\n",
    "    return len(set(li)) / len(li) if len(li) != 0 else 0\n",
    "\n",
    "train[\"desc_lexical_density\"] = train.Description.apply(lambda x: lexical_density(x))\n",
    "test[\"desc_lexical_density\"] = test.Description.apply(lambda x: lexical_density(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "b82a805d4966a5400aee74f0637c68d7a6c05599"
   },
   "outputs": [],
   "source": [
    "def sentences_count(x):\n",
    "    return len(re.split(r'[.!?]+', x))\n",
    "\n",
    "train[\"sentences_count\"] = train.Description.apply(lambda x: sentences_count(x))\n",
    "test[\"sentences_count\"] = test.Description.apply(lambda x: sentences_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "ac708a00a85ec3391ec826258b9fd883fdbc71c7"
   },
   "outputs": [],
   "source": [
    "# description capitals count\n",
    "\n",
    "def find_capitals(x):\n",
    "    return len(re.findall('[A-Z]', x))\n",
    "\n",
    "train[\"desc_capitals\"] = train.Description.apply(lambda x: find_capitals(x))\n",
    "test[\"desc_capitals\"] = test.Description.apply(lambda x: find_capitals(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "942380bb413fc4d76e81f7d0e8089a38ac5b6438"
   },
   "outputs": [],
   "source": [
    "# number of same rescuer, a very important feature, +0.026 in LB\n",
    "\n",
    "rescuer_idx, rescuer_val = all_data.RescuerID.value_counts().index, all_data.RescuerID.value_counts().values\n",
    "rescuer_map = dict()\n",
    "for idx, val in zip(rescuer_idx, rescuer_val):\n",
    "    rescuer_map.update({idx: val})\n",
    "\n",
    "train[\"rescuer_cnt\"] = train.RescuerID.map(rescuer_map)\n",
    "test[\"rescuer_cnt\"] = test.RescuerID.map(rescuer_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "74db8e1bdb137b6599b9e83e603924803b083d77"
   },
   "outputs": [],
   "source": [
    "# state GDP: https://en.wikipedia.org/wiki/List_of_Malaysian_states_by_GDP\n",
    "state_gdp = {\n",
    "    41336: 116.679,\n",
    "    41325: 40.596,\n",
    "    41367: 23.02,\n",
    "    41401: 190.075,\n",
    "    41415: 5.984,\n",
    "    41324: 37.274,\n",
    "    41332: 42.389,\n",
    "    41335: 52.452,\n",
    "    41330: 67.629,\n",
    "    41380: 5.642,\n",
    "    41327: 81.284,\n",
    "    41345: 80.167,\n",
    "    41342: 121.414,\n",
    "    41326: 280.698,\n",
    "    41361: 32.270\n",
    "}\n",
    "\n",
    "# state population: https://zh.wikipedia.org/wiki/%E9%A9%AC%E6%9D%A5%E8%A5%BF%E4%BA%9A\n",
    "state_population = {\n",
    "    41336: 33.48283,\n",
    "    41325: 19.47651,\n",
    "    41367: 15.39601,\n",
    "    41401: 16.74621,\n",
    "    41415: 0.86908,\n",
    "    41324: 8.21110,\n",
    "    41332: 10.21064,\n",
    "    41335: 15.00817,\n",
    "    41330: 23.52743,\n",
    "    41380: 2.31541,\n",
    "    41327: 15.61383,\n",
    "    41345: 32.06742,\n",
    "    41342: 24.71140,\n",
    "    41326: 54.62141,\n",
    "    41361: 10.35977\n",
    "}\n",
    "\n",
    "# state area\n",
    "state_area = {\n",
    "    41336: 19.210,\n",
    "    41325: 9.500,\n",
    "    41367: 15.099,\n",
    "    41401: 0.243,\n",
    "    41415: 0.091,\n",
    "    41324: 1.664,\n",
    "    41332: 6.686,\n",
    "    41335: 36.137,\n",
    "    41330: 21.035,\n",
    "    41380: 2.31541,\n",
    "    41327: 0.821,\n",
    "    41345: 73.631,\n",
    "    41342: 124.450,\n",
    "    41326: 8.104,\n",
    "    41361: 13.035\n",
    "}\n",
    "\n",
    "train[\"state_gdp\"] = train.State.map(state_gdp)\n",
    "train[\"state_population\"] = train.State.map(state_population)\n",
    "train[\"state_area\"] = train.State.map(state_area)\n",
    "test[\"state_gdp\"] = test.State.map(state_gdp)\n",
    "test[\"state_population\"] = test.State.map(state_population)\n",
    "test[\"state_area\"] = test.State.map(state_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_uuid": "63cd3e2c5def81a6c277b8ad79842fe6b7993b00"
   },
   "outputs": [],
   "source": [
    "# Pure breed or not, seem not so important, but +0.010 in LB\n",
    "# {\"Domestic Long Hair\": 264, \"Domestic Medium Hair\": 265, \"Domestic Short Hair\": 266, \"Mixed Breed\": 307}\n",
    "\n",
    "train['Pure_breed'] = 1\n",
    "train.loc[train['Breed2'] != 0, 'Pure_breed'] = 0\n",
    "train.loc[train['Breed1'] == 264, 'Pure_breed'] = 0\n",
    "train.loc[train['Breed1'] == 265, 'Pure_breed'] = 0\n",
    "train.loc[train['Breed1'] == 266, 'Pure_breed'] = 0\n",
    "train.loc[train['Breed1'] == 307, 'Pure_breed'] = 0\n",
    "\n",
    "test['Pure_breed'] = 1\n",
    "test.loc[test['Breed2'] != 0, 'Pure_breed'] = 0\n",
    "test.loc[test['Breed1'] == 264, 'Pure_breed'] = 0\n",
    "test.loc[test['Breed1'] == 265, 'Pure_breed'] = 0\n",
    "test.loc[test['Breed1'] == 266, 'Pure_breed'] = 0\n",
    "test.loc[test['Breed1'] == 307, 'Pure_breed'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_uuid": "b3f0e071d133083e45461232a01899f4de86a059"
   },
   "outputs": [],
   "source": [
    "# drop some not so impantance features\n",
    "\n",
    "train.drop(['vertex_x', 'vertex_y', 'bounding_confidence'], axis=1, inplace=True)\n",
    "test.drop(['vertex_x', 'vertex_y', 'bounding_confidence'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_uuid": "eae52236ea117bcd1bbd10e5581abd310c36123b"
   },
   "outputs": [],
   "source": [
    "n_img_features = 128\n",
    "\n",
    "img_size = 256\n",
    "batch_size = 16\n",
    "\n",
    "inp = Input((img_size, img_size, 3))\n",
    "backbone = DenseNet121(input_tensor=inp, \n",
    "                       weights=\"../input/densenet-keras/DenseNet-BC-121-32-no-top.h5\",\n",
    "                       include_top = False)\n",
    "x = backbone.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\n",
    "x = AveragePooling1D(1024//n_img_features)(x)\n",
    "out = Lambda(lambda x: x[:,:,0])(x)\n",
    "\n",
    "m = Model(inp,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_uuid": "17e963fa7c917aab382242276e5e6b37f0dcfbd0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe107a07f9e41378ffdf06198da60f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86e1089a3</th>\n",
       "      <td>0.080903</td>\n",
       "      <td>0.027070</td>\n",
       "      <td>0.029983</td>\n",
       "      <td>0.005923</td>\n",
       "      <td>0.054676</td>\n",
       "      <td>0.047727</td>\n",
       "      <td>0.005912</td>\n",
       "      <td>0.022859</td>\n",
       "      <td>0.009381</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.036399</td>\n",
       "      <td>0.238115</td>\n",
       "      <td>0.059121</td>\n",
       "      <td>0.286911</td>\n",
       "      <td>0.043492</td>\n",
       "      <td>0.176318</td>\n",
       "      <td>0.092814</td>\n",
       "      <td>0.177671</td>\n",
       "      <td>0.026110</td>\n",
       "      <td>0.439185</td>\n",
       "      <td>0.040107</td>\n",
       "      <td>0.072310</td>\n",
       "      <td>0.011463</td>\n",
       "      <td>0.009991</td>\n",
       "      <td>0.056529</td>\n",
       "      <td>0.038468</td>\n",
       "      <td>0.117934</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>0.190724</td>\n",
       "      <td>0.022009</td>\n",
       "      <td>0.004581</td>\n",
       "      <td>0.196088</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>0.018651</td>\n",
       "      <td>0.036641</td>\n",
       "      <td>0.006995</td>\n",
       "      <td>0.072241</td>\n",
       "      <td>0.195218</td>\n",
       "      <td>0.017664</td>\n",
       "      <td>0.003598</td>\n",
       "      <td>0.034458</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>0.045706</td>\n",
       "      <td>0.096832</td>\n",
       "      <td>0.009699</td>\n",
       "      <td>0.014707</td>\n",
       "      <td>0.275425</td>\n",
       "      <td>0.036237</td>\n",
       "      <td>0.006158</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.071875</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>0.229531</td>\n",
       "      <td>0.286014</td>\n",
       "      <td>0.047121</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>0.103146</td>\n",
       "      <td>0.102677</td>\n",
       "      <td>0.095363</td>\n",
       "      <td>0.005054</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.022126</td>\n",
       "      <td>0.487992</td>\n",
       "      <td>0.579151</td>\n",
       "      <td>0.303814</td>\n",
       "      <td>0.589608</td>\n",
       "      <td>0.413121</td>\n",
       "      <td>0.665153</td>\n",
       "      <td>1.262865</td>\n",
       "      <td>1.033445</td>\n",
       "      <td>0.407925</td>\n",
       "      <td>0.506853</td>\n",
       "      <td>0.366394</td>\n",
       "      <td>0.778760</td>\n",
       "      <td>0.585112</td>\n",
       "      <td>0.547824</td>\n",
       "      <td>0.357888</td>\n",
       "      <td>0.663029</td>\n",
       "      <td>0.347627</td>\n",
       "      <td>0.190081</td>\n",
       "      <td>0.616218</td>\n",
       "      <td>0.964143</td>\n",
       "      <td>0.867626</td>\n",
       "      <td>1.193148</td>\n",
       "      <td>0.282650</td>\n",
       "      <td>0.673261</td>\n",
       "      <td>0.719893</td>\n",
       "      <td>0.329257</td>\n",
       "      <td>0.914918</td>\n",
       "      <td>1.200007</td>\n",
       "      <td>0.694911</td>\n",
       "      <td>0.519954</td>\n",
       "      <td>0.587572</td>\n",
       "      <td>1.510033</td>\n",
       "      <td>0.250937</td>\n",
       "      <td>0.611939</td>\n",
       "      <td>1.002798</td>\n",
       "      <td>0.356929</td>\n",
       "      <td>0.508640</td>\n",
       "      <td>0.922428</td>\n",
       "      <td>0.893496</td>\n",
       "      <td>0.502684</td>\n",
       "      <td>1.394513</td>\n",
       "      <td>0.476291</td>\n",
       "      <td>0.537682</td>\n",
       "      <td>0.648765</td>\n",
       "      <td>0.446755</td>\n",
       "      <td>0.938161</td>\n",
       "      <td>0.590351</td>\n",
       "      <td>1.045655</td>\n",
       "      <td>0.866024</td>\n",
       "      <td>0.504175</td>\n",
       "      <td>0.698822</td>\n",
       "      <td>0.438378</td>\n",
       "      <td>0.775556</td>\n",
       "      <td>1.428558</td>\n",
       "      <td>0.625741</td>\n",
       "      <td>0.920458</td>\n",
       "      <td>0.335475</td>\n",
       "      <td>1.251274</td>\n",
       "      <td>0.513746</td>\n",
       "      <td>0.790772</td>\n",
       "      <td>0.554214</td>\n",
       "      <td>0.699005</td>\n",
       "      <td>0.563984</td>\n",
       "      <td>1.304191</td>\n",
       "      <td>0.730227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6296e909a</th>\n",
       "      <td>0.036908</td>\n",
       "      <td>0.025865</td>\n",
       "      <td>0.093256</td>\n",
       "      <td>0.007122</td>\n",
       "      <td>0.084101</td>\n",
       "      <td>0.077085</td>\n",
       "      <td>0.006319</td>\n",
       "      <td>0.015811</td>\n",
       "      <td>0.005474</td>\n",
       "      <td>0.001847</td>\n",
       "      <td>0.068952</td>\n",
       "      <td>0.130923</td>\n",
       "      <td>0.280532</td>\n",
       "      <td>0.148266</td>\n",
       "      <td>0.035186</td>\n",
       "      <td>0.065542</td>\n",
       "      <td>0.153735</td>\n",
       "      <td>0.237636</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.226996</td>\n",
       "      <td>0.061644</td>\n",
       "      <td>0.143224</td>\n",
       "      <td>0.013348</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.061625</td>\n",
       "      <td>0.053013</td>\n",
       "      <td>0.228513</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.050926</td>\n",
       "      <td>0.288104</td>\n",
       "      <td>0.026730</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.119460</td>\n",
       "      <td>0.013254</td>\n",
       "      <td>0.010652</td>\n",
       "      <td>0.044845</td>\n",
       "      <td>0.054899</td>\n",
       "      <td>0.151147</td>\n",
       "      <td>0.230314</td>\n",
       "      <td>0.061158</td>\n",
       "      <td>0.005482</td>\n",
       "      <td>0.041410</td>\n",
       "      <td>0.002876</td>\n",
       "      <td>0.023397</td>\n",
       "      <td>0.180639</td>\n",
       "      <td>0.012006</td>\n",
       "      <td>0.022440</td>\n",
       "      <td>0.183900</td>\n",
       "      <td>0.060370</td>\n",
       "      <td>0.005367</td>\n",
       "      <td>0.005765</td>\n",
       "      <td>0.113480</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.181085</td>\n",
       "      <td>0.185797</td>\n",
       "      <td>0.011512</td>\n",
       "      <td>0.158204</td>\n",
       "      <td>0.104252</td>\n",
       "      <td>0.275904</td>\n",
       "      <td>0.108166</td>\n",
       "      <td>0.007445</td>\n",
       "      <td>0.002295</td>\n",
       "      <td>0.011510</td>\n",
       "      <td>0.178896</td>\n",
       "      <td>0.299209</td>\n",
       "      <td>0.427445</td>\n",
       "      <td>0.431121</td>\n",
       "      <td>0.455004</td>\n",
       "      <td>0.391252</td>\n",
       "      <td>0.978274</td>\n",
       "      <td>0.606587</td>\n",
       "      <td>0.892098</td>\n",
       "      <td>0.711356</td>\n",
       "      <td>0.418251</td>\n",
       "      <td>1.458445</td>\n",
       "      <td>0.750638</td>\n",
       "      <td>0.309308</td>\n",
       "      <td>0.688439</td>\n",
       "      <td>0.349693</td>\n",
       "      <td>0.644987</td>\n",
       "      <td>0.587616</td>\n",
       "      <td>0.431601</td>\n",
       "      <td>1.036617</td>\n",
       "      <td>0.482585</td>\n",
       "      <td>0.983732</td>\n",
       "      <td>0.381916</td>\n",
       "      <td>0.657240</td>\n",
       "      <td>1.059520</td>\n",
       "      <td>0.484432</td>\n",
       "      <td>0.633328</td>\n",
       "      <td>1.057919</td>\n",
       "      <td>0.795388</td>\n",
       "      <td>0.677788</td>\n",
       "      <td>0.822105</td>\n",
       "      <td>0.617686</td>\n",
       "      <td>0.278425</td>\n",
       "      <td>0.999062</td>\n",
       "      <td>0.974949</td>\n",
       "      <td>0.781161</td>\n",
       "      <td>0.594618</td>\n",
       "      <td>1.310527</td>\n",
       "      <td>0.635394</td>\n",
       "      <td>1.679605</td>\n",
       "      <td>0.785813</td>\n",
       "      <td>0.977226</td>\n",
       "      <td>1.197514</td>\n",
       "      <td>1.132065</td>\n",
       "      <td>0.618098</td>\n",
       "      <td>0.709188</td>\n",
       "      <td>1.259585</td>\n",
       "      <td>1.110054</td>\n",
       "      <td>0.752975</td>\n",
       "      <td>0.580067</td>\n",
       "      <td>0.645088</td>\n",
       "      <td>0.660442</td>\n",
       "      <td>0.824307</td>\n",
       "      <td>0.784991</td>\n",
       "      <td>0.873006</td>\n",
       "      <td>0.497908</td>\n",
       "      <td>0.634934</td>\n",
       "      <td>1.311370</td>\n",
       "      <td>0.563046</td>\n",
       "      <td>1.040382</td>\n",
       "      <td>0.619333</td>\n",
       "      <td>0.711116</td>\n",
       "      <td>1.271373</td>\n",
       "      <td>0.702722</td>\n",
       "      <td>0.811153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3422e4906</th>\n",
       "      <td>0.031333</td>\n",
       "      <td>0.019316</td>\n",
       "      <td>0.063033</td>\n",
       "      <td>0.003909</td>\n",
       "      <td>0.062636</td>\n",
       "      <td>0.052466</td>\n",
       "      <td>0.005522</td>\n",
       "      <td>0.008873</td>\n",
       "      <td>0.013176</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>0.056556</td>\n",
       "      <td>0.227165</td>\n",
       "      <td>0.036671</td>\n",
       "      <td>0.269196</td>\n",
       "      <td>0.062176</td>\n",
       "      <td>0.295367</td>\n",
       "      <td>0.062095</td>\n",
       "      <td>0.224414</td>\n",
       "      <td>0.027769</td>\n",
       "      <td>0.464024</td>\n",
       "      <td>0.037842</td>\n",
       "      <td>0.053576</td>\n",
       "      <td>0.011953</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>0.076405</td>\n",
       "      <td>0.026240</td>\n",
       "      <td>0.069317</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.014946</td>\n",
       "      <td>0.236115</td>\n",
       "      <td>0.017828</td>\n",
       "      <td>0.002726</td>\n",
       "      <td>0.171526</td>\n",
       "      <td>0.014121</td>\n",
       "      <td>0.005398</td>\n",
       "      <td>0.042197</td>\n",
       "      <td>0.015808</td>\n",
       "      <td>0.059264</td>\n",
       "      <td>0.158360</td>\n",
       "      <td>0.017550</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.088531</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.025637</td>\n",
       "      <td>0.081270</td>\n",
       "      <td>0.007263</td>\n",
       "      <td>0.019419</td>\n",
       "      <td>0.275068</td>\n",
       "      <td>0.067482</td>\n",
       "      <td>0.004163</td>\n",
       "      <td>0.007121</td>\n",
       "      <td>0.100102</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.231722</td>\n",
       "      <td>0.423444</td>\n",
       "      <td>0.072245</td>\n",
       "      <td>0.067363</td>\n",
       "      <td>0.230224</td>\n",
       "      <td>0.085733</td>\n",
       "      <td>0.132242</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.023581</td>\n",
       "      <td>0.365315</td>\n",
       "      <td>0.624939</td>\n",
       "      <td>0.407349</td>\n",
       "      <td>0.448166</td>\n",
       "      <td>0.405758</td>\n",
       "      <td>0.579463</td>\n",
       "      <td>0.776017</td>\n",
       "      <td>0.381641</td>\n",
       "      <td>0.596929</td>\n",
       "      <td>0.285997</td>\n",
       "      <td>0.493722</td>\n",
       "      <td>0.474389</td>\n",
       "      <td>0.363883</td>\n",
       "      <td>0.701210</td>\n",
       "      <td>0.701126</td>\n",
       "      <td>0.563001</td>\n",
       "      <td>0.549886</td>\n",
       "      <td>0.276724</td>\n",
       "      <td>0.712891</td>\n",
       "      <td>0.885036</td>\n",
       "      <td>0.442720</td>\n",
       "      <td>0.382157</td>\n",
       "      <td>0.699339</td>\n",
       "      <td>0.186581</td>\n",
       "      <td>1.105836</td>\n",
       "      <td>0.556738</td>\n",
       "      <td>0.366696</td>\n",
       "      <td>0.564532</td>\n",
       "      <td>0.482095</td>\n",
       "      <td>0.922556</td>\n",
       "      <td>0.718453</td>\n",
       "      <td>1.408340</td>\n",
       "      <td>0.820676</td>\n",
       "      <td>0.859361</td>\n",
       "      <td>0.906259</td>\n",
       "      <td>0.762644</td>\n",
       "      <td>0.659158</td>\n",
       "      <td>0.916725</td>\n",
       "      <td>0.640755</td>\n",
       "      <td>1.181538</td>\n",
       "      <td>0.980100</td>\n",
       "      <td>0.628372</td>\n",
       "      <td>0.708757</td>\n",
       "      <td>0.479300</td>\n",
       "      <td>0.785339</td>\n",
       "      <td>0.940515</td>\n",
       "      <td>0.845812</td>\n",
       "      <td>0.741837</td>\n",
       "      <td>0.427259</td>\n",
       "      <td>0.313643</td>\n",
       "      <td>0.499163</td>\n",
       "      <td>0.944294</td>\n",
       "      <td>0.627926</td>\n",
       "      <td>0.576165</td>\n",
       "      <td>0.618234</td>\n",
       "      <td>0.800512</td>\n",
       "      <td>0.794122</td>\n",
       "      <td>0.807578</td>\n",
       "      <td>0.729883</td>\n",
       "      <td>0.764788</td>\n",
       "      <td>0.762196</td>\n",
       "      <td>1.056118</td>\n",
       "      <td>1.733463</td>\n",
       "      <td>1.272634</td>\n",
       "      <td>0.690354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5842f1ff5</th>\n",
       "      <td>0.100765</td>\n",
       "      <td>0.058779</td>\n",
       "      <td>0.044435</td>\n",
       "      <td>0.005573</td>\n",
       "      <td>0.056934</td>\n",
       "      <td>0.152086</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>0.005545</td>\n",
       "      <td>0.003043</td>\n",
       "      <td>0.040706</td>\n",
       "      <td>0.115147</td>\n",
       "      <td>0.086512</td>\n",
       "      <td>0.187995</td>\n",
       "      <td>0.042647</td>\n",
       "      <td>0.095363</td>\n",
       "      <td>0.049536</td>\n",
       "      <td>0.226077</td>\n",
       "      <td>0.023286</td>\n",
       "      <td>0.148373</td>\n",
       "      <td>0.074957</td>\n",
       "      <td>0.063764</td>\n",
       "      <td>0.010141</td>\n",
       "      <td>0.007410</td>\n",
       "      <td>0.095906</td>\n",
       "      <td>0.055115</td>\n",
       "      <td>0.100356</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.056281</td>\n",
       "      <td>0.233078</td>\n",
       "      <td>0.021738</td>\n",
       "      <td>0.003783</td>\n",
       "      <td>0.222727</td>\n",
       "      <td>0.005912</td>\n",
       "      <td>0.019378</td>\n",
       "      <td>0.048208</td>\n",
       "      <td>0.012948</td>\n",
       "      <td>0.073872</td>\n",
       "      <td>0.165543</td>\n",
       "      <td>0.037689</td>\n",
       "      <td>0.004991</td>\n",
       "      <td>0.164023</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>0.063428</td>\n",
       "      <td>0.166494</td>\n",
       "      <td>0.010949</td>\n",
       "      <td>0.021875</td>\n",
       "      <td>0.301821</td>\n",
       "      <td>0.103855</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.030184</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>0.178647</td>\n",
       "      <td>0.227816</td>\n",
       "      <td>0.026276</td>\n",
       "      <td>0.091511</td>\n",
       "      <td>0.180973</td>\n",
       "      <td>0.150826</td>\n",
       "      <td>0.045812</td>\n",
       "      <td>0.007572</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.012983</td>\n",
       "      <td>0.395622</td>\n",
       "      <td>0.475078</td>\n",
       "      <td>0.219794</td>\n",
       "      <td>0.351564</td>\n",
       "      <td>0.441491</td>\n",
       "      <td>0.621629</td>\n",
       "      <td>0.890861</td>\n",
       "      <td>1.104187</td>\n",
       "      <td>0.485312</td>\n",
       "      <td>0.330086</td>\n",
       "      <td>0.737265</td>\n",
       "      <td>1.014804</td>\n",
       "      <td>0.301714</td>\n",
       "      <td>0.498786</td>\n",
       "      <td>0.319726</td>\n",
       "      <td>0.822718</td>\n",
       "      <td>0.683046</td>\n",
       "      <td>0.376709</td>\n",
       "      <td>0.970635</td>\n",
       "      <td>0.657510</td>\n",
       "      <td>1.164067</td>\n",
       "      <td>0.545164</td>\n",
       "      <td>0.525729</td>\n",
       "      <td>0.441820</td>\n",
       "      <td>0.673608</td>\n",
       "      <td>0.844660</td>\n",
       "      <td>1.238436</td>\n",
       "      <td>1.106471</td>\n",
       "      <td>0.526544</td>\n",
       "      <td>0.732316</td>\n",
       "      <td>0.829567</td>\n",
       "      <td>0.576851</td>\n",
       "      <td>0.313880</td>\n",
       "      <td>0.933374</td>\n",
       "      <td>1.216672</td>\n",
       "      <td>0.596536</td>\n",
       "      <td>0.606384</td>\n",
       "      <td>1.112674</td>\n",
       "      <td>1.313025</td>\n",
       "      <td>0.747136</td>\n",
       "      <td>0.935620</td>\n",
       "      <td>1.232728</td>\n",
       "      <td>0.729560</td>\n",
       "      <td>1.369965</td>\n",
       "      <td>0.744446</td>\n",
       "      <td>1.559710</td>\n",
       "      <td>0.504133</td>\n",
       "      <td>0.995418</td>\n",
       "      <td>0.763051</td>\n",
       "      <td>0.530875</td>\n",
       "      <td>1.203119</td>\n",
       "      <td>0.489429</td>\n",
       "      <td>0.598452</td>\n",
       "      <td>0.847590</td>\n",
       "      <td>1.254122</td>\n",
       "      <td>0.853125</td>\n",
       "      <td>1.111104</td>\n",
       "      <td>1.204281</td>\n",
       "      <td>1.003967</td>\n",
       "      <td>0.821134</td>\n",
       "      <td>1.081506</td>\n",
       "      <td>0.771984</td>\n",
       "      <td>1.580613</td>\n",
       "      <td>0.802881</td>\n",
       "      <td>1.027818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850a43f90</th>\n",
       "      <td>0.064575</td>\n",
       "      <td>0.056714</td>\n",
       "      <td>0.129328</td>\n",
       "      <td>0.004659</td>\n",
       "      <td>0.060749</td>\n",
       "      <td>0.198181</td>\n",
       "      <td>0.004649</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>0.009161</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>0.055383</td>\n",
       "      <td>0.227129</td>\n",
       "      <td>0.013477</td>\n",
       "      <td>0.325656</td>\n",
       "      <td>0.083217</td>\n",
       "      <td>0.194120</td>\n",
       "      <td>0.034585</td>\n",
       "      <td>0.387116</td>\n",
       "      <td>0.026539</td>\n",
       "      <td>0.223599</td>\n",
       "      <td>0.112257</td>\n",
       "      <td>0.077782</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>0.047016</td>\n",
       "      <td>0.051680</td>\n",
       "      <td>0.205966</td>\n",
       "      <td>0.001437</td>\n",
       "      <td>0.007965</td>\n",
       "      <td>0.362459</td>\n",
       "      <td>0.017941</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>0.290657</td>\n",
       "      <td>0.005715</td>\n",
       "      <td>0.024016</td>\n",
       "      <td>0.065434</td>\n",
       "      <td>0.100374</td>\n",
       "      <td>0.086999</td>\n",
       "      <td>0.149593</td>\n",
       "      <td>0.034557</td>\n",
       "      <td>0.004126</td>\n",
       "      <td>0.138259</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>0.056932</td>\n",
       "      <td>0.060603</td>\n",
       "      <td>0.010333</td>\n",
       "      <td>0.022251</td>\n",
       "      <td>0.333058</td>\n",
       "      <td>0.090267</td>\n",
       "      <td>0.006838</td>\n",
       "      <td>0.007620</td>\n",
       "      <td>0.072204</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.277812</td>\n",
       "      <td>0.309486</td>\n",
       "      <td>0.058816</td>\n",
       "      <td>0.074558</td>\n",
       "      <td>0.461126</td>\n",
       "      <td>0.139310</td>\n",
       "      <td>0.048256</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>0.019297</td>\n",
       "      <td>0.100398</td>\n",
       "      <td>0.391858</td>\n",
       "      <td>0.327033</td>\n",
       "      <td>0.485347</td>\n",
       "      <td>0.373845</td>\n",
       "      <td>0.600073</td>\n",
       "      <td>0.851016</td>\n",
       "      <td>0.849287</td>\n",
       "      <td>0.486980</td>\n",
       "      <td>0.261849</td>\n",
       "      <td>0.450096</td>\n",
       "      <td>0.922868</td>\n",
       "      <td>0.325289</td>\n",
       "      <td>0.387753</td>\n",
       "      <td>0.192341</td>\n",
       "      <td>0.404544</td>\n",
       "      <td>0.446913</td>\n",
       "      <td>0.302947</td>\n",
       "      <td>0.880942</td>\n",
       "      <td>0.725538</td>\n",
       "      <td>0.797994</td>\n",
       "      <td>0.589420</td>\n",
       "      <td>0.807926</td>\n",
       "      <td>0.270234</td>\n",
       "      <td>1.186765</td>\n",
       "      <td>0.402791</td>\n",
       "      <td>0.398661</td>\n",
       "      <td>0.590872</td>\n",
       "      <td>0.441502</td>\n",
       "      <td>0.662898</td>\n",
       "      <td>0.418006</td>\n",
       "      <td>0.918066</td>\n",
       "      <td>0.820240</td>\n",
       "      <td>0.637656</td>\n",
       "      <td>0.677504</td>\n",
       "      <td>0.509595</td>\n",
       "      <td>1.009955</td>\n",
       "      <td>0.681656</td>\n",
       "      <td>0.998213</td>\n",
       "      <td>0.545514</td>\n",
       "      <td>0.919542</td>\n",
       "      <td>0.798344</td>\n",
       "      <td>0.749780</td>\n",
       "      <td>0.794771</td>\n",
       "      <td>0.212994</td>\n",
       "      <td>1.172289</td>\n",
       "      <td>0.363446</td>\n",
       "      <td>0.811347</td>\n",
       "      <td>0.456034</td>\n",
       "      <td>0.244271</td>\n",
       "      <td>0.801077</td>\n",
       "      <td>0.873605</td>\n",
       "      <td>0.665535</td>\n",
       "      <td>0.960412</td>\n",
       "      <td>0.568228</td>\n",
       "      <td>0.766188</td>\n",
       "      <td>0.464421</td>\n",
       "      <td>0.703810</td>\n",
       "      <td>0.798323</td>\n",
       "      <td>0.535618</td>\n",
       "      <td>0.675487</td>\n",
       "      <td>0.579390</td>\n",
       "      <td>0.648065</td>\n",
       "      <td>0.813327</td>\n",
       "      <td>0.385569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5    \\\n",
       "86e1089a3  0.080903  0.027070  0.029983  0.005923  0.054676  0.047727   \n",
       "6296e909a  0.036908  0.025865  0.093256  0.007122  0.084101  0.077085   \n",
       "3422e4906  0.031333  0.019316  0.063033  0.003909  0.062636  0.052466   \n",
       "5842f1ff5  0.100765  0.058779  0.044435  0.005573  0.056934  0.152086   \n",
       "850a43f90  0.064575  0.056714  0.129328  0.004659  0.060749  0.198181   \n",
       "\n",
       "                6         7         8         9         10        11   \\\n",
       "86e1089a3  0.005912  0.022859  0.009381  0.002341  0.036399  0.238115   \n",
       "6296e909a  0.006319  0.015811  0.005474  0.001847  0.068952  0.130923   \n",
       "3422e4906  0.005522  0.008873  0.013176  0.002032  0.056556  0.227165   \n",
       "5842f1ff5  0.003599  0.013090  0.005545  0.003043  0.040706  0.115147   \n",
       "850a43f90  0.004649  0.013841  0.009161  0.002527  0.055383  0.227129   \n",
       "\n",
       "                12        13        14        15        16        17   \\\n",
       "86e1089a3  0.059121  0.286911  0.043492  0.176318  0.092814  0.177671   \n",
       "6296e909a  0.280532  0.148266  0.035186  0.065542  0.153735  0.237636   \n",
       "3422e4906  0.036671  0.269196  0.062176  0.295367  0.062095  0.224414   \n",
       "5842f1ff5  0.086512  0.187995  0.042647  0.095363  0.049536  0.226077   \n",
       "850a43f90  0.013477  0.325656  0.083217  0.194120  0.034585  0.387116   \n",
       "\n",
       "                18        19        20        21        22        23   \\\n",
       "86e1089a3  0.026110  0.439185  0.040107  0.072310  0.011463  0.009991   \n",
       "6296e909a  0.015100  0.226996  0.061644  0.143224  0.013348  0.003795   \n",
       "3422e4906  0.027769  0.464024  0.037842  0.053576  0.011953  0.006924   \n",
       "5842f1ff5  0.023286  0.148373  0.074957  0.063764  0.010141  0.007410   \n",
       "850a43f90  0.026539  0.223599  0.112257  0.077782  0.014100  0.010001   \n",
       "\n",
       "                24        25        26        27        28        29   \\\n",
       "86e1089a3  0.056529  0.038468  0.117934  0.001337  0.016928  0.190724   \n",
       "6296e909a  0.061625  0.053013  0.228513  0.001519  0.050926  0.288104   \n",
       "3422e4906  0.076405  0.026240  0.069317  0.001182  0.014946  0.236115   \n",
       "5842f1ff5  0.095906  0.055115  0.100356  0.001387  0.056281  0.233078   \n",
       "850a43f90  0.047016  0.051680  0.205966  0.001437  0.007965  0.362459   \n",
       "\n",
       "                30        31        32        33        34        35   \\\n",
       "86e1089a3  0.022009  0.004581  0.196088  0.002710  0.018651  0.036641   \n",
       "6296e909a  0.026730  0.003126  0.119460  0.013254  0.010652  0.044845   \n",
       "3422e4906  0.017828  0.002726  0.171526  0.014121  0.005398  0.042197   \n",
       "5842f1ff5  0.021738  0.003783  0.222727  0.005912  0.019378  0.048208   \n",
       "850a43f90  0.017941  0.003566  0.290657  0.005715  0.024016  0.065434   \n",
       "\n",
       "                36        37        38        39        40        41   \\\n",
       "86e1089a3  0.006995  0.072241  0.195218  0.017664  0.003598  0.034458   \n",
       "6296e909a  0.054899  0.151147  0.230314  0.061158  0.005482  0.041410   \n",
       "3422e4906  0.015808  0.059264  0.158360  0.017550  0.003582  0.088531   \n",
       "5842f1ff5  0.012948  0.073872  0.165543  0.037689  0.004991  0.164023   \n",
       "850a43f90  0.100374  0.086999  0.149593  0.034557  0.004126  0.138259   \n",
       "\n",
       "                42        43        44        45        46        47   \\\n",
       "86e1089a3  0.003159  0.045706  0.096832  0.009699  0.014707  0.275425   \n",
       "6296e909a  0.002876  0.023397  0.180639  0.012006  0.022440  0.183900   \n",
       "3422e4906  0.001900  0.025637  0.081270  0.007263  0.019419  0.275068   \n",
       "5842f1ff5  0.002971  0.063428  0.166494  0.010949  0.021875  0.301821   \n",
       "850a43f90  0.002639  0.056932  0.060603  0.010333  0.022251  0.333058   \n",
       "\n",
       "                48        49        50        51        52        53   \\\n",
       "86e1089a3  0.036237  0.006158  0.005701  0.071875  0.004418  0.229531   \n",
       "6296e909a  0.060370  0.005367  0.005765  0.113480  0.002440  0.181085   \n",
       "3422e4906  0.067482  0.004163  0.007121  0.100102  0.002837  0.231722   \n",
       "5842f1ff5  0.103855  0.005201  0.008208  0.030184  0.003318  0.178647   \n",
       "850a43f90  0.090267  0.006838  0.007620  0.072204  0.002903  0.277812   \n",
       "\n",
       "                54        55        56        57        58        59   \\\n",
       "86e1089a3  0.286014  0.047121  0.206055  0.103146  0.102677  0.095363   \n",
       "6296e909a  0.185797  0.011512  0.158204  0.104252  0.275904  0.108166   \n",
       "3422e4906  0.423444  0.072245  0.067363  0.230224  0.085733  0.132242   \n",
       "5842f1ff5  0.227816  0.026276  0.091511  0.180973  0.150826  0.045812   \n",
       "850a43f90  0.309486  0.058816  0.074558  0.461126  0.139310  0.048256   \n",
       "\n",
       "                60        61        62        63        64        65   \\\n",
       "86e1089a3  0.005054  0.002239  0.022126  0.487992  0.579151  0.303814   \n",
       "6296e909a  0.007445  0.002295  0.011510  0.178896  0.299209  0.427445   \n",
       "3422e4906  0.007480  0.001563  0.023581  0.365315  0.624939  0.407349   \n",
       "5842f1ff5  0.007572  0.003175  0.012983  0.395622  0.475078  0.219794   \n",
       "850a43f90  0.008033  0.003616  0.019297  0.100398  0.391858  0.327033   \n",
       "\n",
       "                66        67        68        69        70        71   \\\n",
       "86e1089a3  0.589608  0.413121  0.665153  1.262865  1.033445  0.407925   \n",
       "6296e909a  0.431121  0.455004  0.391252  0.978274  0.606587  0.892098   \n",
       "3422e4906  0.448166  0.405758  0.579463  0.776017  0.381641  0.596929   \n",
       "5842f1ff5  0.351564  0.441491  0.621629  0.890861  1.104187  0.485312   \n",
       "850a43f90  0.485347  0.373845  0.600073  0.851016  0.849287  0.486980   \n",
       "\n",
       "                72        73        74        75        76        77   \\\n",
       "86e1089a3  0.506853  0.366394  0.778760  0.585112  0.547824  0.357888   \n",
       "6296e909a  0.711356  0.418251  1.458445  0.750638  0.309308  0.688439   \n",
       "3422e4906  0.285997  0.493722  0.474389  0.363883  0.701210  0.701126   \n",
       "5842f1ff5  0.330086  0.737265  1.014804  0.301714  0.498786  0.319726   \n",
       "850a43f90  0.261849  0.450096  0.922868  0.325289  0.387753  0.192341   \n",
       "\n",
       "                78        79        80        81        82        83   \\\n",
       "86e1089a3  0.663029  0.347627  0.190081  0.616218  0.964143  0.867626   \n",
       "6296e909a  0.349693  0.644987  0.587616  0.431601  1.036617  0.482585   \n",
       "3422e4906  0.563001  0.549886  0.276724  0.712891  0.885036  0.442720   \n",
       "5842f1ff5  0.822718  0.683046  0.376709  0.970635  0.657510  1.164067   \n",
       "850a43f90  0.404544  0.446913  0.302947  0.880942  0.725538  0.797994   \n",
       "\n",
       "                84        85        86        87        88        89   \\\n",
       "86e1089a3  1.193148  0.282650  0.673261  0.719893  0.329257  0.914918   \n",
       "6296e909a  0.983732  0.381916  0.657240  1.059520  0.484432  0.633328   \n",
       "3422e4906  0.382157  0.699339  0.186581  1.105836  0.556738  0.366696   \n",
       "5842f1ff5  0.545164  0.525729  0.441820  0.673608  0.844660  1.238436   \n",
       "850a43f90  0.589420  0.807926  0.270234  1.186765  0.402791  0.398661   \n",
       "\n",
       "                90        91        92        93        94        95   \\\n",
       "86e1089a3  1.200007  0.694911  0.519954  0.587572  1.510033  0.250937   \n",
       "6296e909a  1.057919  0.795388  0.677788  0.822105  0.617686  0.278425   \n",
       "3422e4906  0.564532  0.482095  0.922556  0.718453  1.408340  0.820676   \n",
       "5842f1ff5  1.106471  0.526544  0.732316  0.829567  0.576851  0.313880   \n",
       "850a43f90  0.590872  0.441502  0.662898  0.418006  0.918066  0.820240   \n",
       "\n",
       "                96        97        98        99        100       101  \\\n",
       "86e1089a3  0.611939  1.002798  0.356929  0.508640  0.922428  0.893496   \n",
       "6296e909a  0.999062  0.974949  0.781161  0.594618  1.310527  0.635394   \n",
       "3422e4906  0.859361  0.906259  0.762644  0.659158  0.916725  0.640755   \n",
       "5842f1ff5  0.933374  1.216672  0.596536  0.606384  1.112674  1.313025   \n",
       "850a43f90  0.637656  0.677504  0.509595  1.009955  0.681656  0.998213   \n",
       "\n",
       "                102       103       104       105       106       107  \\\n",
       "86e1089a3  0.502684  1.394513  0.476291  0.537682  0.648765  0.446755   \n",
       "6296e909a  1.679605  0.785813  0.977226  1.197514  1.132065  0.618098   \n",
       "3422e4906  1.181538  0.980100  0.628372  0.708757  0.479300  0.785339   \n",
       "5842f1ff5  0.747136  0.935620  1.232728  0.729560  1.369965  0.744446   \n",
       "850a43f90  0.545514  0.919542  0.798344  0.749780  0.794771  0.212994   \n",
       "\n",
       "                108       109       110       111       112       113  \\\n",
       "86e1089a3  0.938161  0.590351  1.045655  0.866024  0.504175  0.698822   \n",
       "6296e909a  0.709188  1.259585  1.110054  0.752975  0.580067  0.645088   \n",
       "3422e4906  0.940515  0.845812  0.741837  0.427259  0.313643  0.499163   \n",
       "5842f1ff5  1.559710  0.504133  0.995418  0.763051  0.530875  1.203119   \n",
       "850a43f90  1.172289  0.363446  0.811347  0.456034  0.244271  0.801077   \n",
       "\n",
       "                114       115       116       117       118       119  \\\n",
       "86e1089a3  0.438378  0.775556  1.428558  0.625741  0.920458  0.335475   \n",
       "6296e909a  0.660442  0.824307  0.784991  0.873006  0.497908  0.634934   \n",
       "3422e4906  0.944294  0.627926  0.576165  0.618234  0.800512  0.794122   \n",
       "5842f1ff5  0.489429  0.598452  0.847590  1.254122  0.853125  1.111104   \n",
       "850a43f90  0.873605  0.665535  0.960412  0.568228  0.766188  0.464421   \n",
       "\n",
       "                120       121       122       123       124       125  \\\n",
       "86e1089a3  1.251274  0.513746  0.790772  0.554214  0.699005  0.563984   \n",
       "6296e909a  1.311370  0.563046  1.040382  0.619333  0.711116  1.271373   \n",
       "3422e4906  0.807578  0.729883  0.764788  0.762196  1.056118  1.733463   \n",
       "5842f1ff5  1.204281  1.003967  0.821134  1.081506  0.771984  1.580613   \n",
       "850a43f90  0.703810  0.798323  0.535618  0.675487  0.579390  0.648065   \n",
       "\n",
       "                126       127  \n",
       "86e1089a3  1.304191  0.730227  \n",
       "6296e909a  0.702722  0.811153  \n",
       "3422e4906  1.272634  0.690354  \n",
       "5842f1ff5  0.802881  1.027818  \n",
       "850a43f90  0.813327  0.385569  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pet_ids = train_id.values\n",
    "n_batches = len(pet_ids) // batch_size + 1\n",
    "\n",
    "features = {}\n",
    "for b in tqdm_notebook(range(n_batches)):\n",
    "    start = b*batch_size\n",
    "    end = (b+1)*batch_size\n",
    "    batch_pets = pet_ids[start:end]\n",
    "    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n",
    "    for i,pet_id in enumerate(batch_pets):\n",
    "        try:\n",
    "            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/train_images/\", pet_id)\n",
    "        except:\n",
    "            pass\n",
    "    batch_preds = m.predict(batch_images)\n",
    "    for i,pet_id in enumerate(batch_pets):\n",
    "        features[pet_id] = batch_preds[i]\n",
    "        \n",
    "train_feats = pd.DataFrame.from_dict(features, orient='index')\n",
    "# train_feats.to_csv('train_img_features.csv')\n",
    "train_feats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_uuid": "61c093a449cd1d14486bac8bd791f09e37421d38"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a802c3526a4d41a6d601d8bbbb6d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=247), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>378fcc4fc</th>\n",
       "      <td>0.046349</td>\n",
       "      <td>0.029440</td>\n",
       "      <td>0.044969</td>\n",
       "      <td>0.005820</td>\n",
       "      <td>0.043166</td>\n",
       "      <td>0.084996</td>\n",
       "      <td>0.006374</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.012883</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.049012</td>\n",
       "      <td>0.113741</td>\n",
       "      <td>0.166774</td>\n",
       "      <td>0.214079</td>\n",
       "      <td>0.117198</td>\n",
       "      <td>0.170769</td>\n",
       "      <td>0.083287</td>\n",
       "      <td>0.369204</td>\n",
       "      <td>0.023654</td>\n",
       "      <td>0.210734</td>\n",
       "      <td>0.075803</td>\n",
       "      <td>0.077567</td>\n",
       "      <td>0.008761</td>\n",
       "      <td>0.006844</td>\n",
       "      <td>0.064534</td>\n",
       "      <td>0.044727</td>\n",
       "      <td>0.163834</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.009899</td>\n",
       "      <td>0.404045</td>\n",
       "      <td>0.032993</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.164170</td>\n",
       "      <td>0.006376</td>\n",
       "      <td>0.011148</td>\n",
       "      <td>0.047406</td>\n",
       "      <td>0.049476</td>\n",
       "      <td>0.117032</td>\n",
       "      <td>0.199826</td>\n",
       "      <td>0.057105</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.080790</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.041794</td>\n",
       "      <td>0.098319</td>\n",
       "      <td>0.009090</td>\n",
       "      <td>0.021795</td>\n",
       "      <td>0.315340</td>\n",
       "      <td>0.063945</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.070421</td>\n",
       "      <td>0.003485</td>\n",
       "      <td>0.202767</td>\n",
       "      <td>0.226186</td>\n",
       "      <td>0.036033</td>\n",
       "      <td>0.109722</td>\n",
       "      <td>0.176980</td>\n",
       "      <td>0.103157</td>\n",
       "      <td>0.074406</td>\n",
       "      <td>0.004510</td>\n",
       "      <td>0.003517</td>\n",
       "      <td>0.017212</td>\n",
       "      <td>0.316111</td>\n",
       "      <td>0.632204</td>\n",
       "      <td>0.349130</td>\n",
       "      <td>0.661242</td>\n",
       "      <td>0.224366</td>\n",
       "      <td>0.411741</td>\n",
       "      <td>1.005475</td>\n",
       "      <td>0.777165</td>\n",
       "      <td>0.706809</td>\n",
       "      <td>0.366492</td>\n",
       "      <td>0.469502</td>\n",
       "      <td>0.788777</td>\n",
       "      <td>0.402848</td>\n",
       "      <td>0.746356</td>\n",
       "      <td>0.456743</td>\n",
       "      <td>0.512674</td>\n",
       "      <td>0.581825</td>\n",
       "      <td>0.606519</td>\n",
       "      <td>0.716501</td>\n",
       "      <td>0.830395</td>\n",
       "      <td>0.817831</td>\n",
       "      <td>0.843343</td>\n",
       "      <td>0.853221</td>\n",
       "      <td>0.477257</td>\n",
       "      <td>1.366606</td>\n",
       "      <td>0.367085</td>\n",
       "      <td>0.759267</td>\n",
       "      <td>0.444994</td>\n",
       "      <td>0.368155</td>\n",
       "      <td>0.848210</td>\n",
       "      <td>0.475449</td>\n",
       "      <td>1.418633</td>\n",
       "      <td>0.340132</td>\n",
       "      <td>0.615128</td>\n",
       "      <td>0.869077</td>\n",
       "      <td>0.535458</td>\n",
       "      <td>0.781738</td>\n",
       "      <td>1.459623</td>\n",
       "      <td>1.053890</td>\n",
       "      <td>1.426533</td>\n",
       "      <td>0.935947</td>\n",
       "      <td>1.259884</td>\n",
       "      <td>0.764008</td>\n",
       "      <td>0.798453</td>\n",
       "      <td>0.469986</td>\n",
       "      <td>1.376810</td>\n",
       "      <td>0.760331</td>\n",
       "      <td>0.792629</td>\n",
       "      <td>0.538545</td>\n",
       "      <td>0.475917</td>\n",
       "      <td>0.986913</td>\n",
       "      <td>0.783445</td>\n",
       "      <td>0.452399</td>\n",
       "      <td>0.940398</td>\n",
       "      <td>1.184496</td>\n",
       "      <td>0.706118</td>\n",
       "      <td>0.780622</td>\n",
       "      <td>1.099451</td>\n",
       "      <td>1.113779</td>\n",
       "      <td>0.588675</td>\n",
       "      <td>1.029418</td>\n",
       "      <td>0.421483</td>\n",
       "      <td>0.689350</td>\n",
       "      <td>0.818461</td>\n",
       "      <td>0.732772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73c10e136</th>\n",
       "      <td>0.045589</td>\n",
       "      <td>0.052937</td>\n",
       "      <td>0.035650</td>\n",
       "      <td>0.005980</td>\n",
       "      <td>0.055054</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>0.006685</td>\n",
       "      <td>0.019189</td>\n",
       "      <td>0.009906</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.032303</td>\n",
       "      <td>0.137175</td>\n",
       "      <td>0.037631</td>\n",
       "      <td>0.233158</td>\n",
       "      <td>0.064486</td>\n",
       "      <td>0.159466</td>\n",
       "      <td>0.067773</td>\n",
       "      <td>0.457415</td>\n",
       "      <td>0.028972</td>\n",
       "      <td>0.203293</td>\n",
       "      <td>0.074134</td>\n",
       "      <td>0.066226</td>\n",
       "      <td>0.009930</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>0.085333</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>0.159706</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.013014</td>\n",
       "      <td>0.254318</td>\n",
       "      <td>0.030654</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.254228</td>\n",
       "      <td>0.007231</td>\n",
       "      <td>0.015752</td>\n",
       "      <td>0.055318</td>\n",
       "      <td>0.038069</td>\n",
       "      <td>0.055355</td>\n",
       "      <td>0.145451</td>\n",
       "      <td>0.055705</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>0.100308</td>\n",
       "      <td>0.002933</td>\n",
       "      <td>0.036124</td>\n",
       "      <td>0.063251</td>\n",
       "      <td>0.012044</td>\n",
       "      <td>0.022930</td>\n",
       "      <td>0.305344</td>\n",
       "      <td>0.100218</td>\n",
       "      <td>0.005209</td>\n",
       "      <td>0.007967</td>\n",
       "      <td>0.063378</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.171008</td>\n",
       "      <td>0.277723</td>\n",
       "      <td>0.030621</td>\n",
       "      <td>0.093463</td>\n",
       "      <td>0.151823</td>\n",
       "      <td>0.105426</td>\n",
       "      <td>0.069669</td>\n",
       "      <td>0.006313</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.010563</td>\n",
       "      <td>0.315773</td>\n",
       "      <td>0.334602</td>\n",
       "      <td>0.196531</td>\n",
       "      <td>0.458193</td>\n",
       "      <td>0.450918</td>\n",
       "      <td>0.526053</td>\n",
       "      <td>1.278021</td>\n",
       "      <td>0.366633</td>\n",
       "      <td>0.501924</td>\n",
       "      <td>0.526905</td>\n",
       "      <td>0.626809</td>\n",
       "      <td>0.533934</td>\n",
       "      <td>0.399937</td>\n",
       "      <td>0.502697</td>\n",
       "      <td>0.604580</td>\n",
       "      <td>0.302201</td>\n",
       "      <td>0.546833</td>\n",
       "      <td>0.522286</td>\n",
       "      <td>0.368751</td>\n",
       "      <td>1.452713</td>\n",
       "      <td>0.502576</td>\n",
       "      <td>0.687203</td>\n",
       "      <td>0.510752</td>\n",
       "      <td>0.608004</td>\n",
       "      <td>0.632271</td>\n",
       "      <td>0.458742</td>\n",
       "      <td>0.499368</td>\n",
       "      <td>0.964196</td>\n",
       "      <td>0.413989</td>\n",
       "      <td>0.366097</td>\n",
       "      <td>0.554588</td>\n",
       "      <td>1.122979</td>\n",
       "      <td>0.289271</td>\n",
       "      <td>0.597558</td>\n",
       "      <td>0.767396</td>\n",
       "      <td>0.567055</td>\n",
       "      <td>0.346644</td>\n",
       "      <td>0.991542</td>\n",
       "      <td>0.165603</td>\n",
       "      <td>0.715267</td>\n",
       "      <td>0.490068</td>\n",
       "      <td>0.773780</td>\n",
       "      <td>0.434031</td>\n",
       "      <td>0.289216</td>\n",
       "      <td>0.552551</td>\n",
       "      <td>0.915964</td>\n",
       "      <td>0.600612</td>\n",
       "      <td>0.985994</td>\n",
       "      <td>0.306613</td>\n",
       "      <td>0.487538</td>\n",
       "      <td>0.658591</td>\n",
       "      <td>0.379439</td>\n",
       "      <td>0.309266</td>\n",
       "      <td>1.170504</td>\n",
       "      <td>0.392955</td>\n",
       "      <td>0.621610</td>\n",
       "      <td>0.259793</td>\n",
       "      <td>0.914024</td>\n",
       "      <td>0.445330</td>\n",
       "      <td>0.630293</td>\n",
       "      <td>0.640174</td>\n",
       "      <td>0.816106</td>\n",
       "      <td>0.351082</td>\n",
       "      <td>0.833028</td>\n",
       "      <td>0.622346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72000c4c5</th>\n",
       "      <td>0.037502</td>\n",
       "      <td>0.050106</td>\n",
       "      <td>0.034061</td>\n",
       "      <td>0.006931</td>\n",
       "      <td>0.038617</td>\n",
       "      <td>0.052741</td>\n",
       "      <td>0.007860</td>\n",
       "      <td>0.022462</td>\n",
       "      <td>0.017758</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.025610</td>\n",
       "      <td>0.156507</td>\n",
       "      <td>0.098218</td>\n",
       "      <td>0.228299</td>\n",
       "      <td>0.040125</td>\n",
       "      <td>0.112434</td>\n",
       "      <td>0.078324</td>\n",
       "      <td>0.399952</td>\n",
       "      <td>0.043204</td>\n",
       "      <td>0.283079</td>\n",
       "      <td>0.060671</td>\n",
       "      <td>0.070839</td>\n",
       "      <td>0.011359</td>\n",
       "      <td>0.010377</td>\n",
       "      <td>0.105774</td>\n",
       "      <td>0.059309</td>\n",
       "      <td>0.150404</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.040381</td>\n",
       "      <td>0.278929</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.274328</td>\n",
       "      <td>0.012994</td>\n",
       "      <td>0.024581</td>\n",
       "      <td>0.047417</td>\n",
       "      <td>0.062418</td>\n",
       "      <td>0.052685</td>\n",
       "      <td>0.226046</td>\n",
       "      <td>0.056199</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>0.102997</td>\n",
       "      <td>0.004224</td>\n",
       "      <td>0.032885</td>\n",
       "      <td>0.049734</td>\n",
       "      <td>0.012410</td>\n",
       "      <td>0.023451</td>\n",
       "      <td>0.276831</td>\n",
       "      <td>0.081434</td>\n",
       "      <td>0.003758</td>\n",
       "      <td>0.006517</td>\n",
       "      <td>0.064746</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>0.171113</td>\n",
       "      <td>0.287124</td>\n",
       "      <td>0.031599</td>\n",
       "      <td>0.158322</td>\n",
       "      <td>0.082809</td>\n",
       "      <td>0.041491</td>\n",
       "      <td>0.067253</td>\n",
       "      <td>0.003629</td>\n",
       "      <td>0.005206</td>\n",
       "      <td>0.023631</td>\n",
       "      <td>0.343334</td>\n",
       "      <td>0.338089</td>\n",
       "      <td>0.324906</td>\n",
       "      <td>0.438208</td>\n",
       "      <td>0.818757</td>\n",
       "      <td>0.473350</td>\n",
       "      <td>1.204175</td>\n",
       "      <td>0.493931</td>\n",
       "      <td>0.496887</td>\n",
       "      <td>0.577731</td>\n",
       "      <td>0.564775</td>\n",
       "      <td>0.721198</td>\n",
       "      <td>0.535011</td>\n",
       "      <td>0.632743</td>\n",
       "      <td>0.652219</td>\n",
       "      <td>0.409810</td>\n",
       "      <td>0.374663</td>\n",
       "      <td>0.378863</td>\n",
       "      <td>0.471195</td>\n",
       "      <td>0.708706</td>\n",
       "      <td>0.767574</td>\n",
       "      <td>0.693229</td>\n",
       "      <td>0.433185</td>\n",
       "      <td>0.296737</td>\n",
       "      <td>0.386405</td>\n",
       "      <td>0.820593</td>\n",
       "      <td>1.000587</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.941541</td>\n",
       "      <td>0.312288</td>\n",
       "      <td>0.711103</td>\n",
       "      <td>1.291658</td>\n",
       "      <td>0.450850</td>\n",
       "      <td>0.525968</td>\n",
       "      <td>1.132253</td>\n",
       "      <td>0.558267</td>\n",
       "      <td>0.407827</td>\n",
       "      <td>1.638329</td>\n",
       "      <td>0.571406</td>\n",
       "      <td>0.462825</td>\n",
       "      <td>0.586985</td>\n",
       "      <td>0.715839</td>\n",
       "      <td>0.458237</td>\n",
       "      <td>0.718740</td>\n",
       "      <td>0.557870</td>\n",
       "      <td>0.824893</td>\n",
       "      <td>0.929462</td>\n",
       "      <td>1.217274</td>\n",
       "      <td>0.638145</td>\n",
       "      <td>0.302924</td>\n",
       "      <td>1.059461</td>\n",
       "      <td>0.219597</td>\n",
       "      <td>0.478852</td>\n",
       "      <td>0.848999</td>\n",
       "      <td>0.864279</td>\n",
       "      <td>0.367135</td>\n",
       "      <td>0.854358</td>\n",
       "      <td>0.943370</td>\n",
       "      <td>0.488411</td>\n",
       "      <td>0.724523</td>\n",
       "      <td>0.395695</td>\n",
       "      <td>1.487489</td>\n",
       "      <td>1.181651</td>\n",
       "      <td>0.963292</td>\n",
       "      <td>1.570807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e147a4b9f</th>\n",
       "      <td>0.071273</td>\n",
       "      <td>0.055146</td>\n",
       "      <td>0.063173</td>\n",
       "      <td>0.007101</td>\n",
       "      <td>0.089654</td>\n",
       "      <td>0.157869</td>\n",
       "      <td>0.006460</td>\n",
       "      <td>0.017983</td>\n",
       "      <td>0.007363</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>0.055125</td>\n",
       "      <td>0.182252</td>\n",
       "      <td>0.100993</td>\n",
       "      <td>0.206117</td>\n",
       "      <td>0.077656</td>\n",
       "      <td>0.169949</td>\n",
       "      <td>0.065301</td>\n",
       "      <td>0.297373</td>\n",
       "      <td>0.029920</td>\n",
       "      <td>0.242749</td>\n",
       "      <td>0.110174</td>\n",
       "      <td>0.120874</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.009682</td>\n",
       "      <td>0.029369</td>\n",
       "      <td>0.036270</td>\n",
       "      <td>0.238934</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.043875</td>\n",
       "      <td>0.326319</td>\n",
       "      <td>0.029494</td>\n",
       "      <td>0.006165</td>\n",
       "      <td>0.217003</td>\n",
       "      <td>0.012426</td>\n",
       "      <td>0.012992</td>\n",
       "      <td>0.065112</td>\n",
       "      <td>0.037583</td>\n",
       "      <td>0.157354</td>\n",
       "      <td>0.189831</td>\n",
       "      <td>0.035991</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.107428</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.039592</td>\n",
       "      <td>0.136870</td>\n",
       "      <td>0.010182</td>\n",
       "      <td>0.027896</td>\n",
       "      <td>0.392095</td>\n",
       "      <td>0.132668</td>\n",
       "      <td>0.009870</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.126272</td>\n",
       "      <td>0.003746</td>\n",
       "      <td>0.292218</td>\n",
       "      <td>0.317352</td>\n",
       "      <td>0.030506</td>\n",
       "      <td>0.098246</td>\n",
       "      <td>0.200996</td>\n",
       "      <td>0.129977</td>\n",
       "      <td>0.112856</td>\n",
       "      <td>0.007267</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.018493</td>\n",
       "      <td>0.138991</td>\n",
       "      <td>0.375109</td>\n",
       "      <td>0.300629</td>\n",
       "      <td>0.322883</td>\n",
       "      <td>0.274114</td>\n",
       "      <td>0.569864</td>\n",
       "      <td>0.987781</td>\n",
       "      <td>0.475167</td>\n",
       "      <td>1.095235</td>\n",
       "      <td>0.567781</td>\n",
       "      <td>0.587673</td>\n",
       "      <td>0.512507</td>\n",
       "      <td>0.345763</td>\n",
       "      <td>0.372966</td>\n",
       "      <td>0.688056</td>\n",
       "      <td>0.495267</td>\n",
       "      <td>0.589257</td>\n",
       "      <td>0.909955</td>\n",
       "      <td>0.999841</td>\n",
       "      <td>1.401905</td>\n",
       "      <td>1.129880</td>\n",
       "      <td>0.711181</td>\n",
       "      <td>0.889036</td>\n",
       "      <td>0.675256</td>\n",
       "      <td>0.744008</td>\n",
       "      <td>1.220147</td>\n",
       "      <td>0.891320</td>\n",
       "      <td>0.790061</td>\n",
       "      <td>0.759829</td>\n",
       "      <td>0.879403</td>\n",
       "      <td>1.166887</td>\n",
       "      <td>1.093494</td>\n",
       "      <td>0.985155</td>\n",
       "      <td>1.375417</td>\n",
       "      <td>1.001402</td>\n",
       "      <td>0.876193</td>\n",
       "      <td>1.793556</td>\n",
       "      <td>1.106975</td>\n",
       "      <td>1.083176</td>\n",
       "      <td>1.340941</td>\n",
       "      <td>1.159961</td>\n",
       "      <td>0.859350</td>\n",
       "      <td>1.782988</td>\n",
       "      <td>1.664184</td>\n",
       "      <td>1.163683</td>\n",
       "      <td>1.246102</td>\n",
       "      <td>0.609332</td>\n",
       "      <td>1.405037</td>\n",
       "      <td>1.099618</td>\n",
       "      <td>0.649382</td>\n",
       "      <td>1.225957</td>\n",
       "      <td>0.930254</td>\n",
       "      <td>0.759043</td>\n",
       "      <td>1.003686</td>\n",
       "      <td>1.041095</td>\n",
       "      <td>1.086146</td>\n",
       "      <td>1.181188</td>\n",
       "      <td>1.385663</td>\n",
       "      <td>0.630985</td>\n",
       "      <td>0.680483</td>\n",
       "      <td>0.379867</td>\n",
       "      <td>1.016909</td>\n",
       "      <td>1.292094</td>\n",
       "      <td>0.996778</td>\n",
       "      <td>1.078207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43fbba852</th>\n",
       "      <td>0.063748</td>\n",
       "      <td>0.035317</td>\n",
       "      <td>0.036975</td>\n",
       "      <td>0.003873</td>\n",
       "      <td>0.042042</td>\n",
       "      <td>0.063653</td>\n",
       "      <td>0.006059</td>\n",
       "      <td>0.013797</td>\n",
       "      <td>0.006523</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.047355</td>\n",
       "      <td>0.250222</td>\n",
       "      <td>0.007864</td>\n",
       "      <td>0.186313</td>\n",
       "      <td>0.025167</td>\n",
       "      <td>0.313727</td>\n",
       "      <td>0.040675</td>\n",
       "      <td>0.352359</td>\n",
       "      <td>0.036918</td>\n",
       "      <td>0.536727</td>\n",
       "      <td>0.099861</td>\n",
       "      <td>0.047764</td>\n",
       "      <td>0.012520</td>\n",
       "      <td>0.006457</td>\n",
       "      <td>0.073195</td>\n",
       "      <td>0.030105</td>\n",
       "      <td>0.142776</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.009923</td>\n",
       "      <td>0.247909</td>\n",
       "      <td>0.013985</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>0.131800</td>\n",
       "      <td>0.005999</td>\n",
       "      <td>0.015766</td>\n",
       "      <td>0.052328</td>\n",
       "      <td>0.026891</td>\n",
       "      <td>0.067437</td>\n",
       "      <td>0.139621</td>\n",
       "      <td>0.015296</td>\n",
       "      <td>0.003765</td>\n",
       "      <td>0.095621</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0.044547</td>\n",
       "      <td>0.067574</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>0.018976</td>\n",
       "      <td>0.331504</td>\n",
       "      <td>0.066930</td>\n",
       "      <td>0.008027</td>\n",
       "      <td>0.007206</td>\n",
       "      <td>0.032980</td>\n",
       "      <td>0.002943</td>\n",
       "      <td>0.253995</td>\n",
       "      <td>0.423106</td>\n",
       "      <td>0.071004</td>\n",
       "      <td>0.093567</td>\n",
       "      <td>0.418462</td>\n",
       "      <td>0.115994</td>\n",
       "      <td>0.055149</td>\n",
       "      <td>0.006420</td>\n",
       "      <td>0.002672</td>\n",
       "      <td>0.020206</td>\n",
       "      <td>0.229727</td>\n",
       "      <td>0.497733</td>\n",
       "      <td>0.224861</td>\n",
       "      <td>0.253585</td>\n",
       "      <td>0.460645</td>\n",
       "      <td>0.886743</td>\n",
       "      <td>0.863309</td>\n",
       "      <td>0.668654</td>\n",
       "      <td>0.564627</td>\n",
       "      <td>0.253530</td>\n",
       "      <td>0.515981</td>\n",
       "      <td>0.649569</td>\n",
       "      <td>0.216576</td>\n",
       "      <td>0.545469</td>\n",
       "      <td>0.586237</td>\n",
       "      <td>0.660649</td>\n",
       "      <td>0.428468</td>\n",
       "      <td>0.429280</td>\n",
       "      <td>0.624565</td>\n",
       "      <td>0.897288</td>\n",
       "      <td>0.807497</td>\n",
       "      <td>0.449550</td>\n",
       "      <td>0.381019</td>\n",
       "      <td>0.655371</td>\n",
       "      <td>0.621315</td>\n",
       "      <td>0.862708</td>\n",
       "      <td>1.067131</td>\n",
       "      <td>0.896304</td>\n",
       "      <td>0.514717</td>\n",
       "      <td>0.800848</td>\n",
       "      <td>1.439869</td>\n",
       "      <td>0.988998</td>\n",
       "      <td>0.520423</td>\n",
       "      <td>1.207341</td>\n",
       "      <td>1.213966</td>\n",
       "      <td>1.153998</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.591150</td>\n",
       "      <td>0.469768</td>\n",
       "      <td>0.756539</td>\n",
       "      <td>0.986116</td>\n",
       "      <td>0.545458</td>\n",
       "      <td>0.462715</td>\n",
       "      <td>0.945431</td>\n",
       "      <td>0.690648</td>\n",
       "      <td>1.044523</td>\n",
       "      <td>0.706745</td>\n",
       "      <td>0.641306</td>\n",
       "      <td>0.709894</td>\n",
       "      <td>0.647253</td>\n",
       "      <td>0.430361</td>\n",
       "      <td>0.732715</td>\n",
       "      <td>0.409872</td>\n",
       "      <td>0.941131</td>\n",
       "      <td>0.596771</td>\n",
       "      <td>1.157225</td>\n",
       "      <td>0.774777</td>\n",
       "      <td>0.958384</td>\n",
       "      <td>0.607934</td>\n",
       "      <td>0.517225</td>\n",
       "      <td>0.730382</td>\n",
       "      <td>0.709576</td>\n",
       "      <td>1.821009</td>\n",
       "      <td>1.047257</td>\n",
       "      <td>0.976913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5    \\\n",
       "378fcc4fc  0.046349  0.029440  0.044969  0.005820  0.043166  0.084996   \n",
       "73c10e136  0.045589  0.052937  0.035650  0.005980  0.055054  0.105500   \n",
       "72000c4c5  0.037502  0.050106  0.034061  0.006931  0.038617  0.052741   \n",
       "e147a4b9f  0.071273  0.055146  0.063173  0.007101  0.089654  0.157869   \n",
       "43fbba852  0.063748  0.035317  0.036975  0.003873  0.042042  0.063653   \n",
       "\n",
       "                6         7         8         9         10        11   \\\n",
       "378fcc4fc  0.006374  0.012181  0.012883  0.002154  0.049012  0.113741   \n",
       "73c10e136  0.006685  0.019189  0.009906  0.003125  0.032303  0.137175   \n",
       "72000c4c5  0.007860  0.022462  0.017758  0.003180  0.025610  0.156507   \n",
       "e147a4b9f  0.006460  0.017983  0.007363  0.003201  0.055125  0.182252   \n",
       "43fbba852  0.006059  0.013797  0.006523  0.002535  0.047355  0.250222   \n",
       "\n",
       "                12        13        14        15        16        17   \\\n",
       "378fcc4fc  0.166774  0.214079  0.117198  0.170769  0.083287  0.369204   \n",
       "73c10e136  0.037631  0.233158  0.064486  0.159466  0.067773  0.457415   \n",
       "72000c4c5  0.098218  0.228299  0.040125  0.112434  0.078324  0.399952   \n",
       "e147a4b9f  0.100993  0.206117  0.077656  0.169949  0.065301  0.297373   \n",
       "43fbba852  0.007864  0.186313  0.025167  0.313727  0.040675  0.352359   \n",
       "\n",
       "                18        19        20        21        22        23   \\\n",
       "378fcc4fc  0.023654  0.210734  0.075803  0.077567  0.008761  0.006844   \n",
       "73c10e136  0.028972  0.203293  0.074134  0.066226  0.009930  0.007108   \n",
       "72000c4c5  0.043204  0.283079  0.060671  0.070839  0.011359  0.010377   \n",
       "e147a4b9f  0.029920  0.242749  0.110174  0.120874  0.011839  0.009682   \n",
       "43fbba852  0.036918  0.536727  0.099861  0.047764  0.012520  0.006457   \n",
       "\n",
       "                24        25        26        27        28        29   \\\n",
       "378fcc4fc  0.064534  0.044727  0.163834  0.001316  0.009899  0.404045   \n",
       "73c10e136  0.085333  0.056994  0.159706  0.001488  0.013014  0.254318   \n",
       "72000c4c5  0.105774  0.059309  0.150404  0.001649  0.040381  0.278929   \n",
       "e147a4b9f  0.029369  0.036270  0.238934  0.001332  0.043875  0.326319   \n",
       "43fbba852  0.073195  0.030105  0.142776  0.001311  0.009923  0.247909   \n",
       "\n",
       "                30        31        32        33        34        35   \\\n",
       "378fcc4fc  0.032993  0.004249  0.164170  0.006376  0.011148  0.047406   \n",
       "73c10e136  0.030654  0.004489  0.254228  0.007231  0.015752  0.055318   \n",
       "72000c4c5  0.028854  0.004602  0.274328  0.012994  0.024581  0.047417   \n",
       "e147a4b9f  0.029494  0.006165  0.217003  0.012426  0.012992  0.065112   \n",
       "43fbba852  0.013985  0.003276  0.131800  0.005999  0.015766  0.052328   \n",
       "\n",
       "                36        37        38        39        40        41   \\\n",
       "378fcc4fc  0.049476  0.117032  0.199826  0.057105  0.003518  0.080790   \n",
       "73c10e136  0.038069  0.055355  0.145451  0.055705  0.004485  0.100308   \n",
       "72000c4c5  0.062418  0.052685  0.226046  0.056199  0.004751  0.102997   \n",
       "e147a4b9f  0.037583  0.157354  0.189831  0.035991  0.004140  0.107428   \n",
       "43fbba852  0.026891  0.067437  0.139621  0.015296  0.003765  0.095621   \n",
       "\n",
       "                42        43        44        45        46        47   \\\n",
       "378fcc4fc  0.002620  0.041794  0.098319  0.009090  0.021795  0.315340   \n",
       "73c10e136  0.002933  0.036124  0.063251  0.012044  0.022930  0.305344   \n",
       "72000c4c5  0.004224  0.032885  0.049734  0.012410  0.023451  0.276831   \n",
       "e147a4b9f  0.003690  0.039592  0.136870  0.010182  0.027896  0.392095   \n",
       "43fbba852  0.002168  0.044547  0.067574  0.009020  0.018976  0.331504   \n",
       "\n",
       "                48        49        50        51        52        53   \\\n",
       "378fcc4fc  0.063945  0.004730  0.006920  0.070421  0.003485  0.202767   \n",
       "73c10e136  0.100218  0.005209  0.007967  0.063378  0.002757  0.171008   \n",
       "72000c4c5  0.081434  0.003758  0.006517  0.064746  0.002303  0.171113   \n",
       "e147a4b9f  0.132668  0.009870  0.006920  0.126272  0.003746  0.292218   \n",
       "43fbba852  0.066930  0.008027  0.007206  0.032980  0.002943  0.253995   \n",
       "\n",
       "                54        55        56        57        58        59   \\\n",
       "378fcc4fc  0.226186  0.036033  0.109722  0.176980  0.103157  0.074406   \n",
       "73c10e136  0.277723  0.030621  0.093463  0.151823  0.105426  0.069669   \n",
       "72000c4c5  0.287124  0.031599  0.158322  0.082809  0.041491  0.067253   \n",
       "e147a4b9f  0.317352  0.030506  0.098246  0.200996  0.129977  0.112856   \n",
       "43fbba852  0.423106  0.071004  0.093567  0.418462  0.115994  0.055149   \n",
       "\n",
       "                60        61        62        63        64        65   \\\n",
       "378fcc4fc  0.004510  0.003517  0.017212  0.316111  0.632204  0.349130   \n",
       "73c10e136  0.006313  0.004545  0.010563  0.315773  0.334602  0.196531   \n",
       "72000c4c5  0.003629  0.005206  0.023631  0.343334  0.338089  0.324906   \n",
       "e147a4b9f  0.007267  0.004949  0.018493  0.138991  0.375109  0.300629   \n",
       "43fbba852  0.006420  0.002672  0.020206  0.229727  0.497733  0.224861   \n",
       "\n",
       "                66        67        68        69        70        71   \\\n",
       "378fcc4fc  0.661242  0.224366  0.411741  1.005475  0.777165  0.706809   \n",
       "73c10e136  0.458193  0.450918  0.526053  1.278021  0.366633  0.501924   \n",
       "72000c4c5  0.438208  0.818757  0.473350  1.204175  0.493931  0.496887   \n",
       "e147a4b9f  0.322883  0.274114  0.569864  0.987781  0.475167  1.095235   \n",
       "43fbba852  0.253585  0.460645  0.886743  0.863309  0.668654  0.564627   \n",
       "\n",
       "                72        73        74        75        76        77   \\\n",
       "378fcc4fc  0.366492  0.469502  0.788777  0.402848  0.746356  0.456743   \n",
       "73c10e136  0.526905  0.626809  0.533934  0.399937  0.502697  0.604580   \n",
       "72000c4c5  0.577731  0.564775  0.721198  0.535011  0.632743  0.652219   \n",
       "e147a4b9f  0.567781  0.587673  0.512507  0.345763  0.372966  0.688056   \n",
       "43fbba852  0.253530  0.515981  0.649569  0.216576  0.545469  0.586237   \n",
       "\n",
       "                78        79        80        81        82        83   \\\n",
       "378fcc4fc  0.512674  0.581825  0.606519  0.716501  0.830395  0.817831   \n",
       "73c10e136  0.302201  0.546833  0.522286  0.368751  1.452713  0.502576   \n",
       "72000c4c5  0.409810  0.374663  0.378863  0.471195  0.708706  0.767574   \n",
       "e147a4b9f  0.495267  0.589257  0.909955  0.999841  1.401905  1.129880   \n",
       "43fbba852  0.660649  0.428468  0.429280  0.624565  0.897288  0.807497   \n",
       "\n",
       "                84        85        86        87        88        89   \\\n",
       "378fcc4fc  0.843343  0.853221  0.477257  1.366606  0.367085  0.759267   \n",
       "73c10e136  0.687203  0.510752  0.608004  0.632271  0.458742  0.499368   \n",
       "72000c4c5  0.693229  0.433185  0.296737  0.386405  0.820593  1.000587   \n",
       "e147a4b9f  0.711181  0.889036  0.675256  0.744008  1.220147  0.891320   \n",
       "43fbba852  0.449550  0.381019  0.655371  0.621315  0.862708  1.067131   \n",
       "\n",
       "                90        91        92        93        94        95   \\\n",
       "378fcc4fc  0.444994  0.368155  0.848210  0.475449  1.418633  0.340132   \n",
       "73c10e136  0.964196  0.413989  0.366097  0.554588  1.122979  0.289271   \n",
       "72000c4c5  0.926746  0.941541  0.312288  0.711103  1.291658  0.450850   \n",
       "e147a4b9f  0.790061  0.759829  0.879403  1.166887  1.093494  0.985155   \n",
       "43fbba852  0.896304  0.514717  0.800848  1.439869  0.988998  0.520423   \n",
       "\n",
       "                96        97        98        99        100       101  \\\n",
       "378fcc4fc  0.615128  0.869077  0.535458  0.781738  1.459623  1.053890   \n",
       "73c10e136  0.597558  0.767396  0.567055  0.346644  0.991542  0.165603   \n",
       "72000c4c5  0.525968  1.132253  0.558267  0.407827  1.638329  0.571406   \n",
       "e147a4b9f  1.375417  1.001402  0.876193  1.793556  1.106975  1.083176   \n",
       "43fbba852  1.207341  1.213966  1.153998  0.195709  0.591150  0.469768   \n",
       "\n",
       "                102       103       104       105       106       107  \\\n",
       "378fcc4fc  1.426533  0.935947  1.259884  0.764008  0.798453  0.469986   \n",
       "73c10e136  0.715267  0.490068  0.773780  0.434031  0.289216  0.552551   \n",
       "72000c4c5  0.462825  0.586985  0.715839  0.458237  0.718740  0.557870   \n",
       "e147a4b9f  1.340941  1.159961  0.859350  1.782988  1.664184  1.163683   \n",
       "43fbba852  0.756539  0.986116  0.545458  0.462715  0.945431  0.690648   \n",
       "\n",
       "                108       109       110       111       112       113  \\\n",
       "378fcc4fc  1.376810  0.760331  0.792629  0.538545  0.475917  0.986913   \n",
       "73c10e136  0.915964  0.600612  0.985994  0.306613  0.487538  0.658591   \n",
       "72000c4c5  0.824893  0.929462  1.217274  0.638145  0.302924  1.059461   \n",
       "e147a4b9f  1.246102  0.609332  1.405037  1.099618  0.649382  1.225957   \n",
       "43fbba852  1.044523  0.706745  0.641306  0.709894  0.647253  0.430361   \n",
       "\n",
       "                114       115       116       117       118       119  \\\n",
       "378fcc4fc  0.783445  0.452399  0.940398  1.184496  0.706118  0.780622   \n",
       "73c10e136  0.379439  0.309266  1.170504  0.392955  0.621610  0.259793   \n",
       "72000c4c5  0.219597  0.478852  0.848999  0.864279  0.367135  0.854358   \n",
       "e147a4b9f  0.930254  0.759043  1.003686  1.041095  1.086146  1.181188   \n",
       "43fbba852  0.732715  0.409872  0.941131  0.596771  1.157225  0.774777   \n",
       "\n",
       "                120       121       122       123       124       125  \\\n",
       "378fcc4fc  1.099451  1.113779  0.588675  1.029418  0.421483  0.689350   \n",
       "73c10e136  0.914024  0.445330  0.630293  0.640174  0.816106  0.351082   \n",
       "72000c4c5  0.943370  0.488411  0.724523  0.395695  1.487489  1.181651   \n",
       "e147a4b9f  1.385663  0.630985  0.680483  0.379867  1.016909  1.292094   \n",
       "43fbba852  0.958384  0.607934  0.517225  0.730382  0.709576  1.821009   \n",
       "\n",
       "                126       127  \n",
       "378fcc4fc  0.818461  0.732772  \n",
       "73c10e136  0.833028  0.622346  \n",
       "72000c4c5  0.963292  1.570807  \n",
       "e147a4b9f  0.996778  1.078207  \n",
       "43fbba852  1.047257  0.976913  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pet_ids = test_id.values\n",
    "n_batches = len(pet_ids) // batch_size + 1\n",
    "\n",
    "features = {}\n",
    "for b in tqdm_notebook(range(n_batches)):\n",
    "    start = b*batch_size\n",
    "    end = (b+1)*batch_size\n",
    "    batch_pets = pet_ids[start:end]\n",
    "    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n",
    "    for i,pet_id in enumerate(batch_pets):\n",
    "        try:\n",
    "            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/test_images/\", pet_id)\n",
    "        except:\n",
    "            pass\n",
    "    batch_preds = m.predict(batch_images)\n",
    "    for i,pet_id in enumerate(batch_pets):\n",
    "        features[pet_id] = batch_preds[i]\n",
    "        \n",
    "test_feats = pd.DataFrame.from_dict(features, orient='index')\n",
    "# test_feats.to_csv('test_img_features.csv')\n",
    "test_feats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_uuid": "49c1c8cc0730b7f93b38d1798f66dae6940aec7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 327) (3948, 326)\n"
     ]
    }
   ],
   "source": [
    "train_feats.columns = [\"img_feat{}\".format(i) for i in range(n_img_features)]\n",
    "test_feats.columns = [\"img_feat{}\".format(i) for i in range(n_img_features)]\n",
    "\n",
    "train_feats[\"PetID\"] = train_feats.index\n",
    "test_feats[\"PetID\"] = test_feats.index\n",
    "\n",
    "train = pd.merge(train, train_feats, on=\"PetID\")\n",
    "test = pd.merge(test, test_feats, on=\"PetID\")\n",
    "\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_uuid": "36c070d5e964ee142985e4cbfc8c551dea68e81e"
   },
   "outputs": [],
   "source": [
    "train.drop(['AdoptionSpeed', 'PetID'], axis=1, inplace=True)\n",
    "test.drop(['PetID'], axis=1, inplace=True)\n",
    "train['ResNet_meta'] = train_img_prob.flatten()         #ImageMeta\n",
    "test['ResNet_meta'] = test_img_prob.flatten()           #ImageMeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6188c89d617cd775c60a51c15305658fe8807d1b"
   },
   "source": [
    "## SAVE !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_uuid": "9bba84efaff64f065da8b7d52a1c0d59f15b0f86"
   },
   "outputs": [],
   "source": [
    "train.to_csv(\"train_zyl.csv\", index=False)\n",
    "test.to_csv(\"test_zyl.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a852302f5cd353635c8e25655e872e5839414fff"
   },
   "source": [
    "## 这里留给 LR ETC 等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_uuid": "1741012cc747e53334a0345ea8c8a68ffccbccde"
   },
   "outputs": [],
   "source": [
    "# 占位用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b2ea3e1cbb6fdac6069f3a0b8d73920911081ed"
   },
   "source": [
    "## LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_uuid": "64584c98c2dbb6392adc3cd8d9fe58f8bddccfd1"
   },
   "outputs": [],
   "source": [
    "train.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)\n",
    "test.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)\n",
    "\n",
    "# rearrange columns again\n",
    "c = ['Type', 'Age', 'Breed1', 'Breed2', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'Quantity', 'Fee', 'State', 'VideoAmt', 'PhotoAmt', 'Color'] +  [\"img_feat{}\".format(i) for i in range(n_img_features)] + ['doc_sent_mag', 'doc_sent_score'] + ['svd_{}'.format(i) for i in range(n_components)] + ['img_x', 'img_y', 'bounding_importance', 'dominant_blue', 'dominant_green', 'dominant_red', 'dominant_pixel_frac', 'dominant_score','label_description', 'label_score', 'vertex_x_ratio', 'vertex_y_ratio', 'name_length', 'name_cnt', 'desc_length', 'desc_words', 'desc_lexical_density', 'sentences_count', 'desc_capitals', 'rescuer_cnt', 'state_gdp', 'state_population', 'Pure_breed']\n",
    "train = train[c]\n",
    "test = test[c]\n",
    "\n",
    "numeric_cols = ['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'doc_sent_mag', 'doc_sent_score', 'dominant_score', 'dominant_pixel_frac', 'dominant_red', 'dominant_green', 'dominant_blue', 'bounding_importance', 'img_x', 'img_y', 'vertex_x_ratio', 'vertex_y_ratio', 'label_score', 'desc_length', 'desc_words', 'desc_lexical_density', 'sentences_count', 'desc_capitals', 'rescuer_cnt', 'state_gdp', 'state_population', 'Pure_breed', 'name_length', 'name_cnt'] + ['svd_{}'.format(i) for i in range(n_components)] + [\"img_feat{}\".format(i) for i in range(n_img_features)]\n",
    "cat_cols = list(set(train.columns) - set(numeric_cols))\n",
    "\n",
    "train.loc[:, cat_cols] = train[cat_cols].astype('category')\n",
    "test.loc[:, cat_cols] = test[cat_cols].astype('category')\n",
    "\n",
    "foo = train.dtypes\n",
    "cat_feature_names = foo[foo == \"category\"]\n",
    "cat_features = [train.columns.get_loc(c) for c in train.columns if c in cat_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_uuid": "d15e605c71a60c8f81dd77430e504be889bd1bc1"
   },
   "outputs": [],
   "source": [
    "del run_cv_model\n",
    "gc.collect()\n",
    "\n",
    "def run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, label='model'):\n",
    "    kf = FOLDS\n",
    "    n_splits = N_FOLDS\n",
    "    \n",
    "    fold_splits = kf.split(train, target)\n",
    "    cv_scores = []\n",
    "    qwk_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros((train.shape[0], n_splits))\n",
    "    all_coefficients = np.zeros((n_splits, 4))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    i = 1\n",
    "    for dev_index, val_index in fold_splits:\n",
    "        print('Started ' + label + ' fold ' + str(i) + '/' + str(n_splits))\n",
    "        if isinstance(train, pd.DataFrame):\n",
    "            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n",
    "            dev_y, val_y = target[dev_index], target[val_index]\n",
    "        else:\n",
    "            dev_X, val_X = train[dev_index], train[val_index]\n",
    "            dev_y, val_y = target[dev_index], target[val_index]\n",
    "        params2 = params.copy()\n",
    "        pred_val_y, pred_test_y, importances, coefficients, qwk = model_fn(dev_X, dev_y, val_X, val_y, test, params2)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index] = pred_val_y\n",
    "        all_coefficients[i-1, :] = coefficients\n",
    "        if eval_fn is not None:\n",
    "            cv_score = eval_fn(val_y, pred_val_y)\n",
    "            cv_scores.append(cv_score)\n",
    "            qwk_scores.append(qwk)\n",
    "            print(label + ' cv score {}: RMSE {} QWK {}'.format(i, cv_score, qwk))\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = train.columns.values\n",
    "        fold_importance_df['importance'] = importances\n",
    "        fold_importance_df['fold'] = i\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        i += 1\n",
    "    print('{} cv RMSE scores : {}'.format(label, cv_scores))\n",
    "    print('{} cv mean RMSE score : {}'.format(label, np.mean(cv_scores)))\n",
    "    print('{} cv std RMSE score : {}'.format(label, np.mean(cv_scores)))\n",
    "    print('{} cv QWK scores : {}'.format(label, qwk_scores))\n",
    "    print('{} cv mean QWK score : {}'.format(label, np.mean(qwk_scores)))\n",
    "    print('{} cv std QWK score : {}'.format(label, np.std(qwk_scores)))\n",
    "    pred_full_test = pred_full_test / float(n_splits)\n",
    "    results = {'label': label,\n",
    "               'train': pred_train, 'test': pred_full_test,\n",
    "                'cv': cv_scores, 'qwk': qwk_scores,\n",
    "               'importance': feature_importance_df,\n",
    "               'coefficients': all_coefficients}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "_uuid": "c855930ec4501953a8b18a8fc10e602e89e229b9"
   },
   "outputs": [],
   "source": [
    "del runLGB\n",
    "gc.collect()\n",
    "\n",
    "def runLGB(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "    d_train = lgb.Dataset(train_X, label=train_y)\n",
    "    d_valid = lgb.Dataset(test_X, label=test_y)\n",
    "    watchlist = [d_train, d_valid]\n",
    "    print('Train LGB')\n",
    "    try:\n",
    "        num_rounds = params.pop('num_rounds')\n",
    "    except:\n",
    "        pass\n",
    "    verbose_eval = params.pop('verbose_eval')\n",
    "    early_stop = None\n",
    "    if params.get('early_stop'):\n",
    "        early_stop = params.pop('early_stop')\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=10000,\n",
    "                      valid_sets=watchlist,\n",
    "                      verbose_eval=verbose_eval,\n",
    "#                       categorical_feature=list(cat_features),\n",
    "                      callbacks=[lgb.reset_parameter(learning_rate=[0.005]*1000+[0.003]*1000+[0.001]*8000)],\n",
    "                      early_stopping_rounds=early_stop)\n",
    "\n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    init_coef = get_init_coefs(pred_test_y, test_y)\n",
    "    optR = OptimizedRounder_v2(initial_coefs=init_coef)\n",
    "    optR.fit(pred_test_y, test_y)\n",
    "    coefficients = optR.coefficients()\n",
    "    pred_test_y_k = optR.predict(pred_test_y, coefficients)\n",
    "    chi2 = get_chi2(pred_test_y_k, test_y)\n",
    "    print(\"Valid Counts = {}\".format(Counter(test_y)))\n",
    "    print(\"Predicted Counts = {}\".format(Counter(pred_test_y_k)))\n",
    "    print(\"Coefficients = {}\".format(coefficients))\n",
    "    print(\"Chi2 = {}\".format(chi2))\n",
    "    qwk = quadratic_weighted_kappa(test_y, pred_test_y_k)\n",
    "    print(\"QWK = {}\".format(qwk))\n",
    "    print('Predict 2/2')\n",
    "    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance(), coefficients, qwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_uuid": "f93e40673af588c956a81a785791b96725e6dda2"
   },
   "outputs": [],
   "source": [
    "param = {'application': 'regression',\n",
    "         'boosting': 'gbdt', \n",
    "         'metric': 'rmse', \n",
    "         'num_leaves': 149, \n",
    "         'max_depth': 11, \n",
    "         'max_bin': 37, \n",
    "         'bagging_fraction': 0.975419815153193, \n",
    "         'bagging_freq': 1, \n",
    "         'feature_fraction': 0.2705570927694394, \n",
    "         'min_split_gain': 0.7636472013417633, \n",
    "         'min_child_samples': 29, \n",
    "         'min_child_weight': 0.13126728393897313, \n",
    "         'lambda_l2': 0.841358003322472, \n",
    "         'verbosity': -1, \n",
    "         'data_random_seed': 1029, \n",
    "         'early_stop': 100, \n",
    "         'verbose_eval': 2000, \n",
    "         'num_rounds': 10000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_uuid": "7afbe7c002b1e2999348d1c7ed4872313e5af739",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started lgb fold 1/4\n",
      "Train LGB\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2000]\ttraining's rmse: 0.479201\tvalid_1's rmse: 1.03139\n",
      "[4000]\ttraining's rmse: 0.468505\tvalid_1's rmse: 1.02951\n",
      "[6000]\ttraining's rmse: 0.464839\tvalid_1's rmse: 1.02874\n",
      "[8000]\ttraining's rmse: 0.463065\tvalid_1's rmse: 1.02837\n",
      "Early stopping, best iteration is:\n",
      "[8430]\ttraining's rmse: 0.462817\tvalid_1's rmse: 1.02833\n",
      "Predict 1/2\n",
      "Valid Counts = Counter({4: 1050, 2: 1010, 3: 815, 1: 773, 0: 103})\n",
      "Predicted Counts = Counter({4: 1050, 2: 967, 1: 835, 3: 805, 0: 94})\n",
      "Coefficients = [1.6580913  2.1756028  2.49596728 2.79969562]\n",
      "Chi2 = 7.712633340523446\n",
      "QWK = 0.47771905184309094\n",
      "Predict 2/2\n",
      "lgb cv score 1: RMSE 1.0283263588248486 QWK 0.47771905184309094\n",
      "Started lgb fold 2/4\n",
      "Train LGB\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2000]\ttraining's rmse: 0.477637\tvalid_1's rmse: 1.02442\n",
      "[4000]\ttraining's rmse: 0.467012\tvalid_1's rmse: 1.02288\n",
      "[6000]\ttraining's rmse: 0.463164\tvalid_1's rmse: 1.02229\n",
      "Early stopping, best iteration is:\n",
      "[7772]\ttraining's rmse: 0.461693\tvalid_1's rmse: 1.02209\n",
      "Predict 1/2\n",
      "Valid Counts = Counter({4: 1049, 2: 1009, 3: 815, 1: 773, 0: 103})\n",
      "Predicted Counts = Counter({4: 1056, 2: 1001, 3: 812, 1: 811, 0: 69})\n",
      "Coefficients = [1.58237123 2.1498582  2.49154173 2.80312978]\n",
      "Chi2 = 13.212530778696914\n",
      "QWK = 0.49082363766823967\n",
      "Predict 2/2\n",
      "lgb cv score 2: RMSE 1.0220896579011014 QWK 0.49082363766823967\n",
      "Started lgb fold 3/4\n",
      "Train LGB\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2000]\ttraining's rmse: 0.479271\tvalid_1's rmse: 1.02038\n",
      "[4000]\ttraining's rmse: 0.468513\tvalid_1's rmse: 1.01878\n",
      "[6000]\ttraining's rmse: 0.464687\tvalid_1's rmse: 1.01809\n",
      "Early stopping, best iteration is:\n",
      "[7224]\ttraining's rmse: 0.463436\tvalid_1's rmse: 1.01791\n",
      "Predict 1/2\n",
      "Valid Counts = Counter({4: 1049, 2: 1009, 3: 815, 1: 772, 0: 102})\n",
      "Predicted Counts = Counter({2: 1142, 4: 875, 3: 863, 1: 727, 0: 140})\n",
      "Coefficients = [1.71827962 2.13726256 2.53752349 2.87804241]\n",
      "Chi2 = 65.99990575094323\n",
      "QWK = 0.49541118794309846\n",
      "Predict 2/2\n",
      "lgb cv score 3: RMSE 1.0179153701877672 QWK 0.49541118794309846\n",
      "Started lgb fold 4/4\n",
      "Train LGB\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2000]\ttraining's rmse: 0.474416\tvalid_1's rmse: 1.0199\n",
      "[4000]\ttraining's rmse: 0.463026\tvalid_1's rmse: 1.01785\n",
      "[6000]\ttraining's rmse: 0.458961\tvalid_1's rmse: 1.01709\n",
      "[8000]\ttraining's rmse: 0.457164\tvalid_1's rmse: 1.01679\n",
      "Early stopping, best iteration is:\n",
      "[8597]\ttraining's rmse: 0.45681\tvalid_1's rmse: 1.01675\n",
      "Predict 1/2\n",
      "Valid Counts = Counter({4: 1049, 2: 1009, 3: 814, 1: 772, 0: 102})\n",
      "Predicted Counts = Counter({2: 1088, 4: 1031, 1: 789, 3: 728, 0: 110})\n",
      "Coefficients = [1.68384398 2.14742293 2.51199661 2.79347184]\n",
      "Chi2 = 16.581995996159065\n",
      "QWK = 0.4982787548475045\n",
      "Predict 2/2\n",
      "lgb cv score 4: RMSE 1.0167474724664947 QWK 0.4982787548475045\n",
      "lgb cv RMSE scores : [1.0283263588248486, 1.0220896579011014, 1.0179153701877672, 1.0167474724664947]\n",
      "lgb cv mean RMSE score : 1.021269714845053\n",
      "lgb cv std RMSE score : 1.021269714845053\n",
      "lgb cv QWK scores : [0.47771905184309094, 0.49082363766823967, 0.49541118794309846, 0.4982787548475045]\n",
      "lgb cv mean QWK score : 0.4905581580754834\n",
      "lgb cv std QWK score : 0.007875160977290172\n"
     ]
    }
   ],
   "source": [
    "lgb_zyl = run_cv_model(train, test, target, runLGB, param, rmse, 'lgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "026dcc3c53c0477c22c78792d8f60d5bbf557634"
   },
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_uuid": "8ef862d77491b82ff2810aa463b0dfbf9928f600"
   },
   "outputs": [],
   "source": [
    "# 先占位"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c4423eb273fd68e78b62bb98f2cf78afeccda55d"
   },
   "source": [
    "# 453\n",
    "\n",
    "https://www.kaggle.com/ranjoranjan/single-xgboost-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_uuid": "232ecf92e020ca40bf18de33baeeccf1c9217ff8"
   },
   "outputs": [],
   "source": [
    "# 重新导入, 一了百了\n",
    "\n",
    "del train, test\n",
    "gc.collect()\n",
    "\n",
    "train = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\n",
    "test = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_uuid": "fdb22b8759e96bca303862efd9ed59cb09686c2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:24<00:00,  6.51it/s]\n",
      "100%|██████████| 247/247 [00:34<00:00,  7.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#TODO: 这里可以避免重复提取的, 后面要记得优化!!!\n",
    "\n",
    "inp = Input((256,256,3))\n",
    "backbone = DenseNet121(input_tensor = inp, \n",
    "                       weights=\"../input/densenet-keras/DenseNet-BC-121-32-no-top.h5\",\n",
    "                       include_top = False)\n",
    "x = backbone.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\n",
    "x = AveragePooling1D(4)(x)\n",
    "out = Lambda(lambda x: x[:,:,0])(x)\n",
    "\n",
    "m = Model(inp,out)\n",
    "\n",
    "pet_ids = train['PetID'].values\n",
    "n_batches = len(pet_ids) // batch_size + 1\n",
    "\n",
    "features = {}\n",
    "for b in tqdm(range(n_batches)):\n",
    "    start = b*batch_size\n",
    "    end = (b+1)*batch_size\n",
    "    batch_pets = pet_ids[start:end]\n",
    "    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n",
    "    for i,pet_id in enumerate(batch_pets):\n",
    "        try:\n",
    "            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/train_images/\", pet_id)\n",
    "        except:\n",
    "            pass\n",
    "    batch_preds = m.predict(batch_images)\n",
    "    for i,pet_id in enumerate(batch_pets):\n",
    "        features[pet_id] = batch_preds[i]\n",
    "        \n",
    "train_feats = pd.DataFrame.from_dict(features, orient='index')\n",
    "train_feats.columns = [f'pic_{i}' for i in range(train_feats.shape[1])]\n",
    "\n",
    "pet_ids = test['PetID'].values\n",
    "n_batches = len(pet_ids) // batch_size + 1\n",
    "\n",
    "features = {}\n",
    "for b in tqdm(range(n_batches)):\n",
    "    start = b*batch_size\n",
    "    end = (b+1)*batch_size\n",
    "    batch_pets = pet_ids[start:end]\n",
    "    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n",
    "    for i,pet_id in enumerate(batch_pets):\n",
    "        try:\n",
    "            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/test_images/\", pet_id)\n",
    "        except:\n",
    "            pass\n",
    "    batch_preds = m.predict(batch_images)\n",
    "    for i,pet_id in enumerate(batch_pets):\n",
    "        features[pet_id] = batch_preds[i]\n",
    "        \n",
    "test_feats = pd.DataFrame.from_dict(features, orient='index')\n",
    "test_feats.columns = [f'pic_{i}' for i in range(test_feats.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_uuid": "94de1ca1d5818efa8ccf7a681fe2617634487dd0"
   },
   "outputs": [],
   "source": [
    "train_feats = train_feats.reset_index()\n",
    "train_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n",
    "\n",
    "test_feats = test_feats.reset_index()\n",
    "test_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "_uuid": "368130e50ffdbe332f52a8dd576348eb1d1e180b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18941, 1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = pd.concat([train, test], axis=0, ignore_index=True, sort=False)[['PetID']]\n",
    "all_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "_uuid": "edc65a21c0797ac84624706173fd9fd59bec9c9f"
   },
   "outputs": [],
   "source": [
    "n_components = 32\n",
    "svd_ = TruncatedSVD(n_components=n_components, random_state=1337)\n",
    "\n",
    "features_df = pd.concat([train_feats, test_feats], axis=0)\n",
    "features = features_df[[f'pic_{i}' for i in range(256)]].values\n",
    "\n",
    "svd_col = svd_.fit_transform(features)\n",
    "svd_col = pd.DataFrame(svd_col)\n",
    "svd_col = svd_col.add_prefix('IMG_SVD_')\n",
    "\n",
    "img_features = pd.concat([all_ids, svd_col], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_uuid": "4355fbe6695b1e4243a8a744f7a8d6c0e98e283d"
   },
   "outputs": [],
   "source": [
    "labels_breed = pd.read_csv('../input/petfinder-adoption-prediction/breed_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_uuid": "6431035e3c414c5f40913c1fe5ece1b47a7b0041"
   },
   "outputs": [],
   "source": [
    "train_image_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_images/*.jpg'))\n",
    "train_metadata_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_metadata/*.json'))\n",
    "train_sentiment_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_sentiment/*.json'))\n",
    "\n",
    "test_image_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_images/*.jpg'))\n",
    "test_metadata_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_metadata/*.json'))\n",
    "test_sentiment_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_sentiment/*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "_uuid": "05acad8dfc7ec249c0eb5a1291d02742b9367073"
   },
   "outputs": [],
   "source": [
    "split_char = '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "_uuid": "2275c76dcc0085761e61a42694f54254004ea162"
   },
   "outputs": [],
   "source": [
    "train_df_ids = train[['PetID']]\n",
    "\n",
    "train_df_ids = train[['PetID']]\n",
    "train_df_metadata = pd.DataFrame(train_metadata_files)\n",
    "train_df_metadata.columns = ['metadata_filename']\n",
    "train_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n",
    "train_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\n",
    "\n",
    "train_df_ids = train[['PetID']]\n",
    "train_df_sentiment = pd.DataFrame(train_sentiment_files)\n",
    "train_df_sentiment.columns = ['sentiment_filename']\n",
    "train_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\n",
    "train_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "_uuid": "34a100d54acaa3577da2d1aad6775a7f4dc16ba8"
   },
   "outputs": [],
   "source": [
    "test_df_ids = test[['PetID']]\n",
    "\n",
    "test_df_metadata = pd.DataFrame(test_metadata_files)\n",
    "test_df_metadata.columns = ['metadata_filename']\n",
    "test_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n",
    "test_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\n",
    "\n",
    "test_df_sentiment = pd.DataFrame(test_sentiment_files)\n",
    "test_df_sentiment.columns = ['sentiment_filename']\n",
    "test_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\n",
    "test_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "_uuid": "1ac4c4ec5ccb59846ca1eb22f849713cd87668b9"
   },
   "outputs": [],
   "source": [
    "class PetFinderParser(object):\n",
    "    \n",
    "    def __init__(self, debug=False):        \n",
    "        self.debug = debug\n",
    "        self.sentence_sep = ' '        \n",
    "        self.extract_sentiment_text = False\n",
    "    \n",
    "    def open_json_file(self, filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            json_file = json.load(f)\n",
    "        return json_file\n",
    "        \n",
    "    def parse_sentiment_file(self, file):\n",
    "        file_sentiment = file['documentSentiment']\n",
    "        file_entities = [x['name'] for x in file['entities']]\n",
    "        file_entities = self.sentence_sep.join(file_entities)       \n",
    "        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]        \n",
    "        file_sentences_sentiment = pd.DataFrame.from_dict(\n",
    "            file_sentences_sentiment, orient='columns')\n",
    "        file_sentences_sentiment_df = pd.DataFrame(\n",
    "            {\n",
    "                'magnitude_sum': file_sentences_sentiment['magnitude'].sum(axis=0),\n",
    "                'score_sum': file_sentences_sentiment['score'].sum(axis=0),\n",
    "                'magnitude_mean': file_sentences_sentiment['magnitude'].mean(axis=0),\n",
    "                'score_mean': file_sentences_sentiment['score'].mean(axis=0),\n",
    "                'magnitude_var': file_sentences_sentiment['magnitude'].var(axis=0),\n",
    "                'score_var': file_sentences_sentiment['score'].var(axis=0),\n",
    "            }, index=[0]\n",
    "        )        \n",
    "        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n",
    "        df_sentiment = pd.concat([df_sentiment, file_sentences_sentiment_df], axis=1)            \n",
    "        df_sentiment['entities'] = file_entities\n",
    "        df_sentiment = df_sentiment.add_prefix('sentiment_')        \n",
    "        return df_sentiment\n",
    "    \n",
    "    def parse_metadata_file(self, file):\n",
    "        file_keys = list(file.keys())        \n",
    "        if 'labelAnnotations' in file_keys:\n",
    "            file_annots = file['labelAnnotations']\n",
    "            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n",
    "            file_top_desc = [x['description'] for x in file_annots]\n",
    "        else:\n",
    "            file_top_score = np.nan\n",
    "            file_top_desc = ['']        \n",
    "        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n",
    "        file_crops = file['cropHintsAnnotation']['cropHints']\n",
    "        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n",
    "        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n",
    "        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()        \n",
    "        if 'importanceFraction' in file_crops[0].keys():\n",
    "            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n",
    "        else:\n",
    "            file_crop_importance = np.nan\n",
    "        df_metadata = {\n",
    "            'annots_score': file_top_score,\n",
    "            'color_score': file_color_score,\n",
    "            'color_pixelfrac': file_color_pixelfrac,\n",
    "            'crop_conf': file_crop_conf,\n",
    "            'crop_importance': file_crop_importance,\n",
    "            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n",
    "        }        \n",
    "        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n",
    "        df_metadata = df_metadata.add_prefix('metadata_')        \n",
    "        return df_metadata\n",
    "    \n",
    "def extract_additional_features(pet_id, mode='train'):\n",
    "    sentiment_filename = f'../input/petfinder-adoption-prediction/{mode}_sentiment/{pet_id}.json'\n",
    "    try:\n",
    "        sentiment_file = pet_parser.open_json_file(sentiment_filename)\n",
    "        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n",
    "        df_sentiment['PetID'] = pet_id\n",
    "    except FileNotFoundError:\n",
    "        df_sentiment = []\n",
    "    dfs_metadata = []\n",
    "    metadata_filenames = sorted(glob.glob(f'../input/petfinder-adoption-prediction/{mode}_metadata/{pet_id}*.json'))\n",
    "    if len(metadata_filenames) > 0:\n",
    "        for f in metadata_filenames:\n",
    "            metadata_file = pet_parser.open_json_file(f)\n",
    "            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n",
    "            df_metadata['PetID'] = pet_id\n",
    "            dfs_metadata.append(df_metadata)\n",
    "        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n",
    "    dfs = [df_sentiment, dfs_metadata]    \n",
    "    return dfs\n",
    "\n",
    "pet_parser = PetFinderParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "_uuid": "5c97527c16c4d32b83e831813f3c41dce34e4445"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   19.6s\n",
      "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:   39.5s\n",
      "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1246 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1796 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2446 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3196 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4046 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4996 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6046 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7196 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done 8446 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-1)]: Done 9796 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 11246 tasks      | elapsed: 14.9min\n",
      "[Parallel(n_jobs=-1)]: Done 12796 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=-1)]: Done 14446 tasks      | elapsed: 19.2min\n",
      "[Parallel(n_jobs=-1)]: Done 14993 out of 14993 | elapsed: 19.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14442, 10) (58311, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 172 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 772 tasks      | elapsed:   23.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1772 tasks      | elapsed:   53.4s\n",
      "[Parallel(n_jobs=-1)]: Done 3172 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3948 out of 3948 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3815, 10) (15040, 7)\n"
     ]
    }
   ],
   "source": [
    "# take a long time here, about 25 minutes\n",
    "\n",
    "train_pet_ids = train.PetID.unique()\n",
    "test_pet_ids = test.PetID.unique()\n",
    "\n",
    "dfs_train = Parallel(n_jobs=-1, verbose=1)(\n",
    "    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n",
    "train_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\n",
    "train_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n",
    "train_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\n",
    "train_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n",
    "print(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n",
    "\n",
    "dfs_test = Parallel(n_jobs=-1, verbose=1)(\n",
    "    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n",
    "test_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\n",
    "test_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n",
    "test_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\n",
    "test_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n",
    "print(test_dfs_sentiment.shape, test_dfs_metadata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "_uuid": "875ea01cd56d9f4a8113ab27291309ba6ef6c876"
   },
   "outputs": [],
   "source": [
    "aggregates = ['sum', 'mean', 'var']\n",
    "sent_agg = ['sum']\n",
    "\n",
    "train_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\n",
    "train_metadata_desc = train_metadata_desc.reset_index()\n",
    "train_metadata_desc[\n",
    "    'metadata_annots_top_desc'] = train_metadata_desc[\n",
    "    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'metadata'\n",
    "train_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\n",
    "for i in train_metadata_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\n",
    "train_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\n",
    "train_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in train_metadata_gr.columns.tolist()])\n",
    "train_metadata_gr = train_metadata_gr.reset_index()\n",
    "\n",
    "train_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\n",
    "train_sentiment_desc = train_sentiment_desc.reset_index()\n",
    "train_sentiment_desc[\n",
    "    'sentiment_entities'] = train_sentiment_desc[\n",
    "    'sentiment_entities'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'sentiment'\n",
    "train_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\n",
    "for i in train_sentiment_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\n",
    "train_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(sent_agg)\n",
    "train_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in train_sentiment_gr.columns.tolist()])\n",
    "train_sentiment_gr = train_sentiment_gr.reset_index()\n",
    "\n",
    "\n",
    "test_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\n",
    "test_metadata_desc = test_metadata_desc.reset_index()\n",
    "test_metadata_desc[\n",
    "    'metadata_annots_top_desc'] = test_metadata_desc[\n",
    "    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'metadata'\n",
    "test_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\n",
    "for i in test_metadata_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\n",
    "test_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\n",
    "test_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in test_metadata_gr.columns.tolist()])\n",
    "test_metadata_gr = test_metadata_gr.reset_index()\n",
    "\n",
    "test_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\n",
    "test_sentiment_desc = test_sentiment_desc.reset_index()\n",
    "test_sentiment_desc[\n",
    "    'sentiment_entities'] = test_sentiment_desc[\n",
    "    'sentiment_entities'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'sentiment'\n",
    "test_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities'], axis=1)\n",
    "for i in test_sentiment_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\n",
    "test_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(sent_agg)\n",
    "test_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in test_sentiment_gr.columns.tolist()])\n",
    "test_sentiment_gr = test_sentiment_gr.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "_uuid": "e14b17f128df3a506b88138466c584fdfb77d316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 49) (3948, 48)\n"
     ]
    }
   ],
   "source": [
    "train_proc = train.copy()\n",
    "train_proc = train_proc.merge(\n",
    "    train_sentiment_gr, how='left', on='PetID')\n",
    "train_proc = train_proc.merge(\n",
    "    train_metadata_gr, how='left', on='PetID')\n",
    "train_proc = train_proc.merge(\n",
    "    train_metadata_desc, how='left', on='PetID')\n",
    "train_proc = train_proc.merge(\n",
    "    train_sentiment_desc, how='left', on='PetID')\n",
    "\n",
    "test_proc = test.copy()\n",
    "test_proc = test_proc.merge(\n",
    "    test_sentiment_gr, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_metadata_gr, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_metadata_desc, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_sentiment_desc, how='left', on='PetID')\n",
    "\n",
    "print(train_proc.shape, test_proc.shape)\n",
    "assert train_proc.shape[0] == train.shape[0]\n",
    "assert test_proc.shape[0] == test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "_uuid": "c6a46f86abd30df4fc6c613bc5cce4b518b82a7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 53) (3948, 52)\n"
     ]
    }
   ],
   "source": [
    "train_breed_main = train_proc[['Breed1']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed1', right_on='BreedID',\n",
    "    suffixes=('', '_main_breed'))\n",
    "train_breed_main = train_breed_main.iloc[:, 2:]\n",
    "train_breed_main = train_breed_main.add_prefix('main_breed_')\n",
    "train_breed_second = train_proc[['Breed2']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed2', right_on='BreedID',\n",
    "    suffixes=('', '_second_breed'))\n",
    "train_breed_second = train_breed_second.iloc[:, 2:]\n",
    "train_breed_second = train_breed_second.add_prefix('second_breed_')\n",
    "train_proc = pd.concat(\n",
    "    [train_proc, train_breed_main, train_breed_second], axis=1)\n",
    "\n",
    "test_breed_main = test_proc[['Breed1']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed1', right_on='BreedID',\n",
    "    suffixes=('', '_main_breed'))\n",
    "test_breed_main = test_breed_main.iloc[:, 2:]\n",
    "test_breed_main = test_breed_main.add_prefix('main_breed_')\n",
    "test_breed_second = test_proc[['Breed2']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed2', right_on='BreedID',\n",
    "    suffixes=('', '_second_breed'))\n",
    "test_breed_second = test_breed_second.iloc[:, 2:]\n",
    "test_breed_second = test_breed_second.add_prefix('second_breed_')\n",
    "test_proc = pd.concat(\n",
    "    [test_proc, test_breed_main, test_breed_second], axis=1)\n",
    "\n",
    "print(train_proc.shape, test_proc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "_uuid": "936333e2ddcf97e697f2653581eb101f1175f249"
   },
   "outputs": [],
   "source": [
    "X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "_uuid": "921f2615dd33c8ca4c1e4b269097676560de620c"
   },
   "outputs": [],
   "source": [
    "X_temp = X.copy()\n",
    "\n",
    "text_columns = ['Description', 'metadata_annots_top_desc', 'sentiment_entities']\n",
    "categorical_columns = ['main_breed_BreedName', 'second_breed_BreedName']\n",
    "\n",
    "to_drop_columns = ['PetID', 'Name', 'RescuerID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "_uuid": "9faf88981566aad9050270839e26551dc4702127"
   },
   "outputs": [],
   "source": [
    "rescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index()\n",
    "rescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n",
    "\n",
    "X_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "_uuid": "cc54e76b0e08bbfa56620048394ac3f80828d044"
   },
   "outputs": [],
   "source": [
    "for i in categorical_columns:\n",
    "    X_temp.loc[:, i] = pd.factorize(X_temp.loc[:, i])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "_uuid": "cad5744f4acab6aa9e2ff0116d978006e3d03745"
   },
   "outputs": [],
   "source": [
    "X_text = X_temp[text_columns]\n",
    "\n",
    "for i in X_text.columns:\n",
    "    X_text.loc[:, i] = X_text.loc[:, i].fillna('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "_uuid": "b3a5e69067caf2f88a4fb17e51a9347774004a53"
   },
   "outputs": [],
   "source": [
    "X_temp['Length_Description'] = X_text['Description'].map(len)\n",
    "X_temp['Length_metadata_annots_top_desc'] = X_text['metadata_annots_top_desc'].map(len)\n",
    "X_temp['Lengths_sentiment_entities'] = X_text['sentiment_entities'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "_uuid": "d064795c722b641d260925af1e6bfba99ef34a00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating features from: Description\n",
      "generating features from: metadata_annots_top_desc\n",
      "generating features from: sentiment_entities\n"
     ]
    }
   ],
   "source": [
    "n_components = 16\n",
    "text_features = []\n",
    "\n",
    "for i in X_text.columns:\n",
    "    print(f'generating features from: {i}')\n",
    "    tfv = TfidfVectorizer(min_df=2,  max_features=None,\n",
    "                          strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n",
    "                          ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "    svd_ = TruncatedSVD(\n",
    "        n_components=n_components, random_state=1337)\n",
    "    tfidf_col = tfv.fit_transform(X_text.loc[:, i].values)    \n",
    "    svd_col = svd_.fit_transform(tfidf_col)\n",
    "    svd_col = pd.DataFrame(svd_col)\n",
    "    svd_col = svd_col.add_prefix('TFIDF_{}_'.format(i))    \n",
    "    text_features.append(svd_col)\n",
    "    \n",
    "text_features = pd.concat(text_features, axis=1)\n",
    "\n",
    "X_temp = pd.concat([X_temp, text_features], axis=1)\n",
    "\n",
    "for i in X_text.columns:\n",
    "    X_temp = X_temp.drop(i, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "_uuid": "77fb8cdbfb4e5d33aa0cf13de6b052f879157039"
   },
   "outputs": [],
   "source": [
    "X_temp = X_temp.merge(img_features, how='left', on='PetID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "_uuid": "8d1b7d4e203ae17789a3ccef5fcaf447417e8087"
   },
   "outputs": [],
   "source": [
    "train_df_ids = train[['PetID']]\n",
    "test_df_ids = test[['PetID']]\n",
    "\n",
    "train_df_imgs = pd.DataFrame(train_image_files)\n",
    "train_df_imgs.columns = ['image_filename']\n",
    "train_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n",
    "\n",
    "test_df_imgs = pd.DataFrame(test_image_files)\n",
    "test_df_imgs.columns = ['image_filename']\n",
    "test_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n",
    "\n",
    "train_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\n",
    "test_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\n",
    "\n",
    "def getSize(filename):\n",
    "    st = os.stat(filename)\n",
    "    return st.st_size\n",
    "\n",
    "def getDimensions(filename):\n",
    "    img_size = Image.open(filename).size\n",
    "    return img_size \n",
    "\n",
    "train_df_imgs['image_size'] = train_df_imgs['image_filename'].apply(getSize)\n",
    "train_df_imgs['temp_size'] = train_df_imgs['image_filename'].apply(getDimensions)\n",
    "train_df_imgs['width'] = train_df_imgs['temp_size'].apply(lambda x : x[0])\n",
    "train_df_imgs['height'] = train_df_imgs['temp_size'].apply(lambda x : x[1])\n",
    "train_df_imgs = train_df_imgs.drop(['temp_size'], axis=1)\n",
    "\n",
    "test_df_imgs['image_size'] = test_df_imgs['image_filename'].apply(getSize)\n",
    "test_df_imgs['temp_size'] = test_df_imgs['image_filename'].apply(getDimensions)\n",
    "test_df_imgs['width'] = test_df_imgs['temp_size'].apply(lambda x : x[0])\n",
    "test_df_imgs['height'] = test_df_imgs['temp_size'].apply(lambda x : x[1])\n",
    "test_df_imgs = test_df_imgs.drop(['temp_size'], axis=1)\n",
    "\n",
    "aggs = {\n",
    "    'image_size': ['sum', 'mean', 'var'],\n",
    "    'width': ['sum', 'mean', 'var'],\n",
    "    'height': ['sum', 'mean', 'var'],\n",
    "}\n",
    "agg_train_imgs = train_df_imgs.groupby('PetID').agg(aggs)\n",
    "new_columns = [\n",
    "    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n",
    "]\n",
    "agg_train_imgs.columns = new_columns\n",
    "agg_train_imgs = agg_train_imgs.reset_index()\n",
    "\n",
    "agg_test_imgs = test_df_imgs.groupby('PetID').agg(aggs)\n",
    "new_columns = [\n",
    "    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n",
    "]\n",
    "agg_test_imgs.columns = new_columns\n",
    "agg_test_imgs = agg_test_imgs.reset_index()\n",
    "\n",
    "agg_imgs = pd.concat([agg_train_imgs, agg_test_imgs], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "_uuid": "fac322e21458cbbd236e0316b00b74fc6a9d178c"
   },
   "outputs": [],
   "source": [
    "X_temp = X_temp.merge(agg_imgs, how='left', on='PetID')\n",
    "\n",
    "X_temp = X_temp.drop(to_drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "_uuid": "8a24d680d4f50c5a70bf422a990b70bc1ce2bafb"
   },
   "outputs": [],
   "source": [
    "X_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\n",
    "X_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n",
    "\n",
    "X_test = X_test.drop(['AdoptionSpeed'], axis=1)\n",
    "\n",
    "assert X_train.shape[0] == train.shape[0]\n",
    "assert X_test.shape[0] == test.shape[0]\n",
    "\n",
    "train_cols = X_train.columns.tolist()\n",
    "train_cols.remove('AdoptionSpeed')\n",
    "\n",
    "test_cols = X_test.columns.tolist()\n",
    "\n",
    "assert np.all(train_cols == test_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "_uuid": "9ea0415a381b321e8bb341a605552bf56e6a1979"
   },
   "outputs": [],
   "source": [
    "X_train_non_null = X_train.fillna(-1)\n",
    "X_test_non_null = X_test.fillna(-1)\n",
    "X_train_non_null['ResNet_meta'] = train_img_prob.flatten()         # ADD IMG ResNet50 metafeature\n",
    "X_test_non_null['ResNet_meta'] = test_img_prob.flatten()           # ADD IMG ResNet50 metafeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "_uuid": "65f111d010d8ea785b769f97552590ad82e6247f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_non_null.isnull().any().any(), X_test_non_null.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "_uuid": "f899e9d6f18742a3735dfa188b02ea79baba688c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14993, 141), (3948, 140))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_non_null.shape, X_test_non_null.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bbeea72bf0f28a4e9d00641b30bd18455dc1817f"
   },
   "source": [
    "## SAVE !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "_uuid": "75ebe709fc62c49b677b471c90372fea654b34b0"
   },
   "outputs": [],
   "source": [
    "X_train_non_null.to_csv(\"train_453.csv\", index=False)\n",
    "X_test_non_null.to_csv(\"test_453.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c83b64002c92e506b81ea74066677c944c68cad7"
   },
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "_uuid": "58b5ba6bd0198b61f5b5ffe812f65a6050a8319c"
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': 1337,\n",
    "    'eta': 0.0123,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.85,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'device': 'gpu',\n",
    "    'silent': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "_uuid": "514c20f24c31c5e5edd31c9ebda5a02bafd75482"
   },
   "outputs": [],
   "source": [
    "def run_xgb(params, X_train, X_test):\n",
    "    kf = FOLDS\n",
    "    n_splits = N_FOLDS\n",
    "    \n",
    "    verbose_eval = 1000\n",
    "    num_rounds = 60000\n",
    "    early_stop = 500\n",
    "\n",
    "    oof_train = np.zeros((X_train.shape[0]))\n",
    "    oof_test = np.zeros((X_test.shape[0], n_splits))\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for train_idx, valid_idx in kf.split(X_train, X_train['AdoptionSpeed'].values):\n",
    "\n",
    "        X_tr = X_train.iloc[train_idx, :]\n",
    "        X_val = X_train.iloc[valid_idx, :]\n",
    "\n",
    "        y_tr = X_tr['AdoptionSpeed'].values\n",
    "        X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n",
    "\n",
    "        y_val = X_val['AdoptionSpeed'].values\n",
    "        X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n",
    "\n",
    "        d_train = xgb.DMatrix(data=X_tr, label=y_tr, feature_names=X_tr.columns)\n",
    "        d_valid = xgb.DMatrix(data=X_val, label=y_val, feature_names=X_val.columns)\n",
    "\n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        model = xgb.train(dtrain=d_train, num_boost_round=num_rounds, evals=watchlist,\n",
    "                         early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params)\n",
    "\n",
    "        valid_pred = model.predict(xgb.DMatrix(X_val, feature_names=X_val.columns), ntree_limit=model.best_ntree_limit)\n",
    "        test_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_test.columns), ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "        oof_train[valid_idx] = valid_pred\n",
    "        oof_test[:, i] = test_pred\n",
    "\n",
    "        i += 1\n",
    "    return model, oof_train, oof_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "_uuid": "0a2d026c70a3ab522fd8129fcca6c4abdde6ce7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2.31218\tvalid-rmse:2.31227\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 500 rounds.\n",
      "[1000]\ttrain-rmse:0.637177\tvalid-rmse:1.0308\n",
      "[2000]\ttrain-rmse:0.416152\tvalid-rmse:1.02711\n",
      "[3000]\ttrain-rmse:0.272851\tvalid-rmse:1.02766\n",
      "Stopping. Best iteration:\n",
      "[2573]\ttrain-rmse:0.32597\tvalid-rmse:1.02696\n",
      "\n",
      "[0]\ttrain-rmse:2.31234\tvalid-rmse:2.31232\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 500 rounds.\n",
      "[1000]\ttrain-rmse:0.64406\tvalid-rmse:1.02729\n",
      "[2000]\ttrain-rmse:0.428487\tvalid-rmse:1.02316\n",
      "Stopping. Best iteration:\n",
      "[2122]\ttrain-rmse:0.407167\tvalid-rmse:1.02273\n",
      "\n",
      "[0]\ttrain-rmse:2.3123\tvalid-rmse:2.31305\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 500 rounds.\n",
      "[1000]\ttrain-rmse:0.64667\tvalid-rmse:1.01995\n",
      "[2000]\ttrain-rmse:0.431258\tvalid-rmse:1.01717\n",
      "Stopping. Best iteration:\n",
      "[2417]\ttrain-rmse:0.364279\tvalid-rmse:1.01667\n",
      "\n",
      "[0]\ttrain-rmse:2.31221\tvalid-rmse:2.31304\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 500 rounds.\n",
      "[1000]\ttrain-rmse:0.63704\tvalid-rmse:1.02924\n",
      "[2000]\ttrain-rmse:0.421618\tvalid-rmse:1.02848\n",
      "Stopping. Best iteration:\n",
      "[2478]\ttrain-rmse:0.345585\tvalid-rmse:1.02786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, oof_train, oof_test = run_xgb(xgb_params, X_train_non_null, X_test_non_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "_uuid": "9752f157d9fdcaa4155495c87b962592d5d650f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14993,), (3948,))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_453_train_pred = oof_train\n",
    "xgb_453_test_pred = np.mean(oof_test, axis=1)\n",
    "xgb_453_train_pred.shape, xgb_453_test_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f70565c5da3f1487a3cb3833cf736af1498f005"
   },
   "source": [
    "WarpperNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "_uuid": "964df9bc997cc0bacb5ef5195178b54adea5729b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65d4b62b5717e5c31ef96643ee13b9538d601d48"
   },
   "source": [
    "# Corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "_uuid": "86eb23134bee46334acbb26c57af726e92c8f664"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gzf_lgb</th>\n",
       "      <th>zkr_lgb</th>\n",
       "      <th>zyl_lgb</th>\n",
       "      <th>453_xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gzf_lgb</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971332</td>\n",
       "      <td>0.943132</td>\n",
       "      <td>0.892131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zkr_lgb</th>\n",
       "      <td>0.971332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.948968</td>\n",
       "      <td>0.893275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyl_lgb</th>\n",
       "      <td>0.943132</td>\n",
       "      <td>0.948968</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453_xgb</th>\n",
       "      <td>0.892131</td>\n",
       "      <td>0.893275</td>\n",
       "      <td>0.886197</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          gzf_lgb   zkr_lgb   zyl_lgb   453_xgb\n",
       "gzf_lgb  1.000000  0.971332  0.943132  0.892131\n",
       "zkr_lgb  0.971332  1.000000  0.948968  0.893275\n",
       "zyl_lgb  0.943132  0.948968  1.000000  0.886197\n",
       "453_xgb  0.892131  0.893275  0.886197  1.000000"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gzf_lgb = lgb_gzf[\"test\"].reshape(-1)\n",
    "zkr_lgb = lgb_zkr[\"test\"].reshape(-1)\n",
    "zyl_lgb = lgb_zyl[\"test\"].reshape(-1)\n",
    "\n",
    "dfa = pd.DataFrame({\"gzf_lgb\":gzf_lgb, \n",
    "                    \"zkr_lgb\":zkr_lgb, \n",
    "                    \"zyl_lgb\":zyl_lgb, \n",
    "                    \"453_xgb\": xgb_453_test_pred})\n",
    "dfa.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "67ead707709e414b26d42d0889c8fa2fac513c9a"
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "_uuid": "12f9eee9bcaa0387620a3a7dd0f9825c21635087"
   },
   "outputs": [],
   "source": [
    "gzf_lgb_train_pred = np.mean(lgb_gzf['train'], axis=1)\n",
    "gzf_lgb_test_pred = np.mean(lgb_gzf['test'], axis=1)\n",
    "\n",
    "zkr_lgb_train_pred = np.mean(lgb_zkr['train'], axis=1)\n",
    "zkr_lgb_test_pred = np.mean(lgb_zkr['test'], axis=1)\n",
    "\n",
    "zyl_lgb_train_pred = np.mean(lgb_zyl['train'], axis=1)\n",
    "zyl_lgb_test_pred = np.mean(lgb_zyl['test'], axis=1)\n",
    "\n",
    "\n",
    "train_meta = np.concatenate([gzf_lgb_train_pred.reshape(-1,1),\n",
    "                             zkr_lgb_train_pred.reshape(-1,1),\n",
    "                             zyl_lgb_train_pred.reshape(-1,1),\n",
    "                             xgb_453_train_pred.reshape(-1,1),\n",
    "                             #mlp_zkr_train_pred.reshape(-1,1)\n",
    "                            ], axis=1)\n",
    "test_meta = np.concatenate([gzf_lgb_test_pred.reshape(-1,1),\n",
    "                            zkr_lgb_test_pred.reshape(-1,1),\n",
    "                            zyl_lgb_test_pred.reshape(-1,1),\n",
    "                            xgb_453_test_pred.reshape(-1,1),\n",
    "                            #mlp_zkr_test_pred.reshape(-1,1)\n",
    "                           ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "_uuid": "977e8c61c2149fb4afc9a4e7e1275a4797744664"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "_uuid": "06c42389b7a4a1f4fb17de6330215387af251902"
   },
   "outputs": [],
   "source": [
    "clf = Ridge(alpha=0.1)\n",
    "\n",
    "clf.fit(train_meta, target)\n",
    "train_pred = clf.predict(train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "_uuid": "4d40fcdf8b0bb43806b1f358302b0e9eb726b8b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09241487 0.13100189 0.59196831 0.39523388]\n"
     ]
    }
   ],
   "source": [
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "_uuid": "3f8984d7656a80e3d3932e24dca0963c421fd6ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients:  [1.44182981 2.04854091 2.48911401 2.89681534] \n",
      "\n",
      "True Counter:  Counter({4: 4197, 2: 4037, 3: 3259, 1: 3090, 0: 410})\n",
      "Train Counter:  Counter({2: 4102, 4: 4056, 3: 3281, 1: 3153, 0: 401})\n",
      "\n",
      "Train QWK:  0.499594060615012\n",
      "Train RMSE:  1.0061667493691746\n"
     ]
    }
   ],
   "source": [
    "init_coef = get_init_coefs(train_pred,  target)\n",
    "optR = OptimizedRounder_v2(initial_coefs=init_coef)\n",
    "optR.fit(train_pred, target)\n",
    "coefficients = optR.coefficients()\n",
    "print(\"coefficients: \", coefficients, \"\\n\")\n",
    "\n",
    "print(\"True Counter: \", Counter(target))\n",
    "\n",
    "optR = OptimizedRounder_v2()\n",
    "train_predictions = optR.predict(train_pred, coefficients).astype(int)\n",
    "print(\"Train Counter: \", Counter(train_predictions))\n",
    "\n",
    "print(\"\\nTrain QWK: \", quadratic_weighted_kappa(target, train_predictions))\n",
    "print(\"Train RMSE: \", rmse(target, train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "_uuid": "f98732821cd1ed5817ac133efd89b23866669e88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Counter:  Counter({4: 1105, 2: 1066, 3: 840, 1: 839, 0: 98}) \n",
      "\n",
      "True Distribution:\n",
      "0    0.027346\n",
      "1    0.206096\n",
      "2    0.269259\n",
      "3    0.217368\n",
      "4    0.279931\n",
      "Name: AdoptionSpeed, dtype: float64\n",
      "Train Predicted Distribution:\n",
      "0    0.026746\n",
      "1    0.210298\n",
      "2    0.273594\n",
      "3    0.218835\n",
      "4    0.270526\n",
      "dtype: float64\n",
      "Test Predicted Distribution:\n",
      "0    0.024823\n",
      "1    0.212513\n",
      "2    0.270010\n",
      "3    0.212766\n",
      "4    0.279889\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(test_meta)\n",
    "# optR = OptimizedRounder_v2()\n",
    "# test_predictions = optR.predict(predictions, coefficients).astype(int)\n",
    "optR = OptimizedRounder_v3()\n",
    "test_predictions = optR.predict(predictions, coefficients, 110).astype(int)\n",
    "print(\"Test Counter: \", Counter(test_predictions), \"\\n\")\n",
    "\n",
    "print(\"True Distribution:\")\n",
    "print(pd.value_counts(target, normalize=True).sort_index())\n",
    "print(\"Train Predicted Distribution:\")\n",
    "print(pd.value_counts(train_predictions, normalize=True).sort_index())\n",
    "print(\"Test Predicted Distribution:\")\n",
    "print(pd.value_counts(test_predictions, normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "_uuid": "36ee65ded47bdbde1d8033c30224e65abdc17f43"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PetID</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>378fcc4fc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73c10e136</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72000c4c5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e147a4b9f</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43fbba852</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>77a490ec9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28c4b1b13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>d1eada628</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>d134dec34</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bcd464bb8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PetID  AdoptionSpeed\n",
       "0  378fcc4fc              2\n",
       "1  73c10e136              4\n",
       "2  72000c4c5              3\n",
       "3  e147a4b9f              4\n",
       "4  43fbba852              4\n",
       "5  77a490ec9              3\n",
       "6  28c4b1b13              4\n",
       "7  d1eada628              3\n",
       "8  d134dec34              3\n",
       "9  bcd464bb8              2"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({'PetID': test_id, 'AdoptionSpeed': test_predictions})\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "_uuid": "4cd97ac32dfe34fb81e6e2e19af373cbd8509e29"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "_uuid": "4166d8db9a2dade4cd47febf966b80b56b3d65cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PetID,AdoptionSpeed\r\n",
      "378fcc4fc,2\r\n",
      "73c10e136,4\r\n",
      "72000c4c5,3\r\n",
      "e147a4b9f,4\r\n",
      "43fbba852,4\r\n",
      "77a490ec9,3\r\n",
      "28c4b1b13,4\r\n",
      "d1eada628,3\r\n",
      "d134dec34,3\r\n"
     ]
    }
   ],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "_uuid": "b2becea193304df8748f921b43e4e286cc70be09"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
